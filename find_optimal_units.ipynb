{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "find_optimal_units.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMFhd6v0hZrTnf2svvpLagz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Inha-AI/DACON-semiconductor-competition/blob/feature%2FYoonSungLee/find_optimal_units.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-eXuhv5qwgg",
        "colab_type": "code",
        "outputId": "c494c7de-0cc7-4c42-d78f-1f0f48ec69cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "pip install bayesian-optimization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/72/0c/173ac467d0a53e33e41b521e4ceba74a8ac7c7873d7b857a8fbdca88302d/bayesian-optimization-1.0.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.17.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.22.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.14.1)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.0.1-cp36-none-any.whl size=10032 sha256=bbba662eac651d5a31fad677e8834a48514797055de445092394c7c98eb874f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/0d/3b/6b9d4477a34b3905f246ff4e7acf6aafd4cc9b77d473629b77\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vVeUdyssBKw",
        "colab_type": "code",
        "outputId": "61fa8211-55ee-457a-c5c4-9c7cddd394c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bayes_opt import BayesianOptimization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwHXDEigsNB0",
        "colab_type": "code",
        "outputId": "75162b8b-1c9d-4857-eb7d-949cf31c8dbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQF6BGGF1DqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 자료형을 적절히 변형시켜 데이터의 크기를 줄이는 방법\n",
        "\n",
        "# for col in df_train.columns:\n",
        "#     col_type = df_train[col].dtypes\n",
        "#     min1 = df_train[col].min()\n",
        "#     max1 = df_train[col].max()\n",
        "#     if str(col_type)[:3] == 'int':\n",
        "#         df_train[col] = df_train[col].astype(np.int16)\n",
        "#     else:\n",
        "#         if min1 > np.finfo(np.float16).min and max1 < np.finfo(np.float16).max:\n",
        "#             df_train[col] = trdf_trainain[col].astype(np.float16)\n",
        "#         elif min1 > np.finfo(np.float32).min and max1 < np.finfo(np.float32).max:\n",
        "#             df_train[col] = df_train[col].astype(np.float32)\n",
        "#         else:\n",
        "#             df_train[col] = df_train[col].astype(np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BNXAU5rsPJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv('/gdrive/My Drive/DACON-semiconductor-competition/dataset/train.csv')\n",
        "df_test = pd.read_csv('/gdrive/My Drive/DACON-semiconductor-competition/dataset/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdYXclfgsVr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 독립변수와 종속변수를 분리합니다.\n",
        "\n",
        "train_X = df_train.iloc[:,4:]\n",
        "train_Y = df_train.iloc[:,0:4]\n",
        "test_X = df_test.iloc[:,1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogdcz8YctuVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train set을 shuffle하여 다시 train set과 validation set으로 분리합니다.\n",
        "\n",
        "train_X, val_X, train_Y, val_Y = train_test_split(train_X, train_Y, test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tT4w6XasaSQ",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PSSfoS5sfWK",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGEkxH551LkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 케라스를 통해 모델 생성을 시작합니다.\n",
        "\n",
        "def create_model(units):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=units, input_dim=226, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=units, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=units, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=units, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=units, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=units, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=units, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=units, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=units, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=units, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=4, activation='linear'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Activation Function 정의\n",
        "\n",
        "def swish(x) :\n",
        "    return x * keras.activations.sigmoid(x)\n",
        "\n",
        "\n",
        "# layer의 특정 units 하에서 학습을 수행한 후, 검증 성능을 출력하는 목적 함수 정의\n",
        "\n",
        "def train_and_validate(units):\n",
        "    model = create_model(int(units))\n",
        "    adam = keras.optimizers.Adam(0.001)\n",
        "    model.compile(loss='mae', optimizer=adam, metrics=['accuracy'])\n",
        "    hist = model.fit(train_X, train_Y, epochs=20, batch_size=630,\n",
        "                    validation_data=(val_X, val_Y))\n",
        "    best_val_score = max(hist.history['val_acc'])\n",
        "\n",
        "    return best_val_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35DQBYFkofR6",
        "colab_type": "code",
        "outputId": "ea49064f-ca00-426e-8a8c-4cf063eea4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# BayesianOptimization 객체 생성, 실행 및 최종 결과 출력\n",
        "# 최적의 layer units 수를 찾기 위함\n",
        "\n",
        "bayes_optimizer = BayesianOptimization(\n",
        "    f=train_and_validate,\n",
        "    pbounds={\n",
        "         'units' : (100, 800)\n",
        "    },\n",
        "    random_state=0,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "bayes_optimizer.maximize(init_points=3, n_iter=27, acq='ei',xi=0.01)\n",
        "\n",
        "for i, res in enumerate(bayes_optimizer.res):\n",
        "    print('iteration {}: \\n\\t{}'.format(i, res))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   |   units   |\n",
            "-------------------------------------\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "607500/607500 [==============================] - 29s 47us/step - loss: 72.3486 - acc: 0.3736 - val_loss: 67.9759 - val_acc: 0.3303\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 27.0557 - acc: 0.7234 - val_loss: 25.4235 - val_acc: 0.7373\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 16.6041 - acc: 0.8296 - val_loss: 15.8608 - val_acc: 0.8363\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 12.8150 - acc: 0.8668 - val_loss: 14.3804 - val_acc: 0.8540\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 10.6862 - acc: 0.8875 - val_loss: 11.5420 - val_acc: 0.8775\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 9.3785 - acc: 0.9006 - val_loss: 8.9325 - val_acc: 0.9104\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 8.4680 - acc: 0.9092 - val_loss: 8.4750 - val_acc: 0.9041\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 7.7744 - acc: 0.9162 - val_loss: 8.4337 - val_acc: 0.9115\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 7.2528 - acc: 0.9214 - val_loss: 7.7134 - val_acc: 0.9144\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 6.8377 - acc: 0.9250 - val_loss: 7.4064 - val_acc: 0.9216\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 6.4466 - acc: 0.9290 - val_loss: 6.6150 - val_acc: 0.9323\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 6.0940 - acc: 0.9319 - val_loss: 6.4802 - val_acc: 0.9293\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 5.8708 - acc: 0.9338 - val_loss: 6.2550 - val_acc: 0.9287\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 5.6463 - acc: 0.9361 - val_loss: 5.4388 - val_acc: 0.9384\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 5.4249 - acc: 0.9379 - val_loss: 5.6695 - val_acc: 0.9360\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 5.2572 - acc: 0.9393 - val_loss: 5.8712 - val_acc: 0.9293\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 5.0819 - acc: 0.9411 - val_loss: 4.9618 - val_acc: 0.9485\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 4.9366 - acc: 0.9429 - val_loss: 5.3587 - val_acc: 0.9357\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 4.7874 - acc: 0.9442 - val_loss: 4.8604 - val_acc: 0.9450\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 4.6513 - acc: 0.9451 - val_loss: 4.9386 - val_acc: 0.9446\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 71.9378 - acc: 0.3440 - val_loss: 57.4569 - val_acc: 0.4077\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 30.9027 - acc: 0.6777 - val_loss: 26.7133 - val_acc: 0.7347\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 18.7202 - acc: 0.8064 - val_loss: 17.9578 - val_acc: 0.8167\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 13.9658 - acc: 0.8541 - val_loss: 16.1296 - val_acc: 0.8300\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 11.5100 - acc: 0.8786 - val_loss: 13.0254 - val_acc: 0.8583\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 9.9431 - acc: 0.8946 - val_loss: 11.2576 - val_acc: 0.8769\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 8.8293 - acc: 0.9062 - val_loss: 9.5927 - val_acc: 0.9021\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 8.0164 - acc: 0.9138 - val_loss: 8.7467 - val_acc: 0.9014\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 7.4018 - acc: 0.9198 - val_loss: 8.5660 - val_acc: 0.9037\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 6.8823 - acc: 0.9246 - val_loss: 6.9076 - val_acc: 0.9261\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 6.5349 - acc: 0.9273 - val_loss: 7.1406 - val_acc: 0.9201\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 6.1296 - acc: 0.9317 - val_loss: 6.7209 - val_acc: 0.9316\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 5.8516 - acc: 0.9345 - val_loss: 5.5830 - val_acc: 0.9362\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 5.5994 - acc: 0.9359 - val_loss: 5.5236 - val_acc: 0.9358\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 5.3921 - acc: 0.9389 - val_loss: 4.9973 - val_acc: 0.9432\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 5.1653 - acc: 0.9402 - val_loss: 5.1290 - val_acc: 0.9413\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 5.0111 - acc: 0.9420 - val_loss: 5.2148 - val_acc: 0.9440\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 4.8582 - acc: 0.9430 - val_loss: 4.4067 - val_acc: 0.9512\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 4.6595 - acc: 0.9453 - val_loss: 4.5514 - val_acc: 0.9413\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 4.5449 - acc: 0.9458 - val_loss: 4.4198 - val_acc: 0.9490\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.9512  \u001b[0m | \u001b[95m 600.6   \u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 71.8619 - acc: 0.3669 - val_loss: 58.8442 - val_acc: 0.4163\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 27.6681 - acc: 0.7159 - val_loss: 25.7268 - val_acc: 0.7316\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 17.2969 - acc: 0.8217 - val_loss: 17.9094 - val_acc: 0.8120\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 13.2634 - acc: 0.8615 - val_loss: 13.0948 - val_acc: 0.8651\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 11.0725 - acc: 0.8838 - val_loss: 11.9110 - val_acc: 0.8701\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 9.5920 - acc: 0.8982 - val_loss: 10.9112 - val_acc: 0.8835\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 8.6524 - acc: 0.9080 - val_loss: 8.9255 - val_acc: 0.9073\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 7.9199 - acc: 0.9147 - val_loss: 8.9099 - val_acc: 0.9010\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 7.3384 - acc: 0.9200 - val_loss: 7.5846 - val_acc: 0.9183\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 6.8771 - acc: 0.9240 - val_loss: 6.9650 - val_acc: 0.9225\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 6.5284 - acc: 0.9277 - val_loss: 6.8489 - val_acc: 0.9259\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 6.2075 - acc: 0.9309 - val_loss: 6.3550 - val_acc: 0.9351\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 5.9485 - acc: 0.9335 - val_loss: 5.3431 - val_acc: 0.9411\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 5.6706 - acc: 0.9357 - val_loss: 5.3124 - val_acc: 0.9401\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 5.4742 - acc: 0.9380 - val_loss: 5.7930 - val_acc: 0.9308\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 5.3032 - acc: 0.9396 - val_loss: 5.0623 - val_acc: 0.9398\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 5.1083 - acc: 0.9412 - val_loss: 4.4709 - val_acc: 0.9554\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 4.9894 - acc: 0.9420 - val_loss: 5.3153 - val_acc: 0.9351\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 4.8717 - acc: 0.9433 - val_loss: 4.2563 - val_acc: 0.9563\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 4.6739 - acc: 0.9450 - val_loss: 4.8948 - val_acc: 0.9456\n",
            "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.9563  \u001b[0m | \u001b[95m 521.9   \u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 105.4357 - acc: 0.2764 - val_loss: 69.6548 - val_acc: 0.2648\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 45.9578 - acc: 0.4917 - val_loss: 41.1512 - val_acc: 0.5790\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 28.3973 - acc: 0.7073 - val_loss: 28.1801 - val_acc: 0.7057\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 22.7654 - acc: 0.7651 - val_loss: 22.8764 - val_acc: 0.7592\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 20.0684 - acc: 0.7892 - val_loss: 19.9490 - val_acc: 0.7916\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 18.2971 - acc: 0.8063 - val_loss: 17.8381 - val_acc: 0.8105\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 17.0544 - acc: 0.8177 - val_loss: 16.7682 - val_acc: 0.8231\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 16.0993 - acc: 0.8276 - val_loss: 15.8033 - val_acc: 0.8324\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 17s 27us/step - loss: 15.3018 - acc: 0.8364 - val_loss: 14.9672 - val_acc: 0.8419\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 14.6540 - acc: 0.8438 - val_loss: 15.5220 - val_acc: 0.8340\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 17s 27us/step - loss: 14.1317 - acc: 0.8492 - val_loss: 14.3597 - val_acc: 0.8481\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 13.6011 - acc: 0.8552 - val_loss: 13.2871 - val_acc: 0.8593\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 13.1984 - acc: 0.8594 - val_loss: 13.4568 - val_acc: 0.8548\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 12.7913 - acc: 0.8638 - val_loss: 12.8313 - val_acc: 0.8658\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 12.4433 - acc: 0.8670 - val_loss: 12.1337 - val_acc: 0.8741\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 12.1742 - acc: 0.8700 - val_loss: 11.8383 - val_acc: 0.8751\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 11.8537 - acc: 0.8741 - val_loss: 11.7610 - val_acc: 0.8741\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 11.6027 - acc: 0.8764 - val_loss: 12.8031 - val_acc: 0.8644\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 11.4066 - acc: 0.8778 - val_loss: 10.7776 - val_acc: 0.8814\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 11.1321 - acc: 0.8805 - val_loss: 11.2186 - val_acc: 0.8822\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 27s 44us/step - loss: 66.6809 - acc: 0.3774 - val_loss: 58.0656 - val_acc: 0.4322\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 24s 39us/step - loss: 27.5600 - acc: 0.7156 - val_loss: 28.8797 - val_acc: 0.7198\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 24s 39us/step - loss: 17.8496 - acc: 0.8161 - val_loss: 17.1106 - val_acc: 0.8260\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 13.5929 - acc: 0.8583 - val_loss: 13.6147 - val_acc: 0.8527\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 11.2974 - acc: 0.8815 - val_loss: 12.4739 - val_acc: 0.8783\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 24s 39us/step - loss: 9.8041 - acc: 0.8960 - val_loss: 10.2499 - val_acc: 0.8968\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 8.6946 - acc: 0.9075 - val_loss: 8.8685 - val_acc: 0.9012\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 24s 39us/step - loss: 7.9072 - acc: 0.9149 - val_loss: 8.5583 - val_acc: 0.9018\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 7.2876 - acc: 0.9209 - val_loss: 7.2592 - val_acc: 0.9265\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 24s 39us/step - loss: 6.8127 - acc: 0.9248 - val_loss: 7.1387 - val_acc: 0.9240\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 24s 39us/step - loss: 6.4357 - acc: 0.9288 - val_loss: 6.9104 - val_acc: 0.9237\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 24s 39us/step - loss: 6.1044 - acc: 0.9323 - val_loss: 5.8676 - val_acc: 0.9357\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 24s 39us/step - loss: 5.7527 - acc: 0.9348 - val_loss: 5.7022 - val_acc: 0.9423\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 23s 38us/step - loss: 5.5349 - acc: 0.9373 - val_loss: 5.4891 - val_acc: 0.9385\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 5.3292 - acc: 0.9394 - val_loss: 5.3980 - val_acc: 0.9372\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 5.1085 - acc: 0.9409 - val_loss: 5.4224 - val_acc: 0.9406\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 4.8804 - acc: 0.9430 - val_loss: 5.7407 - val_acc: 0.9357\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 22s 37us/step - loss: 4.7205 - acc: 0.9448 - val_loss: 4.3463 - val_acc: 0.9520\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 22s 37us/step - loss: 4.5825 - acc: 0.9461 - val_loss: 4.0772 - val_acc: 0.9423\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 4.4840 - acc: 0.9468 - val_loss: 4.0054 - val_acc: 0.9582\n",
            "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.9582  \u001b[0m | \u001b[95m 800.0   \u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 81.5221 - acc: 0.3524 - val_loss: 50.9117 - val_acc: 0.4907\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 27.4921 - acc: 0.7158 - val_loss: 24.4220 - val_acc: 0.7553\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 17.1929 - acc: 0.8234 - val_loss: 16.6285 - val_acc: 0.8356\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 13.7079 - acc: 0.8580 - val_loss: 13.6970 - val_acc: 0.8581\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 11.7453 - acc: 0.8782 - val_loss: 12.1978 - val_acc: 0.8692\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 10.4180 - acc: 0.8907 - val_loss: 10.3409 - val_acc: 0.8930\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 9.5453 - acc: 0.8989 - val_loss: 9.7884 - val_acc: 0.9022\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 8.8240 - acc: 0.9066 - val_loss: 8.7696 - val_acc: 0.9123\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 8.3228 - acc: 0.9111 - val_loss: 9.7883 - val_acc: 0.9043\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 22s 37us/step - loss: 7.8911 - acc: 0.9156 - val_loss: 7.7624 - val_acc: 0.9180\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 7.4629 - acc: 0.9197 - val_loss: 7.5963 - val_acc: 0.9156\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 7.1520 - acc: 0.9224 - val_loss: 7.5837 - val_acc: 0.9214\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 6.9018 - acc: 0.9248 - val_loss: 6.4207 - val_acc: 0.9274\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 6.6326 - acc: 0.9272 - val_loss: 6.5983 - val_acc: 0.9310\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 6.4290 - acc: 0.9292 - val_loss: 6.2349 - val_acc: 0.9344\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 6.2451 - acc: 0.9312 - val_loss: 6.0126 - val_acc: 0.9378\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 6.0866 - acc: 0.9326 - val_loss: 5.9447 - val_acc: 0.9315\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 5.9197 - acc: 0.9344 - val_loss: 5.6353 - val_acc: 0.9357\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 5.8043 - acc: 0.9349 - val_loss: 5.4237 - val_acc: 0.9407\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 5.6387 - acc: 0.9365 - val_loss: 5.5696 - val_acc: 0.9414\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 27s 44us/step - loss: 70.5035 - acc: 0.3436 - val_loss: 57.2452 - val_acc: 0.4202\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 23s 38us/step - loss: 33.5815 - acc: 0.6488 - val_loss: 34.1550 - val_acc: 0.6427\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 23s 37us/step - loss: 20.9055 - acc: 0.7859 - val_loss: 22.1333 - val_acc: 0.7785\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 23s 37us/step - loss: 15.3613 - acc: 0.8397 - val_loss: 16.3960 - val_acc: 0.8270\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 12.4200 - acc: 0.8696 - val_loss: 14.1026 - val_acc: 0.8564\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 10.4519 - acc: 0.8888 - val_loss: 10.0008 - val_acc: 0.8960\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 9.2545 - acc: 0.9009 - val_loss: 9.1366 - val_acc: 0.8963\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 22s 37us/step - loss: 8.3119 - acc: 0.9113 - val_loss: 8.6038 - val_acc: 0.9068\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 7.6291 - acc: 0.9173 - val_loss: 8.2137 - val_acc: 0.9034\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 7.0899 - acc: 0.9225 - val_loss: 6.7675 - val_acc: 0.9294\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 6.5970 - acc: 0.9275 - val_loss: 7.3666 - val_acc: 0.9199\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 6.2745 - acc: 0.9304 - val_loss: 6.4316 - val_acc: 0.9304\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 5.9558 - acc: 0.9334 - val_loss: 5.6545 - val_acc: 0.9318\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 5.6993 - acc: 0.9353 - val_loss: 5.2628 - val_acc: 0.9446\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 5.4566 - acc: 0.9380 - val_loss: 5.9574 - val_acc: 0.9334\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 5.2288 - acc: 0.9402 - val_loss: 5.1984 - val_acc: 0.9357\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 23s 39us/step - loss: 5.0565 - acc: 0.9418 - val_loss: 5.8314 - val_acc: 0.9371\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 23s 38us/step - loss: 4.8556 - acc: 0.9433 - val_loss: 4.2232 - val_acc: 0.9517\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 4.7226 - acc: 0.9449 - val_loss: 5.4730 - val_acc: 0.9395\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 23s 37us/step - loss: 4.5564 - acc: 0.9456 - val_loss: 5.1327 - val_acc: 0.9370\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 24s 39us/step - loss: 76.1575 - acc: 0.3770 - val_loss: 53.4114 - val_acc: 0.4640\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 25.9198 - acc: 0.7362 - val_loss: 23.4878 - val_acc: 0.7572\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 16.5008 - acc: 0.8305 - val_loss: 17.3306 - val_acc: 0.8217\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 12.9610 - acc: 0.8660 - val_loss: 12.7480 - val_acc: 0.8710\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 10.9873 - acc: 0.8852 - val_loss: 10.6007 - val_acc: 0.8857\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 9.6957 - acc: 0.8975 - val_loss: 10.0963 - val_acc: 0.8973\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 8.8244 - acc: 0.9057 - val_loss: 8.6182 - val_acc: 0.9097\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 8.1579 - acc: 0.9122 - val_loss: 8.7591 - val_acc: 0.9091\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 7.6010 - acc: 0.9178 - val_loss: 7.8705 - val_acc: 0.9088\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 7.1599 - acc: 0.9217 - val_loss: 7.1126 - val_acc: 0.9256\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 6.8133 - acc: 0.9248 - val_loss: 6.6500 - val_acc: 0.9222\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 6.5030 - acc: 0.9277 - val_loss: 6.1041 - val_acc: 0.9298\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 6.2725 - acc: 0.9298 - val_loss: 6.9870 - val_acc: 0.9164\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 5.9945 - acc: 0.9322 - val_loss: 5.6845 - val_acc: 0.9330\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 5.8167 - acc: 0.9340 - val_loss: 5.4620 - val_acc: 0.9368\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 5.6722 - acc: 0.9354 - val_loss: 5.8679 - val_acc: 0.9379\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 5.4546 - acc: 0.9372 - val_loss: 5.5480 - val_acc: 0.9327\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 5.3630 - acc: 0.9381 - val_loss: 5.1631 - val_acc: 0.9411\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 5.2050 - acc: 0.9396 - val_loss: 5.4039 - val_acc: 0.9409\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 5.0772 - acc: 0.9408 - val_loss: 4.8984 - val_acc: 0.9418\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 22s 37us/step - loss: 89.0064 - acc: 0.3046 - val_loss: 55.8394 - val_acc: 0.3746\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 33.6688 - acc: 0.6451 - val_loss: 27.4425 - val_acc: 0.7176\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 19.8206 - acc: 0.7973 - val_loss: 18.1547 - val_acc: 0.8066\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 15.6563 - acc: 0.8362 - val_loss: 15.4275 - val_acc: 0.8383\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 13.4623 - acc: 0.8580 - val_loss: 13.9404 - val_acc: 0.8528\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 12.0678 - acc: 0.8719 - val_loss: 11.8963 - val_acc: 0.8697\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 11.0991 - acc: 0.8820 - val_loss: 11.2960 - val_acc: 0.8794\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 10.3179 - acc: 0.8895 - val_loss: 9.9630 - val_acc: 0.8956\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 9.6792 - acc: 0.8966 - val_loss: 9.5517 - val_acc: 0.8943\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 9.1857 - acc: 0.9015 - val_loss: 9.7391 - val_acc: 0.8994\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 8.7512 - acc: 0.9054 - val_loss: 8.2705 - val_acc: 0.9116\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 8.3834 - acc: 0.9094 - val_loss: 7.8726 - val_acc: 0.9095\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 8.0993 - acc: 0.9119 - val_loss: 7.9160 - val_acc: 0.9148\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 7.8394 - acc: 0.9144 - val_loss: 8.3293 - val_acc: 0.9176\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 7.5568 - acc: 0.9171 - val_loss: 7.3048 - val_acc: 0.9172\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 7.4011 - acc: 0.9189 - val_loss: 7.2988 - val_acc: 0.9175\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 7.1779 - acc: 0.9207 - val_loss: 6.5620 - val_acc: 0.9303\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 6.9940 - acc: 0.9229 - val_loss: 7.5165 - val_acc: 0.9214\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 6.8255 - acc: 0.9238 - val_loss: 6.7201 - val_acc: 0.9224\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 6.7125 - acc: 0.9248 - val_loss: 6.6953 - val_acc: 0.9231\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 25s 42us/step - loss: 68.9384 - acc: 0.3805 - val_loss: 49.8952 - val_acc: 0.5102\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 29.2383 - acc: 0.7025 - val_loss: 28.2386 - val_acc: 0.7239\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 23s 38us/step - loss: 18.3125 - acc: 0.8110 - val_loss: 17.2790 - val_acc: 0.8246\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 13.7090 - acc: 0.8557 - val_loss: 14.8828 - val_acc: 0.8364\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 11.3063 - acc: 0.8807 - val_loss: 12.8405 - val_acc: 0.8758\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 22s 37us/step - loss: 9.7854 - acc: 0.8964 - val_loss: 10.9012 - val_acc: 0.8877\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 23s 37us/step - loss: 8.6260 - acc: 0.9079 - val_loss: 9.1119 - val_acc: 0.9023\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 23s 37us/step - loss: 7.8899 - acc: 0.9149 - val_loss: 7.7748 - val_acc: 0.9149\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 23s 38us/step - loss: 7.3143 - acc: 0.9198 - val_loss: 7.4868 - val_acc: 0.9115\n",
            "Epoch 10/20\n",
            "541800/607500 [=========================>....] - ETA: 2s - loss: 6.8294 - acc: 0.9243Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGe09MH3TGPc",
        "colab_type": "code",
        "outputId": "8af3e62d-2828-47fa-c530-a3afb43999bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print('Final result: ', bayes_optimizer.max)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final result:  {'target': 0.9581975173950196, 'params': {'units': 799.999308421973}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3IaqhJqmV_5",
        "colab_type": "text"
      },
      "source": [
        "### Bayesian Optimization\n",
        "http://research.sualab.com/introduction/practice/2019/02/19/bayesian-optimization-overview-1.html<br>\n",
        "http://research.sualab.com/introduction/practice/2019/04/01/bayesian-optimization-overview-2.html<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNwDuaRWmWg4",
        "colab_type": "text"
      },
      "source": [
        "### Swish Activation\n",
        "https://www.machinecurve.com/index.php/2019/05/30/why-swish-could-perform-better-than-relu/#todays-activation-functions"
      ]
    }
  ]
}