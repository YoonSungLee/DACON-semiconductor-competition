{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('dataset/train.csv')\n",
    "df_test = pd.read_csv('dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_1</th>\n",
       "      <th>layer_2</th>\n",
       "      <th>layer_3</th>\n",
       "      <th>layer_4</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.254551</td>\n",
       "      <td>0.258823</td>\n",
       "      <td>0.254659</td>\n",
       "      <td>0.252085</td>\n",
       "      <td>0.247678</td>\n",
       "      <td>0.253614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354750</td>\n",
       "      <td>0.369223</td>\n",
       "      <td>0.388184</td>\n",
       "      <td>0.408496</td>\n",
       "      <td>0.414564</td>\n",
       "      <td>0.429403</td>\n",
       "      <td>0.419225</td>\n",
       "      <td>0.443250</td>\n",
       "      <td>0.433414</td>\n",
       "      <td>0.465502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.205062</td>\n",
       "      <td>0.225544</td>\n",
       "      <td>0.217758</td>\n",
       "      <td>0.202169</td>\n",
       "      <td>0.199633</td>\n",
       "      <td>0.207380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557203</td>\n",
       "      <td>0.573656</td>\n",
       "      <td>0.587998</td>\n",
       "      <td>0.612754</td>\n",
       "      <td>0.627825</td>\n",
       "      <td>0.633393</td>\n",
       "      <td>0.637706</td>\n",
       "      <td>0.625981</td>\n",
       "      <td>0.653231</td>\n",
       "      <td>0.637853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.189196</td>\n",
       "      <td>0.165869</td>\n",
       "      <td>0.177655</td>\n",
       "      <td>0.156822</td>\n",
       "      <td>0.175094</td>\n",
       "      <td>0.177755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699864</td>\n",
       "      <td>0.708688</td>\n",
       "      <td>0.721982</td>\n",
       "      <td>0.713464</td>\n",
       "      <td>0.743030</td>\n",
       "      <td>0.741709</td>\n",
       "      <td>0.747743</td>\n",
       "      <td>0.746037</td>\n",
       "      <td>0.737356</td>\n",
       "      <td>0.750391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>0.131003</td>\n",
       "      <td>0.120076</td>\n",
       "      <td>0.138975</td>\n",
       "      <td>0.117931</td>\n",
       "      <td>0.130566</td>\n",
       "      <td>0.131262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764786</td>\n",
       "      <td>0.763788</td>\n",
       "      <td>0.770017</td>\n",
       "      <td>0.787571</td>\n",
       "      <td>0.778866</td>\n",
       "      <td>0.776969</td>\n",
       "      <td>0.774712</td>\n",
       "      <td>0.801526</td>\n",
       "      <td>0.805305</td>\n",
       "      <td>0.784057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>0.091033</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>0.108125</td>\n",
       "      <td>0.080405</td>\n",
       "      <td>0.105917</td>\n",
       "      <td>0.077083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786677</td>\n",
       "      <td>0.802271</td>\n",
       "      <td>0.806557</td>\n",
       "      <td>0.799614</td>\n",
       "      <td>0.789333</td>\n",
       "      <td>0.804087</td>\n",
       "      <td>0.787763</td>\n",
       "      <td>0.794948</td>\n",
       "      <td>0.819105</td>\n",
       "      <td>0.801781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   layer_1  layer_2  layer_3  layer_4         0         1         2         3  \\\n",
       "0       10       10       10       10  0.254551  0.258823  0.254659  0.252085   \n",
       "1       10       10       10       20  0.205062  0.225544  0.217758  0.202169   \n",
       "2       10       10       10       30  0.189196  0.165869  0.177655  0.156822   \n",
       "3       10       10       10       40  0.131003  0.120076  0.138975  0.117931   \n",
       "4       10       10       10       50  0.091033  0.086893  0.108125  0.080405   \n",
       "\n",
       "          4         5  ...       216       217       218       219       220  \\\n",
       "0  0.247678  0.253614  ...  0.354750  0.369223  0.388184  0.408496  0.414564   \n",
       "1  0.199633  0.207380  ...  0.557203  0.573656  0.587998  0.612754  0.627825   \n",
       "2  0.175094  0.177755  ...  0.699864  0.708688  0.721982  0.713464  0.743030   \n",
       "3  0.130566  0.131262  ...  0.764786  0.763788  0.770017  0.787571  0.778866   \n",
       "4  0.105917  0.077083  ...  0.786677  0.802271  0.806557  0.799614  0.789333   \n",
       "\n",
       "        221       222       223       224       225  \n",
       "0  0.429403  0.419225  0.443250  0.433414  0.465502  \n",
       "1  0.633393  0.637706  0.625981  0.653231  0.637853  \n",
       "2  0.741709  0.747743  0.746037  0.737356  0.750391  \n",
       "3  0.776969  0.774712  0.801526  0.805305  0.784057  \n",
       "4  0.804087  0.787763  0.794948  0.819105  0.801781  \n",
       "\n",
       "[5 rows x 230 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 810000 entries, 0 to 809999\n",
      "Columns: 230 entries, layer_1 to 225\n",
      "dtypes: float64(226), int64(4)\n",
      "memory usage: 1.4 GB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_1</th>\n",
       "      <th>layer_2</th>\n",
       "      <th>layer_3</th>\n",
       "      <th>layer_4</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>155.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>0.292653</td>\n",
       "      <td>0.292893</td>\n",
       "      <td>0.293125</td>\n",
       "      <td>0.293363</td>\n",
       "      <td>0.293666</td>\n",
       "      <td>0.293994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600336</td>\n",
       "      <td>0.606206</td>\n",
       "      <td>0.612238</td>\n",
       "      <td>0.618456</td>\n",
       "      <td>0.623942</td>\n",
       "      <td>0.625395</td>\n",
       "      <td>0.627100</td>\n",
       "      <td>0.628997</td>\n",
       "      <td>0.631166</td>\n",
       "      <td>0.633594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>86.554468</td>\n",
       "      <td>86.554468</td>\n",
       "      <td>86.554468</td>\n",
       "      <td>86.554468</td>\n",
       "      <td>0.181642</td>\n",
       "      <td>0.181857</td>\n",
       "      <td>0.182055</td>\n",
       "      <td>0.182197</td>\n",
       "      <td>0.182361</td>\n",
       "      <td>0.182529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199727</td>\n",
       "      <td>0.198644</td>\n",
       "      <td>0.197473</td>\n",
       "      <td>0.196177</td>\n",
       "      <td>0.195028</td>\n",
       "      <td>0.194909</td>\n",
       "      <td>0.194730</td>\n",
       "      <td>0.194493</td>\n",
       "      <td>0.194146</td>\n",
       "      <td>0.193725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>-0.014902</td>\n",
       "      <td>-0.014798</td>\n",
       "      <td>-0.014897</td>\n",
       "      <td>-0.014709</td>\n",
       "      <td>-0.014903</td>\n",
       "      <td>-0.014662</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011992</td>\n",
       "      <td>-0.008661</td>\n",
       "      <td>-0.011430</td>\n",
       "      <td>-0.009827</td>\n",
       "      <td>-0.007632</td>\n",
       "      <td>-0.007411</td>\n",
       "      <td>-0.007073</td>\n",
       "      <td>-0.007101</td>\n",
       "      <td>-0.005519</td>\n",
       "      <td>-0.006074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.135139</td>\n",
       "      <td>0.135180</td>\n",
       "      <td>0.135258</td>\n",
       "      <td>0.135478</td>\n",
       "      <td>0.135585</td>\n",
       "      <td>0.135705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469345</td>\n",
       "      <td>0.476970</td>\n",
       "      <td>0.484727</td>\n",
       "      <td>0.492739</td>\n",
       "      <td>0.500232</td>\n",
       "      <td>0.501650</td>\n",
       "      <td>0.503811</td>\n",
       "      <td>0.506252</td>\n",
       "      <td>0.509036</td>\n",
       "      <td>0.512067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>155.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>0.286510</td>\n",
       "      <td>0.286874</td>\n",
       "      <td>0.287194</td>\n",
       "      <td>0.287553</td>\n",
       "      <td>0.287830</td>\n",
       "      <td>0.288151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643685</td>\n",
       "      <td>0.649886</td>\n",
       "      <td>0.656258</td>\n",
       "      <td>0.662860</td>\n",
       "      <td>0.668727</td>\n",
       "      <td>0.670287</td>\n",
       "      <td>0.672145</td>\n",
       "      <td>0.674283</td>\n",
       "      <td>0.676692</td>\n",
       "      <td>0.679339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>230.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>0.435696</td>\n",
       "      <td>0.435956</td>\n",
       "      <td>0.436112</td>\n",
       "      <td>0.436326</td>\n",
       "      <td>0.436634</td>\n",
       "      <td>0.437142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760737</td>\n",
       "      <td>0.765462</td>\n",
       "      <td>0.770333</td>\n",
       "      <td>0.775263</td>\n",
       "      <td>0.779555</td>\n",
       "      <td>0.780846</td>\n",
       "      <td>0.782387</td>\n",
       "      <td>0.783979</td>\n",
       "      <td>0.785774</td>\n",
       "      <td>0.787759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>0.748205</td>\n",
       "      <td>0.753103</td>\n",
       "      <td>0.749494</td>\n",
       "      <td>0.747389</td>\n",
       "      <td>0.748827</td>\n",
       "      <td>0.750392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.935423</td>\n",
       "      <td>0.934867</td>\n",
       "      <td>0.938873</td>\n",
       "      <td>0.937817</td>\n",
       "      <td>0.942214</td>\n",
       "      <td>0.940367</td>\n",
       "      <td>0.940387</td>\n",
       "      <td>0.941548</td>\n",
       "      <td>0.942411</td>\n",
       "      <td>0.943648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             layer_1        layer_2        layer_3        layer_4  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean      155.000000     155.000000     155.000000     155.000000   \n",
       "std        86.554468      86.554468      86.554468      86.554468   \n",
       "min        10.000000      10.000000      10.000000      10.000000   \n",
       "25%        80.000000      80.000000      80.000000      80.000000   \n",
       "50%       155.000000     155.000000     155.000000     155.000000   \n",
       "75%       230.000000     230.000000     230.000000     230.000000   \n",
       "max       300.000000     300.000000     300.000000     300.000000   \n",
       "\n",
       "                   0              1              2              3  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        0.292653       0.292893       0.293125       0.293363   \n",
       "std         0.181642       0.181857       0.182055       0.182197   \n",
       "min        -0.014902      -0.014798      -0.014897      -0.014709   \n",
       "25%         0.135139       0.135180       0.135258       0.135478   \n",
       "50%         0.286510       0.286874       0.287194       0.287553   \n",
       "75%         0.435696       0.435956       0.436112       0.436326   \n",
       "max         0.748205       0.753103       0.749494       0.747389   \n",
       "\n",
       "                   4              5  ...            216            217  \\\n",
       "count  810000.000000  810000.000000  ...  810000.000000  810000.000000   \n",
       "mean        0.293666       0.293994  ...       0.600336       0.606206   \n",
       "std         0.182361       0.182529  ...       0.199727       0.198644   \n",
       "min        -0.014903      -0.014662  ...      -0.011992      -0.008661   \n",
       "25%         0.135585       0.135705  ...       0.469345       0.476970   \n",
       "50%         0.287830       0.288151  ...       0.643685       0.649886   \n",
       "75%         0.436634       0.437142  ...       0.760737       0.765462   \n",
       "max         0.748827       0.750392  ...       0.935423       0.934867   \n",
       "\n",
       "                 218            219            220            221  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        0.612238       0.618456       0.623942       0.625395   \n",
       "std         0.197473       0.196177       0.195028       0.194909   \n",
       "min        -0.011430      -0.009827      -0.007632      -0.007411   \n",
       "25%         0.484727       0.492739       0.500232       0.501650   \n",
       "50%         0.656258       0.662860       0.668727       0.670287   \n",
       "75%         0.770333       0.775263       0.779555       0.780846   \n",
       "max         0.938873       0.937817       0.942214       0.940367   \n",
       "\n",
       "                 222            223            224            225  \n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000  \n",
       "mean        0.627100       0.628997       0.631166       0.633594  \n",
       "std         0.194730       0.194493       0.194146       0.193725  \n",
       "min        -0.007073      -0.007101      -0.005519      -0.006074  \n",
       "25%         0.503811       0.506252       0.509036       0.512067  \n",
       "50%         0.672145       0.674283       0.676692       0.679339  \n",
       "75%         0.782387       0.783979       0.785774       0.787759  \n",
       "max         0.940387       0.941548       0.942411       0.943648  \n",
       "\n",
       "[8 rows x 230 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "layer_1    0\n",
       "layer_2    0\n",
       "layer_3    0\n",
       "layer_4    0\n",
       "0          0\n",
       "          ..\n",
       "221        0\n",
       "222        0\n",
       "223        0\n",
       "224        0\n",
       "225        0\n",
       "Length: 230, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "counts = 0\n",
    "for i in null_count:\n",
    "    if i != 0:\n",
    "        counts += 1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.535410</td>\n",
       "      <td>0.520775</td>\n",
       "      <td>0.494087</td>\n",
       "      <td>0.465134</td>\n",
       "      <td>0.430339</td>\n",
       "      <td>0.401751</td>\n",
       "      <td>0.355986</td>\n",
       "      <td>0.326427</td>\n",
       "      <td>0.282340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748339</td>\n",
       "      <td>0.757575</td>\n",
       "      <td>0.768130</td>\n",
       "      <td>0.777062</td>\n",
       "      <td>0.769173</td>\n",
       "      <td>0.768253</td>\n",
       "      <td>0.738704</td>\n",
       "      <td>0.739460</td>\n",
       "      <td>0.702139</td>\n",
       "      <td>0.702238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.351099</td>\n",
       "      <td>0.398179</td>\n",
       "      <td>0.413809</td>\n",
       "      <td>0.418529</td>\n",
       "      <td>0.433257</td>\n",
       "      <td>0.455410</td>\n",
       "      <td>0.451065</td>\n",
       "      <td>0.464230</td>\n",
       "      <td>0.476011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333931</td>\n",
       "      <td>0.276307</td>\n",
       "      <td>0.211513</td>\n",
       "      <td>0.159223</td>\n",
       "      <td>0.110982</td>\n",
       "      <td>0.083130</td>\n",
       "      <td>0.099780</td>\n",
       "      <td>0.145420</td>\n",
       "      <td>0.260501</td>\n",
       "      <td>0.343857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.490537</td>\n",
       "      <td>0.435958</td>\n",
       "      <td>0.413428</td>\n",
       "      <td>0.355796</td>\n",
       "      <td>0.335777</td>\n",
       "      <td>0.299944</td>\n",
       "      <td>0.242745</td>\n",
       "      <td>0.210555</td>\n",
       "      <td>0.180739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709371</td>\n",
       "      <td>0.746826</td>\n",
       "      <td>0.781436</td>\n",
       "      <td>0.788292</td>\n",
       "      <td>0.828630</td>\n",
       "      <td>0.835166</td>\n",
       "      <td>0.845859</td>\n",
       "      <td>0.846032</td>\n",
       "      <td>0.836724</td>\n",
       "      <td>0.846779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.051634</td>\n",
       "      <td>0.075802</td>\n",
       "      <td>0.133983</td>\n",
       "      <td>0.154546</td>\n",
       "      <td>0.209387</td>\n",
       "      <td>0.251700</td>\n",
       "      <td>0.287552</td>\n",
       "      <td>0.333000</td>\n",
       "      <td>0.340617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075046</td>\n",
       "      <td>0.056651</td>\n",
       "      <td>0.079884</td>\n",
       "      <td>0.147469</td>\n",
       "      <td>0.213112</td>\n",
       "      <td>0.298096</td>\n",
       "      <td>0.382823</td>\n",
       "      <td>0.489381</td>\n",
       "      <td>0.562383</td>\n",
       "      <td>0.599247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.154031</td>\n",
       "      <td>0.201728</td>\n",
       "      <td>0.270414</td>\n",
       "      <td>0.283799</td>\n",
       "      <td>0.343050</td>\n",
       "      <td>0.340233</td>\n",
       "      <td>0.379244</td>\n",
       "      <td>0.378511</td>\n",
       "      <td>0.373017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255070</td>\n",
       "      <td>0.242396</td>\n",
       "      <td>0.271287</td>\n",
       "      <td>0.328828</td>\n",
       "      <td>0.397950</td>\n",
       "      <td>0.486436</td>\n",
       "      <td>0.530573</td>\n",
       "      <td>0.582752</td>\n",
       "      <td>0.637296</td>\n",
       "      <td>0.637238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id         0         1         2         3         4         5         6  \\\n",
       "0   0  0.535410  0.520775  0.494087  0.465134  0.430339  0.401751  0.355986   \n",
       "1   1  0.351099  0.398179  0.413809  0.418529  0.433257  0.455410  0.451065   \n",
       "2   2  0.490537  0.435958  0.413428  0.355796  0.335777  0.299944  0.242745   \n",
       "3   3  0.051634  0.075802  0.133983  0.154546  0.209387  0.251700  0.287552   \n",
       "4   4  0.154031  0.201728  0.270414  0.283799  0.343050  0.340233  0.379244   \n",
       "\n",
       "          7         8  ...       216       217       218       219       220  \\\n",
       "0  0.326427  0.282340  ...  0.748339  0.757575  0.768130  0.777062  0.769173   \n",
       "1  0.464230  0.476011  ...  0.333931  0.276307  0.211513  0.159223  0.110982   \n",
       "2  0.210555  0.180739  ...  0.709371  0.746826  0.781436  0.788292  0.828630   \n",
       "3  0.333000  0.340617  ...  0.075046  0.056651  0.079884  0.147469  0.213112   \n",
       "4  0.378511  0.373017  ...  0.255070  0.242396  0.271287  0.328828  0.397950   \n",
       "\n",
       "        221       222       223       224       225  \n",
       "0  0.768253  0.738704  0.739460  0.702139  0.702238  \n",
       "1  0.083130  0.099780  0.145420  0.260501  0.343857  \n",
       "2  0.835166  0.845859  0.846032  0.836724  0.846779  \n",
       "3  0.298096  0.382823  0.489381  0.562383  0.599247  \n",
       "4  0.486436  0.530573  0.582752  0.637296  0.637238  \n",
       "\n",
       "[5 rows x 227 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "counts = 0\n",
    "for i in null_count:\n",
    "    if i != 0:\n",
    "        counts += 1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mycom\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>...</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>id</th>\n",
       "      <th>layer_1</th>\n",
       "      <th>layer_2</th>\n",
       "      <th>layer_3</th>\n",
       "      <th>layer_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.254551</td>\n",
       "      <td>0.258823</td>\n",
       "      <td>0.253870</td>\n",
       "      <td>0.164719</td>\n",
       "      <td>0.151643</td>\n",
       "      <td>0.164846</td>\n",
       "      <td>0.170996</td>\n",
       "      <td>0.148866</td>\n",
       "      <td>0.158070</td>\n",
       "      <td>0.157883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153949</td>\n",
       "      <td>0.165214</td>\n",
       "      <td>0.147266</td>\n",
       "      <td>0.170829</td>\n",
       "      <td>0.145234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.205062</td>\n",
       "      <td>0.225544</td>\n",
       "      <td>0.198726</td>\n",
       "      <td>0.081874</td>\n",
       "      <td>0.069062</td>\n",
       "      <td>0.071412</td>\n",
       "      <td>0.065123</td>\n",
       "      <td>0.082904</td>\n",
       "      <td>0.060860</td>\n",
       "      <td>0.079682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066174</td>\n",
       "      <td>0.072723</td>\n",
       "      <td>0.070791</td>\n",
       "      <td>0.075919</td>\n",
       "      <td>0.082080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.189196</td>\n",
       "      <td>0.165869</td>\n",
       "      <td>0.148831</td>\n",
       "      <td>0.066248</td>\n",
       "      <td>0.049662</td>\n",
       "      <td>0.055313</td>\n",
       "      <td>0.062557</td>\n",
       "      <td>0.070852</td>\n",
       "      <td>0.080750</td>\n",
       "      <td>0.074811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049527</td>\n",
       "      <td>0.064183</td>\n",
       "      <td>0.044467</td>\n",
       "      <td>0.056002</td>\n",
       "      <td>0.059647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.131003</td>\n",
       "      <td>0.120076</td>\n",
       "      <td>0.099958</td>\n",
       "      <td>0.117332</td>\n",
       "      <td>0.133968</td>\n",
       "      <td>0.137508</td>\n",
       "      <td>0.120664</td>\n",
       "      <td>0.126985</td>\n",
       "      <td>0.136193</td>\n",
       "      <td>0.134217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098705</td>\n",
       "      <td>0.103562</td>\n",
       "      <td>0.115785</td>\n",
       "      <td>0.121671</td>\n",
       "      <td>0.104612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.091033</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>0.086023</td>\n",
       "      <td>0.214727</td>\n",
       "      <td>0.242656</td>\n",
       "      <td>0.237504</td>\n",
       "      <td>0.229492</td>\n",
       "      <td>0.241289</td>\n",
       "      <td>0.252776</td>\n",
       "      <td>0.246082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200239</td>\n",
       "      <td>0.185882</td>\n",
       "      <td>0.216706</td>\n",
       "      <td>0.211741</td>\n",
       "      <td>0.202940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1        10       100       101       102       103  \\\n",
       "0  0.254551  0.258823  0.253870  0.164719  0.151643  0.164846  0.170996   \n",
       "1  0.205062  0.225544  0.198726  0.081874  0.069062  0.071412  0.065123   \n",
       "2  0.189196  0.165869  0.148831  0.066248  0.049662  0.055313  0.062557   \n",
       "3  0.131003  0.120076  0.099958  0.117332  0.133968  0.137508  0.120664   \n",
       "4  0.091033  0.086893  0.086023  0.214727  0.242656  0.237504  0.229492   \n",
       "\n",
       "        104       105       106  ...        95        96        97        98  \\\n",
       "0  0.148866  0.158070  0.157883  ...  0.153949  0.165214  0.147266  0.170829   \n",
       "1  0.082904  0.060860  0.079682  ...  0.066174  0.072723  0.070791  0.075919   \n",
       "2  0.070852  0.080750  0.074811  ...  0.049527  0.064183  0.044467  0.056002   \n",
       "3  0.126985  0.136193  0.134217  ...  0.098705  0.103562  0.115785  0.121671   \n",
       "4  0.241289  0.252776  0.246082  ...  0.200239  0.185882  0.216706  0.211741   \n",
       "\n",
       "         99  id  layer_1  layer_2  layer_3  layer_4  \n",
       "0  0.145234 NaN     10.0     10.0     10.0     10.0  \n",
       "1  0.082080 NaN     10.0     10.0     10.0     20.0  \n",
       "2  0.059647 NaN     10.0     10.0     10.0     30.0  \n",
       "3  0.104612 NaN     10.0     10.0     10.0     40.0  \n",
       "4  0.202940 NaN     10.0     10.0     10.0     50.0  \n",
       "\n",
       "[5 rows x 231 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total = pd.concat((df_train, df_test), ignore_index=True)\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 820000 entries, 0 to 819999\n",
      "Columns: 231 entries, 0 to layer_4\n",
      "dtypes: float64(231)\n",
      "memory usage: 1.4 GB\n"
     ]
    }
   ],
   "source": [
    "df_total.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4 layers\n",
    "* 160 units, relu\n",
    "* Adam\n",
    "* epochs 20\n",
    "* batch_size 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독립변수와 종속변수를 분리합니다.\n",
    "\n",
    "train_X = df_train.iloc[:,4:]\n",
    "train_Y = df_train.iloc[:,0:4]\n",
    "test_X = df_test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mycom\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mycom\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mycom\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 케라스를 통해 모델 생성을 시작합니다.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=160, activation='relu', input_dim=226))\n",
    "model.add(Dense(units=160, activation='relu'))\n",
    "model.add(Dense(units=160, activation='relu'))\n",
    "model.add(Dense(units=4, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mycom\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델을 컴파일합니다.\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mycom\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mycom\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mycom\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 769500 samples, validate on 40500 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\mycom\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mycom\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mycom\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mycom\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mycom\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "769500/769500 [==============================] - 9s 12us/step - loss: 95.5491 - mean_absolute_error: 95.5491 - val_loss: 95.7871 - val_mean_absolute_error: 95.7871\n",
      "Epoch 2/20\n",
      "769500/769500 [==============================] - 8s 10us/step - loss: 74.7927 - mean_absolute_error: 74.7927 - val_loss: 94.0377 - val_mean_absolute_error: 94.0377\n",
      "Epoch 3/20\n",
      "769500/769500 [==============================] - 8s 10us/step - loss: 74.0612 - mean_absolute_error: 74.0612 - val_loss: 93.3356 - val_mean_absolute_error: 93.3356\n",
      "Epoch 4/20\n",
      "769500/769500 [==============================] - 7s 10us/step - loss: 72.7184 - mean_absolute_error: 72.7184 - val_loss: 92.9596 - val_mean_absolute_error: 92.9596\n",
      "Epoch 5/20\n",
      "769500/769500 [==============================] - 7s 10us/step - loss: 71.0207 - mean_absolute_error: 71.0207 - val_loss: 92.3258 - val_mean_absolute_error: 92.3258\n",
      "Epoch 6/20\n",
      "769500/769500 [==============================] - 7s 9us/step - loss: 68.7055 - mean_absolute_error: 68.7055 - val_loss: 89.4944 - val_mean_absolute_error: 89.4944\n",
      "Epoch 7/20\n",
      "769500/769500 [==============================] - 7s 10us/step - loss: 65.1853 - mean_absolute_error: 65.1853 - val_loss: 85.4870 - val_mean_absolute_error: 85.4870\n",
      "Epoch 8/20\n",
      "769500/769500 [==============================] - 8s 10us/step - loss: 62.3449 - mean_absolute_error: 62.3449 - val_loss: 80.8653 - val_mean_absolute_error: 80.8653\n",
      "Epoch 9/20\n",
      "769500/769500 [==============================] - 8s 10us/step - loss: 60.5015 - mean_absolute_error: 60.5015 - val_loss: 78.9939 - val_mean_absolute_error: 78.9939\n",
      "Epoch 10/20\n",
      "769500/769500 [==============================] - 7s 10us/step - loss: 59.4042 - mean_absolute_error: 59.4042 - val_loss: 77.4015 - val_mean_absolute_error: 77.4015\n",
      "Epoch 11/20\n",
      "769500/769500 [==============================] - 8s 10us/step - loss: 58.3407 - mean_absolute_error: 58.3407 - val_loss: 76.7934 - val_mean_absolute_error: 76.7934\n",
      "Epoch 12/20\n",
      "769500/769500 [==============================] - 8s 10us/step - loss: 57.1832 - mean_absolute_error: 57.1832 - val_loss: 74.5621 - val_mean_absolute_error: 74.5621\n",
      "Epoch 13/20\n",
      "769500/769500 [==============================] - 8s 10us/step - loss: 56.1043 - mean_absolute_error: 56.1043 - val_loss: 74.7520 - val_mean_absolute_error: 74.7520\n",
      "Epoch 14/20\n",
      "769500/769500 [==============================] - 8s 10us/step - loss: 55.1626 - mean_absolute_error: 55.1626 - val_loss: 73.2317 - val_mean_absolute_error: 73.2317\n",
      "Epoch 15/20\n",
      "769500/769500 [==============================] - 8s 10us/step - loss: 54.4181 - mean_absolute_error: 54.4181 - val_loss: 72.0541 - val_mean_absolute_error: 72.0541\n",
      "Epoch 16/20\n",
      "769500/769500 [==============================] - 7s 10us/step - loss: 53.7271 - mean_absolute_error: 53.7271 - val_loss: 72.9197 - val_mean_absolute_error: 72.9197\n",
      "Epoch 17/20\n",
      "769500/769500 [==============================] - 7s 10us/step - loss: 53.1015 - mean_absolute_error: 53.1015 - val_loss: 73.3182 - val_mean_absolute_error: 73.3182\n",
      "Epoch 18/20\n",
      "769500/769500 [==============================] - 7s 10us/step - loss: 52.4397 - mean_absolute_error: 52.4397 - val_loss: 71.2673 - val_mean_absolute_error: 71.2673\n",
      "Epoch 19/20\n",
      "769500/769500 [==============================] - 8s 10us/step - loss: 51.8241 - mean_absolute_error: 51.8241 - val_loss: 69.2980 - val_mean_absolute_error: 69.2980\n",
      "Epoch 20/20\n",
      "769500/769500 [==============================] - 7s 10us/step - loss: 51.2447 - mean_absolute_error: 51.2447 - val_loss: 67.0328 - val_mean_absolute_error: 67.0328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f180037da0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 학습합니다.\n",
    "\n",
    "model.fit(train_X, train_Y, epochs=20, batch_size=10000, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값을 생성합니다.\n",
    "\n",
    "pred_test = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission 파일을 생성합니다.\n",
    "sample_sub = pd.read_csv('dataset/sample_submission.csv', index_col=0)\n",
    "submission = sample_sub+pred_test\n",
    "submission.to_csv('submission_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 5 layers\n",
    "* 160 units, he_normal, relu\n",
    "* BatchNormalization\n",
    "* Adam(0.001)\n",
    "* epochs 50\n",
    "* batch_size 1000\n",
    "<br><br>\n",
    "* layer를 늘리고, 초기값을 relu 함수에 적합하게 지정\n",
    "* epochs는 늘리고 batch_size는 줄여서 가중치 업데이트량을 증가시킴\n",
    "* BatchNormalization을 사용하면 초깃값에 크게 의존하지 않아도 된다는 것을 경험적으로 도출해 냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 769500 samples, validate on 40500 samples\n",
      "Epoch 1/50\n",
      "769500/769500 [==============================] - 33s 43us/step - loss: 96.6221 - mean_absolute_error: 96.6221 - val_loss: 86.5284 - val_mean_absolute_error: 86.5284\n",
      "Epoch 2/50\n",
      "769500/769500 [==============================] - 29s 37us/step - loss: 42.1786 - mean_absolute_error: 42.1786 - val_loss: 51.1346 - val_mean_absolute_error: 51.1346\n",
      "Epoch 3/50\n",
      "769500/769500 [==============================] - 29s 37us/step - loss: 27.5136 - mean_absolute_error: 27.5136 - val_loss: 44.1087 - val_mean_absolute_error: 44.1087\n",
      "Epoch 4/50\n",
      "769500/769500 [==============================] - 29s 37us/step - loss: 22.9381 - mean_absolute_error: 22.9381 - val_loss: 38.9246 - val_mean_absolute_error: 38.9246\n",
      "Epoch 5/50\n",
      "769500/769500 [==============================] - 29s 38us/step - loss: 20.7333 - mean_absolute_error: 20.7333 - val_loss: 38.5501 - val_mean_absolute_error: 38.5501\n",
      "Epoch 6/50\n",
      "769500/769500 [==============================] - 29s 38us/step - loss: 19.2473 - mean_absolute_error: 19.2473 - val_loss: 35.1719 - val_mean_absolute_error: 35.1719\n",
      "Epoch 7/50\n",
      "769500/769500 [==============================] - 29s 38us/step - loss: 18.1181 - mean_absolute_error: 18.1181 - val_loss: 36.6707 - val_mean_absolute_error: 36.6707\n",
      "Epoch 8/50\n",
      "769500/769500 [==============================] - 29s 37us/step - loss: 17.2358 - mean_absolute_error: 17.2358 - val_loss: 34.9288 - val_mean_absolute_error: 34.9288\n",
      "Epoch 9/50\n",
      "769500/769500 [==============================] - 29s 38us/step - loss: 16.4369 - mean_absolute_error: 16.4369 - val_loss: 33.9064 - val_mean_absolute_error: 33.9064\n",
      "Epoch 10/50\n",
      "769500/769500 [==============================] - 29s 38us/step - loss: 15.8228 - mean_absolute_error: 15.8228 - val_loss: 35.8656 - val_mean_absolute_error: 35.8656\n",
      "Epoch 11/50\n",
      "769500/769500 [==============================] - 30s 38us/step - loss: 15.2547 - mean_absolute_error: 15.2547 - val_loss: 33.6960 - val_mean_absolute_error: 33.6960\n",
      "Epoch 12/50\n",
      "769500/769500 [==============================] - 30s 38us/step - loss: 14.7302 - mean_absolute_error: 14.7302 - val_loss: 32.2248 - val_mean_absolute_error: 32.2248\n",
      "Epoch 13/50\n",
      "769500/769500 [==============================] - 30s 39us/step - loss: 14.3080 - mean_absolute_error: 14.3080 - val_loss: 33.7754 - val_mean_absolute_error: 33.7754\n",
      "Epoch 14/50\n",
      "769500/769500 [==============================] - 30s 39us/step - loss: 13.8776 - mean_absolute_error: 13.8776 - val_loss: 31.7686 - val_mean_absolute_error: 31.7686\n",
      "Epoch 15/50\n",
      "769500/769500 [==============================] - 30s 39us/step - loss: 13.5673 - mean_absolute_error: 13.5673 - val_loss: 33.5919 - val_mean_absolute_error: 33.5919\n",
      "Epoch 16/50\n",
      "769500/769500 [==============================] - 31s 40us/step - loss: 13.1849 - mean_absolute_error: 13.1849 - val_loss: 31.0750 - val_mean_absolute_error: 31.0750\n",
      "Epoch 17/50\n",
      "769500/769500 [==============================] - 32s 42us/step - loss: 12.9368 - mean_absolute_error: 12.9368 - val_loss: 32.7367 - val_mean_absolute_error: 32.7367\n",
      "Epoch 18/50\n",
      "769500/769500 [==============================] - 30s 39us/step - loss: 12.6370 - mean_absolute_error: 12.6370 - val_loss: 31.1765 - val_mean_absolute_error: 31.1765\n",
      "Epoch 19/50\n",
      "769500/769500 [==============================] - 31s 40us/step - loss: 12.3728 - mean_absolute_error: 12.3728 - val_loss: 30.8736 - val_mean_absolute_error: 30.8736\n",
      "Epoch 20/50\n",
      "769500/769500 [==============================] - 32s 42us/step - loss: 12.1517 - mean_absolute_error: 12.1517 - val_loss: 28.6811 - val_mean_absolute_error: 28.6811\n",
      "Epoch 21/50\n",
      "769500/769500 [==============================] - 35s 45us/step - loss: 11.9687 - mean_absolute_error: 11.9687 - val_loss: 29.7278 - val_mean_absolute_error: 29.7278\n",
      "Epoch 22/50\n",
      "769500/769500 [==============================] - 32s 42us/step - loss: 11.7370 - mean_absolute_error: 11.7370 - val_loss: 29.8618 - val_mean_absolute_error: 29.8618\n",
      "Epoch 23/50\n",
      "769500/769500 [==============================] - 34s 44us/step - loss: 11.5647 - mean_absolute_error: 11.5647 - val_loss: 29.2643 - val_mean_absolute_error: 29.2643\n",
      "Epoch 24/50\n",
      "769500/769500 [==============================] - 34s 44us/step - loss: 11.3961 - mean_absolute_error: 11.3961 - val_loss: 29.4471 - val_mean_absolute_error: 29.4471\n",
      "Epoch 25/50\n",
      "769500/769500 [==============================] - 34s 45us/step - loss: 11.2423 - mean_absolute_error: 11.2423 - val_loss: 28.0151 - val_mean_absolute_error: 28.0151\n",
      "Epoch 26/50\n",
      "769500/769500 [==============================] - 34s 44us/step - loss: 11.0798 - mean_absolute_error: 11.0798 - val_loss: 28.6604 - val_mean_absolute_error: 28.6604\n",
      "Epoch 27/50\n",
      "769500/769500 [==============================] - 33s 43us/step - loss: 10.9352 - mean_absolute_error: 10.9352 - val_loss: 28.1819 - val_mean_absolute_error: 28.1819\n",
      "Epoch 28/50\n",
      "769500/769500 [==============================] - 35s 45us/step - loss: 10.8280 - mean_absolute_error: 10.8280 - val_loss: 30.0372 - val_mean_absolute_error: 30.0372\n",
      "Epoch 29/50\n",
      "769500/769500 [==============================] - 40s 52us/step - loss: 10.6928 - mean_absolute_error: 10.6928 - val_loss: 28.7309 - val_mean_absolute_error: 28.7309\n",
      "Epoch 30/50\n",
      "769500/769500 [==============================] - 39s 51us/step - loss: 10.5748 - mean_absolute_error: 10.5748 - val_loss: 27.7046 - val_mean_absolute_error: 27.7046\n",
      "Epoch 31/50\n",
      "769500/769500 [==============================] - 39s 51us/step - loss: 10.4863 - mean_absolute_error: 10.4863 - val_loss: 28.3798 - val_mean_absolute_error: 28.3798\n",
      "Epoch 32/50\n",
      "769500/769500 [==============================] - 35s 45us/step - loss: 10.3643 - mean_absolute_error: 10.3643 - val_loss: 27.6470 - val_mean_absolute_error: 27.6470\n",
      "Epoch 33/50\n",
      "769500/769500 [==============================] - 36s 47us/step - loss: 10.2418 - mean_absolute_error: 10.2418 - val_loss: 28.3808 - val_mean_absolute_error: 28.3808\n",
      "Epoch 34/50\n",
      "769500/769500 [==============================] - 36s 47us/step - loss: 10.1542 - mean_absolute_error: 10.1542 - val_loss: 26.8548 - val_mean_absolute_error: 26.8548\n",
      "Epoch 35/50\n",
      "769500/769500 [==============================] - 36s 47us/step - loss: 10.0984 - mean_absolute_error: 10.0984 - val_loss: 28.9126 - val_mean_absolute_error: 28.9126\n",
      "Epoch 36/50\n",
      "769500/769500 [==============================] - 36s 47us/step - loss: 9.9925 - mean_absolute_error: 9.9925 - val_loss: 27.9053 - val_mean_absolute_error: 27.9053\n",
      "Epoch 37/50\n",
      "769500/769500 [==============================] - 36s 46us/step - loss: 9.8974 - mean_absolute_error: 9.8974 - val_loss: 27.9615 - val_mean_absolute_error: 27.9615\n",
      "Epoch 38/50\n",
      "769500/769500 [==============================] - 38s 49us/step - loss: 9.8346 - mean_absolute_error: 9.8346 - val_loss: 27.0236 - val_mean_absolute_error: 27.0236\n",
      "Epoch 39/50\n",
      "769500/769500 [==============================] - 38s 49us/step - loss: 9.7251 - mean_absolute_error: 9.7251 - val_loss: 27.2348 - val_mean_absolute_error: 27.2348\n",
      "Epoch 40/50\n",
      "769500/769500 [==============================] - 37s 49us/step - loss: 9.6665 - mean_absolute_error: 9.6665 - val_loss: 28.4640 - val_mean_absolute_error: 28.4640\n",
      "Epoch 41/50\n",
      "769500/769500 [==============================] - 38s 49us/step - loss: 9.6244 - mean_absolute_error: 9.6244 - val_loss: 26.3371 - val_mean_absolute_error: 26.3371\n",
      "Epoch 42/50\n",
      "769500/769500 [==============================] - 39s 51us/step - loss: 9.5379 - mean_absolute_error: 9.5379 - val_loss: 25.7155 - val_mean_absolute_error: 25.7155\n",
      "Epoch 43/50\n",
      "769500/769500 [==============================] - 39s 51us/step - loss: 9.4924 - mean_absolute_error: 9.4924 - val_loss: 26.3539 - val_mean_absolute_error: 26.3539\n",
      "Epoch 44/50\n",
      "769500/769500 [==============================] - 41s 53us/step - loss: 9.4157 - mean_absolute_error: 9.4157 - val_loss: 27.9654 - val_mean_absolute_error: 27.9654\n",
      "Epoch 45/50\n",
      "769500/769500 [==============================] - 40s 53us/step - loss: 9.3689 - mean_absolute_error: 9.3689 - val_loss: 26.0254 - val_mean_absolute_error: 26.0254\n",
      "Epoch 46/50\n",
      "769500/769500 [==============================] - 42s 55us/step - loss: 9.2945 - mean_absolute_error: 9.2945 - val_loss: 25.9428 - val_mean_absolute_error: 25.9428\n",
      "Epoch 47/50\n",
      "769500/769500 [==============================] - 40s 53us/step - loss: 9.2388 - mean_absolute_error: 9.2388 - val_loss: 25.5840 - val_mean_absolute_error: 25.5840\n",
      "Epoch 48/50\n",
      "769500/769500 [==============================] - 40s 52us/step - loss: 9.1544 - mean_absolute_error: 9.1544 - val_loss: 27.6552 - val_mean_absolute_error: 27.6552\n",
      "Epoch 49/50\n",
      "769500/769500 [==============================] - 41s 54us/step - loss: 9.1388 - mean_absolute_error: 9.1388 - val_loss: 24.8437 - val_mean_absolute_error: 24.8437\n",
      "Epoch 50/50\n",
      "769500/769500 [==============================] - 42s 54us/step - loss: 9.0836 - mean_absolute_error: 9.0836 - val_loss: 27.9688 - val_mean_absolute_error: 27.9688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f1fdd09278>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 케라스를 통해 모델 생성을 시작합니다.\n",
    "\n",
    "model_t1 = Sequential()\n",
    "model_t1.add(Dense(units=160, input_dim=226, kernel_initializer='he_normal'))\n",
    "model_t1.add(BatchNormalization())\n",
    "model_t1.add(Activation('relu'))\n",
    "model_t1.add(Dense(units=160, kernel_initializer='he_normal'))\n",
    "model_t1.add(BatchNormalization())\n",
    "model_t1.add(Activation('relu'))\n",
    "model_t1.add(Dense(units=160, kernel_initializer='he_normal'))\n",
    "model_t1.add(BatchNormalization())\n",
    "model_t1.add(Activation('relu'))\n",
    "model_t1.add(Dense(units=160, kernel_initializer='he_normal'))\n",
    "model_t1.add(BatchNormalization())\n",
    "model_t1.add(Activation('relu'))\n",
    "model_t1.add(Dense(units=4, activation='linear'))\n",
    "\n",
    "adam = keras.optimizers.Adam(0.001)\n",
    "model_t1.compile(loss='mae', optimizer=adam, metrics=['mae'])\n",
    "\n",
    "model_t1.fit(train_X, train_Y, epochs=50, batch_size=1000, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값을 생성합니다.\n",
    "\n",
    "pred_test_1 = model_t1.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission 파일을 생성합니다.\n",
    "sample_sub = pd.read_csv('dataset/sample_submission.csv', index_col=0)\n",
    "submission = sample_sub+pred_test_1\n",
    "submission.to_csv('submission_01.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
