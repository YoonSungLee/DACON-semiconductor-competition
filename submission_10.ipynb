{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "submission_10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqIznGZ6tWsvYD/v71ChEM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Inha-AI/DACON-semiconductor-competition/blob/feature%2FYoonSungLee/submission_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGoMPskD2vbL",
        "colab_type": "code",
        "outputId": "9ba12969-f94d-4202-bc33-082a35acda62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "pip install bayesian-optimization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/72/0c/173ac467d0a53e33e41b521e4ceba74a8ac7c7873d7b857a8fbdca88302d/bayesian-optimization-1.0.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.17.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.22.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.14.1)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.0.1-cp36-none-any.whl size=10032 sha256=154cc6a24292ddb576a31125ae54c7e5eb49e207ca681777a26ff1dab1c80294\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/0d/3b/6b9d4477a34b3905f246ff4e7acf6aafd4cc9b77d473629b77\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vVeUdyssBKw",
        "colab_type": "code",
        "outputId": "1db6d734-1c73-4375-815d-fac693130936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bayes_opt import BayesianOptimization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwHXDEigsNB0",
        "colab_type": "code",
        "outputId": "144bbef0-17bd-4007-99a2-0da78ce8cf46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BNXAU5rsPJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv('/gdrive/My Drive/DACON-semiconductor-competition/dataset/train.csv')\n",
        "df_test = pd.read_csv('/gdrive/My Drive/DACON-semiconductor-competition/dataset/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95Z3jk-V0oPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 자료형을 적절히 변형시켜 데이터의 크기를 줄이는 방법\n",
        "\n",
        "# for col in df_train.columns:\n",
        "#     col_type = df_train[col].dtypes\n",
        "#     min1 = df_train[col].min()\n",
        "#     max1 = df_train[col].max()\n",
        "#     if str(col_type)[:3] == 'int':\n",
        "#         df_train[col] = df_train[col].astype(np.int16)\n",
        "#     else:\n",
        "#         if min1 > np.finfo(np.float16).min and max1 < np.finfo(np.float16).max:\n",
        "#             df_train[col] = trdf_trainain[col].astype(np.float16)\n",
        "#         elif min1 > np.finfo(np.float32).min and max1 < np.finfo(np.float32).max:\n",
        "#             df_train[col] = df_train[col].astype(np.float32)\n",
        "#         else:\n",
        "#             df_train[col] = df_train[col].astype(np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdYXclfgsVr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 독립변수와 종속변수를 분리합니다.\n",
        "\n",
        "train_X = df_train.iloc[:,4:]\n",
        "train_Y = df_train.iloc[:,0:4]\n",
        "test_X = df_test.iloc[:,1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHhSqNartkLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train set을 shuffle하여 다시 train set과 validation set으로 분리합니다.\n",
        "\n",
        "train_X, val_X, train_Y, val_Y = train_test_split(train_X, train_Y, test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tT4w6XasaSQ",
        "colab_type": "text"
      },
      "source": [
        "# Model 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PppXxVagvBvl",
        "colab_type": "text"
      },
      "source": [
        "* 11 layers\n",
        "* (239, 252, 265, 178, 91) units, he_normal, swish\n",
        "* BatchNormalization\n",
        "* Adam(0.001)\n",
        "* epochs 200\n",
        "* batch_size 630\n",
        "<br><br>\n",
        "* BayesianOptimization을 사용하여 최적의 learning_rate와 batch_size 추출\n",
        "* 입력 데이터는 음수값을 포함하고 있으므로 작은 음수값들은 살리기 위해 activation function을 swish로 변경"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PSSfoS5sfWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 케라스를 통해 모델 생성을 시작합니다.\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=239, input_dim=226, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=239, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=252, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=252, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=265, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=265, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=178, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=178, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=91, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=91, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=4, activation='linear'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Activation Function 정의\n",
        "\n",
        "def swish(x) :\n",
        "    return x * keras.activations.sigmoid(x)\n",
        "\n",
        "\n",
        "# 특정한 초기 학습률 및 배치 사이즈 하에서 학습을 수행한 후, 검증 성능을 출력하는 목적 함수 정의\n",
        "\n",
        "def train_and_validate(learning_rate, batch_size):\n",
        "    model = create_model()\n",
        "    adam = keras.optimizers.Adam(learning_rate)\n",
        "    model.compile(loss='mae', optimizer=adam, metrics=['accuracy'])\n",
        "    hist = model.fit(train_X, train_Y, epochs=20, batch_size=int(batch_size),\n",
        "                    validation_data=(val_X, val_Y))\n",
        "    best_val_score = max(hist.history['val_acc'])\n",
        "\n",
        "    return best_val_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRUU0ITgsURK",
        "colab_type": "code",
        "outputId": "bd23ea05-a35d-4cf7-85be-7915ae46ca4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# BayesianOptimization 객체 생성, 실행 및 최종 결과 출력\n",
        "# 특정 모델에서 최적의 learning_rate와 batch_size를 찾기 위함\n",
        "# model.fit() 의 verbose를 0으로 설정하여 \"Buffered data was truncated after reaching the output size limit.\" 문제 해결\n",
        "\n",
        "bayes_optimizer = BayesianOptimization(\n",
        "    f=train_and_validate,\n",
        "    pbounds={\n",
        "        'learning_rate' : (0.001, 0.1),\n",
        "        'batch_size' : (500,1000)\n",
        "    },\n",
        "    random_state=0,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "bayes_optimizer.maximize(init_points=3, n_iter=27, acq='ei',xi=0.01)\n",
        "\n",
        "for i, res in enumerate(bayes_optimizer.res):\n",
        "    print('iteration {}: \\n\\t{}'.format(i, res))\n",
        "print('Final result: ', bayes_optimizer.max)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | batch_... | learni... |\n",
            "-------------------------------------------------\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "607500/607500 [==============================] - 26s 43us/step - loss: 57.2700 - acc: 0.3511 - val_loss: 61.1310 - val_acc: 0.3638\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 38.6549 - acc: 0.5834 - val_loss: 49.7541 - val_acc: 0.5489\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 25.7873 - acc: 0.7323 - val_loss: 37.2670 - val_acc: 0.6252\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 20.8406 - acc: 0.7811 - val_loss: 26.7745 - val_acc: 0.7200\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 17.7931 - acc: 0.8105 - val_loss: 22.7330 - val_acc: 0.7629\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 15.6509 - acc: 0.8311 - val_loss: 19.9171 - val_acc: 0.7866\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 14.0688 - acc: 0.8468 - val_loss: 16.3186 - val_acc: 0.8239\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 12.9791 - acc: 0.8577 - val_loss: 17.2155 - val_acc: 0.8215\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 12.2649 - acc: 0.8660 - val_loss: 15.8504 - val_acc: 0.8329\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 11.4458 - acc: 0.8732 - val_loss: 13.8927 - val_acc: 0.8417\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 10.9985 - acc: 0.8781 - val_loss: 13.8956 - val_acc: 0.8552\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 10.4125 - acc: 0.8837 - val_loss: 13.5193 - val_acc: 0.8505\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 10.0352 - acc: 0.8879 - val_loss: 12.0650 - val_acc: 0.8612\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 9.7472 - acc: 0.8913 - val_loss: 13.3562 - val_acc: 0.8637\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 9.3616 - acc: 0.8947 - val_loss: 11.6983 - val_acc: 0.8692\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 9.0753 - acc: 0.8988 - val_loss: 11.8639 - val_acc: 0.8625\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 8.8007 - acc: 0.9011 - val_loss: 10.9289 - val_acc: 0.8882\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 8.6356 - acc: 0.9022 - val_loss: 11.5782 - val_acc: 0.8672\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 8.3940 - acc: 0.9049 - val_loss: 10.3160 - val_acc: 0.8901\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 8.2193 - acc: 0.9072 - val_loss: 10.9099 - val_acc: 0.8850\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8901  \u001b[0m | \u001b[0m 774.4   \u001b[0m | \u001b[0m 0.0718  \u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 54.3995 - acc: 0.3711 - val_loss: 60.4107 - val_acc: 0.3531\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 37.5041 - acc: 0.5836 - val_loss: 48.4388 - val_acc: 0.5466\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 25.1008 - acc: 0.7370 - val_loss: 30.2004 - val_acc: 0.6806\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 20.1177 - acc: 0.7846 - val_loss: 22.7694 - val_acc: 0.7560\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 17.0822 - acc: 0.8143 - val_loss: 22.3132 - val_acc: 0.7581\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 14.9600 - acc: 0.8356 - val_loss: 18.7604 - val_acc: 0.8001\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 13.5689 - acc: 0.8518 - val_loss: 15.8038 - val_acc: 0.8334\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 12.3915 - acc: 0.8638 - val_loss: 16.2429 - val_acc: 0.8309\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 11.5742 - acc: 0.8722 - val_loss: 15.7227 - val_acc: 0.8304\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 14s 24us/step - loss: 10.8703 - acc: 0.8787 - val_loss: 14.7723 - val_acc: 0.8483\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 10.2557 - acc: 0.8864 - val_loss: 13.3906 - val_acc: 0.8616\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 9.7893 - acc: 0.8902 - val_loss: 11.6654 - val_acc: 0.8716\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 9.4433 - acc: 0.8949 - val_loss: 11.4207 - val_acc: 0.8776\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 14s 24us/step - loss: 9.0907 - acc: 0.8985 - val_loss: 11.5008 - val_acc: 0.8736\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 14s 24us/step - loss: 8.8011 - acc: 0.9008 - val_loss: 10.1287 - val_acc: 0.8921\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 8.5356 - acc: 0.9041 - val_loss: 10.3164 - val_acc: 0.8834\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 8.2407 - acc: 0.9069 - val_loss: 9.3877 - val_acc: 0.8896\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 14s 24us/step - loss: 8.0109 - acc: 0.9099 - val_loss: 11.0031 - val_acc: 0.8769\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 14s 24us/step - loss: 7.7305 - acc: 0.9118 - val_loss: 9.8349 - val_acc: 0.8962\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 7.5851 - acc: 0.9134 - val_loss: 10.1743 - val_acc: 0.8906\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8962  \u001b[0m | \u001b[95m 801.4   \u001b[0m | \u001b[95m 0.05494 \u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 53.2022 - acc: 0.3986 - val_loss: 77.7166 - val_acc: 0.3712\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 36.3898 - acc: 0.6058 - val_loss: 46.4649 - val_acc: 0.5652\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 25.5340 - acc: 0.7291 - val_loss: 38.7921 - val_acc: 0.6285\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 21.0205 - acc: 0.7754 - val_loss: 27.0428 - val_acc: 0.7180\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 18.1998 - acc: 0.8036 - val_loss: 21.9441 - val_acc: 0.7728\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 16.2137 - acc: 0.8232 - val_loss: 20.6715 - val_acc: 0.7843\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 14.8024 - acc: 0.8377 - val_loss: 17.4166 - val_acc: 0.8198\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 13.7633 - acc: 0.8486 - val_loss: 16.5593 - val_acc: 0.8187\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 12.8226 - acc: 0.8588 - val_loss: 15.1104 - val_acc: 0.8447\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 12.1572 - acc: 0.8654 - val_loss: 16.9990 - val_acc: 0.8101\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 11.5353 - acc: 0.8711 - val_loss: 16.0790 - val_acc: 0.8432\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 11.0163 - acc: 0.8769 - val_loss: 12.8037 - val_acc: 0.8606\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 10.5553 - acc: 0.8816 - val_loss: 13.5335 - val_acc: 0.8300\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 10.1050 - acc: 0.8860 - val_loss: 12.9602 - val_acc: 0.8523\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 9.7316 - acc: 0.8909 - val_loss: 13.4708 - val_acc: 0.8504\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 9.3969 - acc: 0.8945 - val_loss: 10.5753 - val_acc: 0.8797\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 9.0251 - acc: 0.8981 - val_loss: 11.6884 - val_acc: 0.8783\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 8.7772 - acc: 0.9016 - val_loss: 10.2927 - val_acc: 0.8856\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 8.4516 - acc: 0.9050 - val_loss: 11.4264 - val_acc: 0.8714\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 8.2782 - acc: 0.9060 - val_loss: 9.7241 - val_acc: 0.8860\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.886   \u001b[0m | \u001b[0m 711.8   \u001b[0m | \u001b[0m 0.06494 \u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 55.7488 - acc: 0.3612 - val_loss: 75.9956 - val_acc: 0.3237\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 12s 19us/step - loss: 37.6154 - acc: 0.5968 - val_loss: 59.1402 - val_acc: 0.4857\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 12s 20us/step - loss: 26.1303 - acc: 0.7230 - val_loss: 33.2276 - val_acc: 0.6660\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 12s 19us/step - loss: 21.7398 - acc: 0.7647 - val_loss: 28.3210 - val_acc: 0.7166\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 12s 19us/step - loss: 18.7964 - acc: 0.7917 - val_loss: 26.2240 - val_acc: 0.7162\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 12s 20us/step - loss: 16.7355 - acc: 0.8134 - val_loss: 25.1614 - val_acc: 0.7268\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 12s 19us/step - loss: 15.2513 - acc: 0.8290 - val_loss: 21.4768 - val_acc: 0.7729\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 12s 20us/step - loss: 14.2154 - acc: 0.8406 - val_loss: 19.2411 - val_acc: 0.8004\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 12s 20us/step - loss: 13.2508 - acc: 0.8511 - val_loss: 16.7731 - val_acc: 0.8163\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 12s 19us/step - loss: 12.6232 - acc: 0.8570 - val_loss: 17.5099 - val_acc: 0.8133\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 12s 19us/step - loss: 12.0378 - acc: 0.8640 - val_loss: 15.6021 - val_acc: 0.8142\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 12s 19us/step - loss: 11.5187 - acc: 0.8690 - val_loss: 16.8762 - val_acc: 0.8257\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 12s 20us/step - loss: 11.1110 - acc: 0.8740 - val_loss: 14.9307 - val_acc: 0.8420\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 12s 19us/step - loss: 10.7298 - acc: 0.8783 - val_loss: 14.9234 - val_acc: 0.8349\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 12s 20us/step - loss: 10.3770 - acc: 0.8822 - val_loss: 12.4050 - val_acc: 0.8634\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 12s 20us/step - loss: 9.9316 - acc: 0.8874 - val_loss: 13.6083 - val_acc: 0.8492\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 12s 20us/step - loss: 9.7122 - acc: 0.8895 - val_loss: 12.8678 - val_acc: 0.8529\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 12s 20us/step - loss: 9.3445 - acc: 0.8928 - val_loss: 12.2639 - val_acc: 0.8641\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 12s 19us/step - loss: 9.0470 - acc: 0.8970 - val_loss: 12.3590 - val_acc: 0.8597\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 12s 20us/step - loss: 8.8393 - acc: 0.8996 - val_loss: 11.4623 - val_acc: 0.8781\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8781  \u001b[0m | \u001b[0m 1e+03   \u001b[0m | \u001b[0m 0.07694 \u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 23s 37us/step - loss: 52.0197 - acc: 0.4463 - val_loss: 55.6706 - val_acc: 0.4846\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 27.8069 - acc: 0.7089 - val_loss: 30.3186 - val_acc: 0.6814\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 19.9277 - acc: 0.7846 - val_loss: 22.5580 - val_acc: 0.7679\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 15.6268 - acc: 0.8270 - val_loss: 15.3534 - val_acc: 0.8246\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 13.2219 - acc: 0.8528 - val_loss: 15.6769 - val_acc: 0.8263\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 11.7708 - acc: 0.8686 - val_loss: 13.0436 - val_acc: 0.8530\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 10.7411 - acc: 0.8792 - val_loss: 10.8040 - val_acc: 0.8708\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 9.9541 - acc: 0.8886 - val_loss: 11.8727 - val_acc: 0.8622\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 9.2926 - acc: 0.8954 - val_loss: 9.7860 - val_acc: 0.8832\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 8.8061 - acc: 0.8999 - val_loss: 10.2346 - val_acc: 0.8868\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 8.3647 - acc: 0.9058 - val_loss: 9.8679 - val_acc: 0.8990\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 8.0763 - acc: 0.9087 - val_loss: 9.0370 - val_acc: 0.8946\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 7.7278 - acc: 0.9114 - val_loss: 7.5331 - val_acc: 0.9104\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 7.5187 - acc: 0.9147 - val_loss: 7.6174 - val_acc: 0.9096\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 7.2691 - acc: 0.9171 - val_loss: 8.3670 - val_acc: 0.9069\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 7.0880 - acc: 0.9195 - val_loss: 6.4191 - val_acc: 0.9249\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 6.8974 - acc: 0.9209 - val_loss: 6.2033 - val_acc: 0.9250\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 6.7673 - acc: 0.9225 - val_loss: 6.6835 - val_acc: 0.9212\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 6.5760 - acc: 0.9243 - val_loss: 6.6811 - val_acc: 0.9222\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 6.4451 - acc: 0.9264 - val_loss: 6.5880 - val_acc: 0.9262\n",
            "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.9262  \u001b[0m | \u001b[95m 500.0   \u001b[0m | \u001b[95m 0.01462 \u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 51.5814 - acc: 0.4204 - val_loss: 83.4647 - val_acc: 0.3326\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 33.4669 - acc: 0.6401 - val_loss: 38.6648 - val_acc: 0.6142\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 25.5197 - acc: 0.7259 - val_loss: 27.8447 - val_acc: 0.7048\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 21.8238 - acc: 0.7626 - val_loss: 28.8416 - val_acc: 0.6944\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 19.4400 - acc: 0.7844 - val_loss: 24.7127 - val_acc: 0.7499\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 17.7396 - acc: 0.8047 - val_loss: 20.0852 - val_acc: 0.7813\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 16.5162 - acc: 0.8176 - val_loss: 20.9270 - val_acc: 0.7766\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 15.4161 - acc: 0.8293 - val_loss: 22.0138 - val_acc: 0.7689\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 14.6711 - acc: 0.8376 - val_loss: 18.3861 - val_acc: 0.7857\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 13.9483 - acc: 0.8452 - val_loss: 17.9156 - val_acc: 0.8065\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 13.4322 - acc: 0.8509 - val_loss: 16.5451 - val_acc: 0.8166\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 12.9906 - acc: 0.8556 - val_loss: 15.9506 - val_acc: 0.8302\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 22s 37us/step - loss: 12.5091 - acc: 0.8613 - val_loss: 15.9179 - val_acc: 0.8346\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 12.0399 - acc: 0.8650 - val_loss: 14.4504 - val_acc: 0.8388\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 11.6852 - acc: 0.8692 - val_loss: 14.3892 - val_acc: 0.8399\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 11.2740 - acc: 0.8733 - val_loss: 15.6312 - val_acc: 0.8254\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 11.0107 - acc: 0.8765 - val_loss: 13.1729 - val_acc: 0.8574\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 10.7280 - acc: 0.8794 - val_loss: 13.3308 - val_acc: 0.8551\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 10.4759 - acc: 0.8821 - val_loss: 10.7768 - val_acc: 0.8847\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 10.2496 - acc: 0.8852 - val_loss: 12.2784 - val_acc: 0.8557\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8847  \u001b[0m | \u001b[0m 550.3   \u001b[0m | \u001b[0m 0.09954 \u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 77.3957 - acc: 0.3181 - val_loss: 64.8710 - val_acc: 0.3148\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 35.4272 - acc: 0.6231 - val_loss: 33.5693 - val_acc: 0.6656\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 20.1747 - acc: 0.7913 - val_loss: 20.0601 - val_acc: 0.7926\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 14.8692 - acc: 0.8422 - val_loss: 14.1601 - val_acc: 0.8506\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 12.0984 - acc: 0.8700 - val_loss: 13.8384 - val_acc: 0.8493\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 10.4737 - acc: 0.8860 - val_loss: 11.7077 - val_acc: 0.8809\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 9.3844 - acc: 0.8975 - val_loss: 9.9971 - val_acc: 0.8872\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 8.6160 - acc: 0.9052 - val_loss: 8.6425 - val_acc: 0.9072\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 7.9823 - acc: 0.9116 - val_loss: 8.2128 - val_acc: 0.9099\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 7.5620 - acc: 0.9156 - val_loss: 8.5767 - val_acc: 0.8999\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 7.1206 - acc: 0.9203 - val_loss: 9.1899 - val_acc: 0.9034\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 6.8225 - acc: 0.9229 - val_loss: 7.4481 - val_acc: 0.9114\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 6.5743 - acc: 0.9250 - val_loss: 7.0374 - val_acc: 0.9254\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 6.3382 - acc: 0.9282 - val_loss: 6.7850 - val_acc: 0.9210\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 6.1375 - acc: 0.9292 - val_loss: 7.0960 - val_acc: 0.9213\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 5.9551 - acc: 0.9317 - val_loss: 6.8964 - val_acc: 0.9233\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 5.7960 - acc: 0.9331 - val_loss: 5.8566 - val_acc: 0.9283\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 5.6552 - acc: 0.9344 - val_loss: 5.6684 - val_acc: 0.9390\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 5.5462 - acc: 0.9351 - val_loss: 5.6486 - val_acc: 0.9392\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 5.3838 - acc: 0.9371 - val_loss: 5.1225 - val_acc: 0.9371\n",
            "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.9392  \u001b[0m | \u001b[95m 911.2   \u001b[0m | \u001b[95m 0.003202\u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 56.5544 - acc: 0.3521 - val_loss: 68.7191 - val_acc: 0.3280\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 44.8806 - acc: 0.4898 - val_loss: 55.2009 - val_acc: 0.4771\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 33.7787 - acc: 0.6403 - val_loss: 43.3509 - val_acc: 0.5864\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 24.9960 - acc: 0.7280 - val_loss: 32.6812 - val_acc: 0.6561\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 21.3127 - acc: 0.7651 - val_loss: 25.8402 - val_acc: 0.7232\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 18.7015 - acc: 0.7912 - val_loss: 26.8672 - val_acc: 0.7246\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 16.7162 - acc: 0.8130 - val_loss: 22.0188 - val_acc: 0.7641\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 15.2523 - acc: 0.8286 - val_loss: 19.2573 - val_acc: 0.7846\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 14.2703 - acc: 0.8396 - val_loss: 18.0948 - val_acc: 0.8047\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 13.2830 - acc: 0.8496 - val_loss: 16.8493 - val_acc: 0.8163\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 12.6180 - acc: 0.8576 - val_loss: 16.6393 - val_acc: 0.8211\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 12.0621 - acc: 0.8628 - val_loss: 17.1751 - val_acc: 0.8166\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 11.5459 - acc: 0.8689 - val_loss: 13.0973 - val_acc: 0.8566\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 11.1407 - acc: 0.8734 - val_loss: 13.5145 - val_acc: 0.8517\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 10.6956 - acc: 0.8776 - val_loss: 13.2602 - val_acc: 0.8425\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 10.3788 - acc: 0.8813 - val_loss: 14.0167 - val_acc: 0.8361\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 10.0675 - acc: 0.8852 - val_loss: 16.1437 - val_acc: 0.8355\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 9.7495 - acc: 0.8882 - val_loss: 13.8127 - val_acc: 0.8206\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 9.5101 - acc: 0.8911 - val_loss: 13.1092 - val_acc: 0.8466\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 9.2945 - acc: 0.8940 - val_loss: 12.6948 - val_acc: 0.8626\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8626  \u001b[0m | \u001b[0m 939.0   \u001b[0m | \u001b[0m 0.09787 \u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 122.9006 - acc: 0.2611 - val_loss: 78.3843 - val_acc: 0.2659\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 14s 22us/step - loss: 56.2407 - acc: 0.4117 - val_loss: 48.4867 - val_acc: 0.5151\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 14s 22us/step - loss: 29.6478 - acc: 0.7002 - val_loss: 25.8067 - val_acc: 0.7431\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 17.4083 - acc: 0.8216 - val_loss: 17.3612 - val_acc: 0.8218\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 14s 22us/step - loss: 13.6878 - acc: 0.8578 - val_loss: 13.1038 - val_acc: 0.8598\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 11.7546 - acc: 0.8780 - val_loss: 11.7225 - val_acc: 0.8791\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 10.5165 - acc: 0.8904 - val_loss: 10.7311 - val_acc: 0.8867\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 9.6849 - acc: 0.8988 - val_loss: 9.5311 - val_acc: 0.8983\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 8.9761 - acc: 0.9051 - val_loss: 8.9457 - val_acc: 0.9065\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 8.4713 - acc: 0.9102 - val_loss: 8.9993 - val_acc: 0.9007\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 14s 22us/step - loss: 8.0726 - acc: 0.9140 - val_loss: 8.3767 - val_acc: 0.9120\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 7.7159 - acc: 0.9171 - val_loss: 7.7847 - val_acc: 0.9179\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 7.4113 - acc: 0.9202 - val_loss: 7.6097 - val_acc: 0.9235\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 7.1326 - acc: 0.9231 - val_loss: 7.0706 - val_acc: 0.9222\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 6.9218 - acc: 0.9247 - val_loss: 6.3493 - val_acc: 0.9339\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 6.6690 - acc: 0.9274 - val_loss: 6.4671 - val_acc: 0.9269\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 6.5270 - acc: 0.9285 - val_loss: 6.6332 - val_acc: 0.9262\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 6.3678 - acc: 0.9292 - val_loss: 5.7893 - val_acc: 0.9366\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 6.1974 - acc: 0.9315 - val_loss: 6.9396 - val_acc: 0.9152\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 6.0581 - acc: 0.9327 - val_loss: 6.0203 - val_acc: 0.9336\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.9366  \u001b[0m | \u001b[0m 876.1   \u001b[0m | \u001b[0m 0.001   \u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 22s 37us/step - loss: 107.2993 - acc: 0.2565 - val_loss: 69.7325 - val_acc: 0.2574\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 43.3380 - acc: 0.5300 - val_loss: 32.5117 - val_acc: 0.6693\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 21.5140 - acc: 0.7790 - val_loss: 18.4410 - val_acc: 0.8131\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 15.2948 - acc: 0.8409 - val_loss: 14.8091 - val_acc: 0.8439\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 12.6564 - acc: 0.8678 - val_loss: 13.0119 - val_acc: 0.8622\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 11.1483 - acc: 0.8831 - val_loss: 11.2013 - val_acc: 0.8835\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 10.1858 - acc: 0.8925 - val_loss: 10.5692 - val_acc: 0.8895\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 9.4253 - acc: 0.9001 - val_loss: 9.9931 - val_acc: 0.8945\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 8.8212 - acc: 0.9060 - val_loss: 9.2392 - val_acc: 0.9011\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 8.3494 - acc: 0.9110 - val_loss: 8.5042 - val_acc: 0.9030\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 8.0025 - acc: 0.9140 - val_loss: 8.2389 - val_acc: 0.9167\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 7.6592 - acc: 0.9174 - val_loss: 7.3894 - val_acc: 0.9220\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 7.3375 - acc: 0.9210 - val_loss: 7.2395 - val_acc: 0.9245\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 7.0479 - acc: 0.9235 - val_loss: 7.0819 - val_acc: 0.9229\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 6.8646 - acc: 0.9251 - val_loss: 6.3137 - val_acc: 0.9324\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 6.6802 - acc: 0.9269 - val_loss: 6.3121 - val_acc: 0.9246\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 6.4447 - acc: 0.9292 - val_loss: 6.2782 - val_acc: 0.9376\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 6.3556 - acc: 0.9303 - val_loss: 6.0659 - val_acc: 0.9321\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 6.1591 - acc: 0.9318 - val_loss: 5.9043 - val_acc: 0.9356\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 6.0458 - acc: 0.9328 - val_loss: 5.5693 - val_acc: 0.9457\n",
            "| \u001b[95m 10      \u001b[0m | \u001b[95m 0.9457  \u001b[0m | \u001b[95m 633.0   \u001b[0m | \u001b[95m 0.001   \u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 51.8841 - acc: 0.4240 - val_loss: 72.2457 - val_acc: 0.3652\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 33.1325 - acc: 0.6487 - val_loss: 41.0331 - val_acc: 0.6130\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 25.2654 - acc: 0.7309 - val_loss: 32.5089 - val_acc: 0.6656\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 21.4862 - acc: 0.7667 - val_loss: 24.8959 - val_acc: 0.7292\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 18.9658 - acc: 0.7934 - val_loss: 24.3911 - val_acc: 0.7384\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 17.0615 - acc: 0.8127 - val_loss: 20.5819 - val_acc: 0.7811\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 15.5449 - acc: 0.8281 - val_loss: 18.5765 - val_acc: 0.8032\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 14.4680 - acc: 0.8392 - val_loss: 16.3654 - val_acc: 0.8229\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 13.6907 - acc: 0.8473 - val_loss: 16.0977 - val_acc: 0.8184\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 12.8752 - acc: 0.8562 - val_loss: 16.5877 - val_acc: 0.8112\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 12.2633 - acc: 0.8632 - val_loss: 16.6295 - val_acc: 0.8229\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 11.7459 - acc: 0.8694 - val_loss: 14.4840 - val_acc: 0.8464\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 11.2549 - acc: 0.8741 - val_loss: 15.0867 - val_acc: 0.8443\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 10.9193 - acc: 0.8773 - val_loss: 13.3946 - val_acc: 0.8504\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 10.4883 - acc: 0.8828 - val_loss: 13.0787 - val_acc: 0.8603\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 10.2196 - acc: 0.8853 - val_loss: 13.7060 - val_acc: 0.8599\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 9.9162 - acc: 0.8885 - val_loss: 11.6915 - val_acc: 0.8757\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 9.5796 - acc: 0.8916 - val_loss: 11.6656 - val_acc: 0.8706\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 9.3950 - acc: 0.8947 - val_loss: 10.2769 - val_acc: 0.8868\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 9.2744 - acc: 0.8955 - val_loss: 13.5330 - val_acc: 0.8626\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8868  \u001b[0m | \u001b[0m 657.9   \u001b[0m | \u001b[0m 0.1     \u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 24s 39us/step - loss: 105.1714 - acc: 0.2480 - val_loss: 75.7888 - val_acc: 0.2529\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 41.2032 - acc: 0.5618 - val_loss: 30.1991 - val_acc: 0.6935\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 20.8789 - acc: 0.7867 - val_loss: 19.9548 - val_acc: 0.7977\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 15.3715 - acc: 0.8391 - val_loss: 15.7125 - val_acc: 0.8362\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 12.8831 - acc: 0.8651 - val_loss: 11.7485 - val_acc: 0.8768\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 11.3562 - acc: 0.8809 - val_loss: 11.1864 - val_acc: 0.8881\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 10.3045 - acc: 0.8916 - val_loss: 9.8851 - val_acc: 0.8977\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 9.5663 - acc: 0.8981 - val_loss: 8.9852 - val_acc: 0.9023\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 8.9899 - acc: 0.9045 - val_loss: 9.5603 - val_acc: 0.8953\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 8.4606 - acc: 0.9089 - val_loss: 8.1827 - val_acc: 0.9175\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 8.0883 - acc: 0.9127 - val_loss: 7.8820 - val_acc: 0.9133\n",
            "Epoch 12/20\n",
            "424293/607500 [===================>..........] - ETA: 5s - loss: 7.8393 - acc: 0.9147Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw1PeO0zsUW2",
        "colab_type": "code",
        "outputId": "8dabf90f-4532-4633-e334-cb6ee62b1ae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# BayesianOptimization 객체 생성, 실행 및 최종 결과 출력\n",
        "# 특정 모델에서 최적의 learning_rate와 batch_size를 찾기 위함\n",
        "# 마찬가지로 오류 발생\n",
        "\n",
        "bayes_optimizer = BayesianOptimization(\n",
        "    f=train_and_validate,\n",
        "    pbounds={\n",
        "        'learning_rate' : (0.001, 0.1),\n",
        "        'batch_size' : (500,1000)\n",
        "    },\n",
        "    random_state=0,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "bayes_optimizer.maximize(init_points=3, n_iter=27, acq='ei',xi=0.01)\n",
        "\n",
        "for i, res in enumerate(bayes_optimizer.res):\n",
        "    print('iteration {}: \\n\\t{}'.format(i, res))\n",
        "print('Final result: ', bayes_optimizer.max)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | batch_... | learni... |\n",
            "-------------------------------------------------\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 31s 51us/step - loss: 55.2841 - acc: 0.3649 - val_loss: 65.9277 - val_acc: 0.3350\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 17s 27us/step - loss: 39.7310 - acc: 0.5721 - val_loss: 59.9574 - val_acc: 0.4360\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 27.5709 - acc: 0.7136 - val_loss: 37.1839 - val_acc: 0.6202\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 22.2966 - acc: 0.7662 - val_loss: 31.3564 - val_acc: 0.6654\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 19.0175 - acc: 0.7956 - val_loss: 24.3705 - val_acc: 0.7399\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 16.6751 - acc: 0.8186 - val_loss: 20.1325 - val_acc: 0.7886\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 14.9905 - acc: 0.8358 - val_loss: 20.3490 - val_acc: 0.7833\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 17s 27us/step - loss: 13.5833 - acc: 0.8501 - val_loss: 20.3225 - val_acc: 0.7793\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 17s 27us/step - loss: 12.5681 - acc: 0.8603 - val_loss: 14.9672 - val_acc: 0.8331\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 11.8326 - acc: 0.8691 - val_loss: 15.7765 - val_acc: 0.8360\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 11.0692 - acc: 0.8769 - val_loss: 13.3234 - val_acc: 0.8513\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 17s 27us/step - loss: 10.5735 - acc: 0.8821 - val_loss: 13.0094 - val_acc: 0.8623\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 10.0487 - acc: 0.8878 - val_loss: 14.0761 - val_acc: 0.8416\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 9.7372 - acc: 0.8917 - val_loss: 11.5685 - val_acc: 0.8727\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 9.2955 - acc: 0.8951 - val_loss: 11.0213 - val_acc: 0.8764\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 8.9372 - acc: 0.8987 - val_loss: 10.7239 - val_acc: 0.8873\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 8.6578 - acc: 0.9024 - val_loss: 10.4694 - val_acc: 0.8832\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 8.3782 - acc: 0.9052 - val_loss: 11.2210 - val_acc: 0.8762\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 8.1679 - acc: 0.9066 - val_loss: 12.3389 - val_acc: 0.8632\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 8.0256 - acc: 0.9082 - val_loss: 11.2349 - val_acc: 0.8689\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 31s 51us/step - loss: 53.9732 - acc: 0.3955 - val_loss: 69.7299 - val_acc: 0.3870\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 32.3241 - acc: 0.6608 - val_loss: 40.6843 - val_acc: 0.5787\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 23.5652 - acc: 0.7506 - val_loss: 32.7546 - val_acc: 0.6643\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 19.2273 - acc: 0.7909 - val_loss: 25.1631 - val_acc: 0.7305\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 16.3375 - acc: 0.8213 - val_loss: 19.8101 - val_acc: 0.7762\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 14.3474 - acc: 0.8435 - val_loss: 18.5061 - val_acc: 0.8043\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 12.9743 - acc: 0.8567 - val_loss: 18.9316 - val_acc: 0.8053\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 11.8863 - acc: 0.8685 - val_loss: 13.7923 - val_acc: 0.8366\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 11.1569 - acc: 0.8763 - val_loss: 13.4548 - val_acc: 0.8585\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 10.5117 - acc: 0.8824 - val_loss: 12.9704 - val_acc: 0.8410\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 10.0929 - acc: 0.8871 - val_loss: 11.6815 - val_acc: 0.8677\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 9.5947 - acc: 0.8929 - val_loss: 10.9292 - val_acc: 0.8749\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 9.3280 - acc: 0.8947 - val_loss: 11.5732 - val_acc: 0.8697\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 8.9693 - acc: 0.8988 - val_loss: 12.4048 - val_acc: 0.8703\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 8.5976 - acc: 0.9032 - val_loss: 11.3574 - val_acc: 0.8722\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 8.2316 - acc: 0.9067 - val_loss: 10.0871 - val_acc: 0.8868\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 7.9450 - acc: 0.9100 - val_loss: 9.5579 - val_acc: 0.8929\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 16s 27us/step - loss: 7.7677 - acc: 0.9113 - val_loss: 9.7245 - val_acc: 0.8921\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 7.5754 - acc: 0.9140 - val_loss: 9.7717 - val_acc: 0.8948\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 7.3174 - acc: 0.9166 - val_loss: 10.3043 - val_acc: 0.8810\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8948  \u001b[0m | \u001b[95m 801.4   \u001b[0m | \u001b[95m 0.05494 \u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 34s 55us/step - loss: 53.8186 - acc: 0.4047 - val_loss: 66.8363 - val_acc: 0.4111\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 32.1440 - acc: 0.6596 - val_loss: 42.7328 - val_acc: 0.5794\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 23.0132 - acc: 0.7586 - val_loss: 33.1402 - val_acc: 0.6619\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 18.5507 - acc: 0.7990 - val_loss: 25.1185 - val_acc: 0.7340\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 15.6208 - acc: 0.8294 - val_loss: 18.8414 - val_acc: 0.7934\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 13.8300 - acc: 0.8483 - val_loss: 19.1730 - val_acc: 0.7904\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 12.5845 - acc: 0.8607 - val_loss: 30.1646 - val_acc: 0.6865\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 11.6026 - acc: 0.8717 - val_loss: 12.9560 - val_acc: 0.8517\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 10.8913 - acc: 0.8795 - val_loss: 13.0274 - val_acc: 0.8540\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 10.4332 - acc: 0.8835 - val_loss: 14.1424 - val_acc: 0.8479\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 9.8414 - acc: 0.8899 - val_loss: 11.6453 - val_acc: 0.8756\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 9.4141 - acc: 0.8942 - val_loss: 10.1363 - val_acc: 0.8895\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 9.0345 - acc: 0.8988 - val_loss: 11.3883 - val_acc: 0.8763\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 8.7239 - acc: 0.9019 - val_loss: 10.2687 - val_acc: 0.8857\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 17s 29us/step - loss: 8.3294 - acc: 0.9061 - val_loss: 9.6378 - val_acc: 0.8967\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 8.1227 - acc: 0.9082 - val_loss: 9.3831 - val_acc: 0.8972\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 7.8822 - acc: 0.9106 - val_loss: 8.4492 - val_acc: 0.9106\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 7.7048 - acc: 0.9121 - val_loss: 9.0261 - val_acc: 0.9019\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 17s 28us/step - loss: 7.4116 - acc: 0.9158 - val_loss: 9.8919 - val_acc: 0.8905\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 18s 29us/step - loss: 7.3187 - acc: 0.9157 - val_loss: 9.5051 - val_acc: 0.9016\n",
            "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.9106  \u001b[0m | \u001b[95m 711.8   \u001b[0m | \u001b[95m 0.06494 \u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 41s 67us/step - loss: 95.6462 - acc: 0.2765 - val_loss: 55.9205 - val_acc: 0.3806\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 35.9610 - acc: 0.6189 - val_loss: 25.6114 - val_acc: 0.7425\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 25s 40us/step - loss: 19.1656 - acc: 0.8031 - val_loss: 17.2946 - val_acc: 0.8206\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 14.5273 - acc: 0.8498 - val_loss: 14.4391 - val_acc: 0.8364\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 25s 41us/step - loss: 12.3670 - acc: 0.8717 - val_loss: 11.3819 - val_acc: 0.8795\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 11.0524 - acc: 0.8844 - val_loss: 11.5184 - val_acc: 0.8789\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 10.1187 - acc: 0.8933 - val_loss: 9.6879 - val_acc: 0.8991\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 9.4131 - acc: 0.9005 - val_loss: 10.0536 - val_acc: 0.8927\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 8.8532 - acc: 0.9056 - val_loss: 8.6800 - val_acc: 0.9096\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 25s 41us/step - loss: 8.3855 - acc: 0.9103 - val_loss: 7.9668 - val_acc: 0.9043\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 7.9703 - acc: 0.9139 - val_loss: 7.7736 - val_acc: 0.9199\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 7.6633 - acc: 0.9169 - val_loss: 6.7640 - val_acc: 0.9285\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 7.3602 - acc: 0.9200 - val_loss: 7.2821 - val_acc: 0.9245\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 25s 41us/step - loss: 7.0864 - acc: 0.9224 - val_loss: 6.9482 - val_acc: 0.9250\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 6.9151 - acc: 0.9243 - val_loss: 6.2235 - val_acc: 0.9310\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 6.7150 - acc: 0.9252 - val_loss: 6.5158 - val_acc: 0.9224\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 25s 40us/step - loss: 6.5540 - acc: 0.9272 - val_loss: 6.2419 - val_acc: 0.9301\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 6.3894 - acc: 0.9283 - val_loss: 5.8185 - val_acc: 0.9319\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 6.2439 - acc: 0.9299 - val_loss: 6.1456 - val_acc: 0.9324\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 24s 40us/step - loss: 6.0963 - acc: 0.9314 - val_loss: 5.2864 - val_acc: 0.9429\n",
            "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.9429  \u001b[0m | \u001b[95m 500.0   \u001b[0m | \u001b[95m 0.001   \u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 29s 48us/step - loss: 130.6199 - acc: 0.2486 - val_loss: 107.4874 - val_acc: 0.2494\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 14s 22us/step - loss: 64.4612 - acc: 0.3617 - val_loss: 55.8829 - val_acc: 0.4605\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 37.3124 - acc: 0.6246 - val_loss: 30.9845 - val_acc: 0.6910\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 13s 21us/step - loss: 20.9443 - acc: 0.7917 - val_loss: 19.6265 - val_acc: 0.7958\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 14s 22us/step - loss: 15.1130 - acc: 0.8441 - val_loss: 15.0781 - val_acc: 0.8463\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 12.6223 - acc: 0.8687 - val_loss: 12.6177 - val_acc: 0.8710\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 14s 22us/step - loss: 11.1837 - acc: 0.8842 - val_loss: 12.0066 - val_acc: 0.8661\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 10.1433 - acc: 0.8942 - val_loss: 10.3311 - val_acc: 0.8929\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 14s 22us/step - loss: 9.3957 - acc: 0.9016 - val_loss: 9.7233 - val_acc: 0.8969\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 14s 22us/step - loss: 8.8183 - acc: 0.9072 - val_loss: 11.1375 - val_acc: 0.8818\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 8.3219 - acc: 0.9117 - val_loss: 8.5830 - val_acc: 0.9120\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 14s 22us/step - loss: 7.9482 - acc: 0.9156 - val_loss: 8.2345 - val_acc: 0.9143\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 7.6286 - acc: 0.9187 - val_loss: 7.5027 - val_acc: 0.9206\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 7.3149 - acc: 0.9221 - val_loss: 7.0529 - val_acc: 0.9262\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 7.0386 - acc: 0.9244 - val_loss: 8.1647 - val_acc: 0.9125\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 6.8485 - acc: 0.9262 - val_loss: 7.2692 - val_acc: 0.9198\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 6.6304 - acc: 0.9275 - val_loss: 6.7645 - val_acc: 0.9225\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 14s 22us/step - loss: 6.4499 - acc: 0.9302 - val_loss: 6.4583 - val_acc: 0.9307\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 14s 22us/step - loss: 6.2859 - acc: 0.9322 - val_loss: 6.4691 - val_acc: 0.9317\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 13s 22us/step - loss: 6.1373 - acc: 0.9332 - val_loss: 6.1159 - val_acc: 0.9333\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 37s 61us/step - loss: 71.5866 - acc: 0.3823 - val_loss: 47.7477 - val_acc: 0.5045\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 27.7825 - acc: 0.7117 - val_loss: 24.2846 - val_acc: 0.7546\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 18.0066 - acc: 0.8130 - val_loss: 18.5626 - val_acc: 0.8027\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 13.9856 - acc: 0.8539 - val_loss: 13.8762 - val_acc: 0.8542\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 11.6720 - acc: 0.8768 - val_loss: 11.6904 - val_acc: 0.8702\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 10.2476 - acc: 0.8912 - val_loss: 10.1238 - val_acc: 0.8849\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 9.2557 - acc: 0.9006 - val_loss: 9.4985 - val_acc: 0.8986\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 8.5433 - acc: 0.9070 - val_loss: 9.1522 - val_acc: 0.9031\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 7.9878 - acc: 0.9125 - val_loss: 7.9545 - val_acc: 0.9126\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 7.5373 - acc: 0.9172 - val_loss: 7.9165 - val_acc: 0.9133\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 7.1857 - acc: 0.9208 - val_loss: 7.6247 - val_acc: 0.9124\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 6.9053 - acc: 0.9231 - val_loss: 7.5511 - val_acc: 0.9188\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 6.6698 - acc: 0.9252 - val_loss: 6.5982 - val_acc: 0.9327\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 6.4378 - acc: 0.9276 - val_loss: 5.8044 - val_acc: 0.9336\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 6.2568 - acc: 0.9292 - val_loss: 6.4542 - val_acc: 0.9262\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 6.1016 - acc: 0.9301 - val_loss: 6.1615 - val_acc: 0.9268\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 5.9128 - acc: 0.9323 - val_loss: 5.3742 - val_acc: 0.9435\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 5.7850 - acc: 0.9334 - val_loss: 5.5774 - val_acc: 0.9300\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 5.6447 - acc: 0.9351 - val_loss: 5.1915 - val_acc: 0.9378\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 5.5195 - acc: 0.9360 - val_loss: 5.3036 - val_acc: 0.9399\n",
            "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.9435  \u001b[0m | \u001b[95m 585.1   \u001b[0m | \u001b[95m 0.002371\u001b[0m |\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 31s 51us/step - loss: 70.1614 - acc: 0.3421 - val_loss: 90.4322 - val_acc: 0.2690\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 33.5968 - acc: 0.6432 - val_loss: 35.8183 - val_acc: 0.6284\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 20.9146 - acc: 0.7828 - val_loss: 21.1226 - val_acc: 0.7833\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 14s 24us/step - loss: 15.5151 - acc: 0.8356 - val_loss: 16.9081 - val_acc: 0.8225\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 14s 24us/step - loss: 12.4349 - acc: 0.8667 - val_loss: 13.5274 - val_acc: 0.8592\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 10.5513 - acc: 0.8852 - val_loss: 10.4924 - val_acc: 0.8846\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 9.2966 - acc: 0.8981 - val_loss: 10.8372 - val_acc: 0.8901\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 8.4654 - acc: 0.9064 - val_loss: 10.3411 - val_acc: 0.8799\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 14s 24us/step - loss: 7.8741 - acc: 0.9129 - val_loss: 8.7274 - val_acc: 0.9045\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 7.3904 - acc: 0.9169 - val_loss: 8.1548 - val_acc: 0.9080\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 14s 24us/step - loss: 6.9793 - acc: 0.9212 - val_loss: 7.8509 - val_acc: 0.9026\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 6.6536 - acc: 0.9250 - val_loss: 7.2065 - val_acc: 0.9235\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 14s 24us/step - loss: 6.3997 - acc: 0.9271 - val_loss: 7.4209 - val_acc: 0.9181\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 14s 24us/step - loss: 6.1990 - acc: 0.9287 - val_loss: 6.7236 - val_acc: 0.9203\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 14s 24us/step - loss: 6.0579 - acc: 0.9305 - val_loss: 6.6672 - val_acc: 0.9196\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 14s 24us/step - loss: 5.8612 - acc: 0.9318 - val_loss: 6.4891 - val_acc: 0.9173\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 14s 24us/step - loss: 5.6981 - acc: 0.9335 - val_loss: 5.9798 - val_acc: 0.9325\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 5.4875 - acc: 0.9355 - val_loss: 5.5212 - val_acc: 0.9282\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 5.3993 - acc: 0.9365 - val_loss: 6.2365 - val_acc: 0.9301\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 5.3221 - acc: 0.9371 - val_loss: 5.5011 - val_acc: 0.9430\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 37s 61us/step - loss: 54.1720 - acc: 0.3636 - val_loss: 62.9539 - val_acc: 0.3383\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 38.9094 - acc: 0.5764 - val_loss: 48.2843 - val_acc: 0.5298\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 27.2628 - acc: 0.7136 - val_loss: 37.0622 - val_acc: 0.6234\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 23.4187 - acc: 0.7478 - val_loss: 30.4666 - val_acc: 0.6888\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 20.6933 - acc: 0.7736 - val_loss: 26.4514 - val_acc: 0.7221\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 18.9144 - acc: 0.7919 - val_loss: 21.8256 - val_acc: 0.7657\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 17.4805 - acc: 0.8062 - val_loss: 21.2509 - val_acc: 0.7644\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 16.5307 - acc: 0.8164 - val_loss: 20.1534 - val_acc: 0.7891\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 15.6243 - acc: 0.8262 - val_loss: 20.0148 - val_acc: 0.7772\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 14.9064 - acc: 0.8344 - val_loss: 19.1502 - val_acc: 0.7981\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 14.3176 - acc: 0.8403 - val_loss: 17.3816 - val_acc: 0.7850\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 13.7038 - acc: 0.8472 - val_loss: 17.4022 - val_acc: 0.8146\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 13.1667 - acc: 0.8533 - val_loss: 15.9006 - val_acc: 0.8198\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 12.6634 - acc: 0.8579 - val_loss: 15.9676 - val_acc: 0.8351\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 12.2480 - acc: 0.8629 - val_loss: 17.0121 - val_acc: 0.8148\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 11.8764 - acc: 0.8676 - val_loss: 14.6590 - val_acc: 0.8386\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 11.5780 - acc: 0.8708 - val_loss: 14.5531 - val_acc: 0.8373\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 11.1713 - acc: 0.8751 - val_loss: 13.6339 - val_acc: 0.8571\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 10.8880 - acc: 0.8786 - val_loss: 13.1156 - val_acc: 0.8667\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 10.7024 - acc: 0.8806 - val_loss: 13.0391 - val_acc: 0.8625\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 40s 66us/step - loss: 99.1743 - acc: 0.2768 - val_loss: 60.3166 - val_acc: 0.3416\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 22s 37us/step - loss: 36.5124 - acc: 0.6194 - val_loss: 26.4353 - val_acc: 0.7272\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 23s 37us/step - loss: 19.2262 - acc: 0.8023 - val_loss: 17.3370 - val_acc: 0.8144\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 23s 38us/step - loss: 14.5783 - acc: 0.8488 - val_loss: 14.8067 - val_acc: 0.8426\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 23s 37us/step - loss: 12.3942 - acc: 0.8711 - val_loss: 12.3831 - val_acc: 0.8736\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 23s 38us/step - loss: 11.0587 - acc: 0.8838 - val_loss: 10.7334 - val_acc: 0.8865\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 23s 37us/step - loss: 10.1099 - acc: 0.8940 - val_loss: 10.1041 - val_acc: 0.8940\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 23s 37us/step - loss: 9.4043 - acc: 0.9009 - val_loss: 9.2012 - val_acc: 0.9041\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 23s 38us/step - loss: 8.8092 - acc: 0.9066 - val_loss: 8.0789 - val_acc: 0.9139\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 23s 38us/step - loss: 8.3751 - acc: 0.9106 - val_loss: 7.9038 - val_acc: 0.9200\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 23s 37us/step - loss: 7.9891 - acc: 0.9146 - val_loss: 7.3478 - val_acc: 0.9202\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 23s 37us/step - loss: 7.6758 - acc: 0.9172 - val_loss: 7.0489 - val_acc: 0.9222\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 23s 37us/step - loss: 7.4251 - acc: 0.9194 - val_loss: 6.9321 - val_acc: 0.9222\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 23s 38us/step - loss: 7.1464 - acc: 0.9223 - val_loss: 6.8648 - val_acc: 0.9262\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 22s 37us/step - loss: 6.9510 - acc: 0.9239 - val_loss: 6.2424 - val_acc: 0.9340\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 23s 37us/step - loss: 6.7451 - acc: 0.9259 - val_loss: 6.2929 - val_acc: 0.9265\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 23s 38us/step - loss: 6.5603 - acc: 0.9276 - val_loss: 5.8822 - val_acc: 0.9354\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 22s 37us/step - loss: 6.4019 - acc: 0.9290 - val_loss: 6.6603 - val_acc: 0.9275\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 23s 38us/step - loss: 6.2684 - acc: 0.9306 - val_loss: 5.8410 - val_acc: 0.9323\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 23s 38us/step - loss: 6.1492 - acc: 0.9317 - val_loss: 5.6285 - val_acc: 0.9353\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 33s 55us/step - loss: 126.0340 - acc: 0.2461 - val_loss: 109.7559 - val_acc: 0.2608\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 58.6599 - acc: 0.3892 - val_loss: 47.7286 - val_acc: 0.5110\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 31.1205 - acc: 0.6890 - val_loss: 25.7865 - val_acc: 0.7419\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 16s 26us/step - loss: 18.0037 - acc: 0.8155 - val_loss: 17.5878 - val_acc: 0.8118\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 13.9637 - acc: 0.8550 - val_loss: 14.9699 - val_acc: 0.8374\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 12.0020 - acc: 0.8752 - val_loss: 11.9313 - val_acc: 0.8700\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 10.7383 - acc: 0.8877 - val_loss: 10.5102 - val_acc: 0.8929\n",
            "Epoch 8/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 9.8478 - acc: 0.8965 - val_loss: 10.0425 - val_acc: 0.8947\n",
            "Epoch 9/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 9.1920 - acc: 0.9029 - val_loss: 9.7313 - val_acc: 0.8990\n",
            "Epoch 10/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 8.6932 - acc: 0.9084 - val_loss: 8.9787 - val_acc: 0.9097\n",
            "Epoch 11/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 8.2533 - acc: 0.9127 - val_loss: 8.7343 - val_acc: 0.9098\n",
            "Epoch 12/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 7.8912 - acc: 0.9164 - val_loss: 7.9421 - val_acc: 0.9216\n",
            "Epoch 13/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 7.5792 - acc: 0.9186 - val_loss: 7.5137 - val_acc: 0.9206\n",
            "Epoch 14/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 7.2523 - acc: 0.9216 - val_loss: 8.0866 - val_acc: 0.9150\n",
            "Epoch 15/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 7.0372 - acc: 0.9248 - val_loss: 7.8110 - val_acc: 0.9141\n",
            "Epoch 16/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 6.7867 - acc: 0.9264 - val_loss: 6.9954 - val_acc: 0.9300\n",
            "Epoch 17/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 6.6361 - acc: 0.9282 - val_loss: 6.4981 - val_acc: 0.9279\n",
            "Epoch 18/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 6.4551 - acc: 0.9295 - val_loss: 6.4284 - val_acc: 0.9344\n",
            "Epoch 19/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 6.3072 - acc: 0.9310 - val_loss: 6.0193 - val_acc: 0.9348\n",
            "Epoch 20/20\n",
            "607500/607500 [==============================] - 15s 25us/step - loss: 6.1715 - acc: 0.9318 - val_loss: 5.6189 - val_acc: 0.9315\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/20\n",
            "607500/607500 [==============================] - 33s 54us/step - loss: 53.5704 - acc: 0.4110 - val_loss: 69.9291 - val_acc: 0.3727\n",
            "Epoch 2/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 35.7623 - acc: 0.6186 - val_loss: 49.6185 - val_acc: 0.5295\n",
            "Epoch 3/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 26.6028 - acc: 0.7201 - val_loss: 42.5920 - val_acc: 0.5875\n",
            "Epoch 4/20\n",
            "607500/607500 [==============================] - 15s 24us/step - loss: 22.4516 - acc: 0.7603 - val_loss: 29.4703 - val_acc: 0.6993\n",
            "Epoch 5/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 19.3084 - acc: 0.7890 - val_loss: 24.5989 - val_acc: 0.7516\n",
            "Epoch 6/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 17.2208 - acc: 0.8096 - val_loss: 22.1417 - val_acc: 0.7567\n",
            "Epoch 7/20\n",
            "607500/607500 [==============================] - 14s 23us/step - loss: 15.6444 - acc: 0.8239 - val_loss: 21.2508 - val_acc: 0.7744\n",
            "Epoch 8/20\n",
            "238602/607500 [==========>...................] - ETA: 7s - loss: 14.7951 - acc: 0.8346Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e97wL4b-sUbv",
        "colab_type": "code",
        "outputId": "f99e817b-ded6-44b8-c564-5caf8bcd9c17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print('Final result: ', bayes_optimizer.max)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final result:  {'target': 0.938355541283996, 'params': {'batch_size': 622.4291036969094, 'learning_rate': 0.0013658127139896644}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVmZsQnvu6Fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE8elkjNuuuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam = keras.optimizers.Adam(0.001)\n",
        "model.compile(loss='mae', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1SbDK8puxRl",
        "colab_type": "code",
        "outputId": "bda1a081-efd9-49b2-caa6-a40b905d79c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist = model.fit(train_X, train_Y, epochs=200, batch_size=630,\n",
        "                    validation_data=(val_X, val_Y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/200\n",
            "607500/607500 [==============================] - 35s 57us/step - loss: 105.9699 - acc: 0.2801 - val_loss: 66.0642 - val_acc: 0.2597\n",
            "Epoch 2/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 44.3766 - acc: 0.5170 - val_loss: 36.7647 - val_acc: 0.6344\n",
            "Epoch 3/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 22.9436 - acc: 0.7675 - val_loss: 22.3326 - val_acc: 0.7722\n",
            "Epoch 4/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 16.1465 - acc: 0.8332 - val_loss: 16.1079 - val_acc: 0.8304\n",
            "Epoch 5/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 13.3405 - acc: 0.8613 - val_loss: 12.5230 - val_acc: 0.8711\n",
            "Epoch 6/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 11.6178 - acc: 0.8788 - val_loss: 11.7844 - val_acc: 0.8809\n",
            "Epoch 7/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 10.5183 - acc: 0.8900 - val_loss: 10.0391 - val_acc: 0.8950\n",
            "Epoch 8/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 9.6543 - acc: 0.8980 - val_loss: 9.4185 - val_acc: 0.9013\n",
            "Epoch 9/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 9.0491 - acc: 0.9039 - val_loss: 9.3026 - val_acc: 0.8975\n",
            "Epoch 10/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 8.5462 - acc: 0.9088 - val_loss: 8.3172 - val_acc: 0.9130\n",
            "Epoch 11/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 8.1295 - acc: 0.9124 - val_loss: 7.6560 - val_acc: 0.9177\n",
            "Epoch 12/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 7.7898 - acc: 0.9154 - val_loss: 7.5357 - val_acc: 0.9193\n",
            "Epoch 13/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 7.5107 - acc: 0.9186 - val_loss: 7.2588 - val_acc: 0.9181\n",
            "Epoch 14/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 7.2169 - acc: 0.9209 - val_loss: 7.2234 - val_acc: 0.9269\n",
            "Epoch 15/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 7.0165 - acc: 0.9233 - val_loss: 6.4592 - val_acc: 0.9219\n",
            "Epoch 16/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 6.8226 - acc: 0.9250 - val_loss: 6.2756 - val_acc: 0.9246\n",
            "Epoch 17/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 6.6173 - acc: 0.9271 - val_loss: 6.2245 - val_acc: 0.9325\n",
            "Epoch 18/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 6.4660 - acc: 0.9283 - val_loss: 6.4452 - val_acc: 0.9310\n",
            "Epoch 19/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 6.2872 - acc: 0.9303 - val_loss: 6.1795 - val_acc: 0.9249\n",
            "Epoch 20/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 6.1701 - acc: 0.9312 - val_loss: 6.4019 - val_acc: 0.9284\n",
            "Epoch 21/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 6.0256 - acc: 0.9324 - val_loss: 5.6419 - val_acc: 0.9321\n",
            "Epoch 22/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 5.9078 - acc: 0.9335 - val_loss: 5.4357 - val_acc: 0.9367\n",
            "Epoch 23/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 5.8356 - acc: 0.9343 - val_loss: 5.3004 - val_acc: 0.9404\n",
            "Epoch 24/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 5.7034 - acc: 0.9353 - val_loss: 5.8346 - val_acc: 0.9319\n",
            "Epoch 25/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 5.6260 - acc: 0.9362 - val_loss: 5.1394 - val_acc: 0.9388\n",
            "Epoch 26/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 5.5403 - acc: 0.9372 - val_loss: 6.0335 - val_acc: 0.9322\n",
            "Epoch 27/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 5.4667 - acc: 0.9381 - val_loss: 5.0584 - val_acc: 0.9446\n",
            "Epoch 28/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 5.3775 - acc: 0.9388 - val_loss: 5.1217 - val_acc: 0.9438\n",
            "Epoch 29/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 5.2754 - acc: 0.9392 - val_loss: 4.6512 - val_acc: 0.9479\n",
            "Epoch 30/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 5.2831 - acc: 0.9395 - val_loss: 5.3785 - val_acc: 0.9391\n",
            "Epoch 31/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 5.1746 - acc: 0.9402 - val_loss: 4.8908 - val_acc: 0.9452\n",
            "Epoch 32/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 5.1026 - acc: 0.9411 - val_loss: 4.2403 - val_acc: 0.9501\n",
            "Epoch 33/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 5.0634 - acc: 0.9412 - val_loss: 4.2424 - val_acc: 0.9555\n",
            "Epoch 34/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 4.9996 - acc: 0.9422 - val_loss: 4.5167 - val_acc: 0.9404\n",
            "Epoch 35/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 4.9569 - acc: 0.9425 - val_loss: 4.3122 - val_acc: 0.9485\n",
            "Epoch 36/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 4.9055 - acc: 0.9430 - val_loss: 4.1718 - val_acc: 0.9496\n",
            "Epoch 37/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 4.8277 - acc: 0.9435 - val_loss: 4.2152 - val_acc: 0.9459\n",
            "Epoch 38/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 4.7829 - acc: 0.9440 - val_loss: 4.4010 - val_acc: 0.9493\n",
            "Epoch 39/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 4.7311 - acc: 0.9440 - val_loss: 4.2072 - val_acc: 0.9484\n",
            "Epoch 40/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 4.6974 - acc: 0.9448 - val_loss: 4.2449 - val_acc: 0.9457\n",
            "Epoch 41/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 4.6553 - acc: 0.9454 - val_loss: 4.5720 - val_acc: 0.9446\n",
            "Epoch 42/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 4.6215 - acc: 0.9456 - val_loss: 4.1238 - val_acc: 0.9484\n",
            "Epoch 43/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 4.5757 - acc: 0.9458 - val_loss: 4.0253 - val_acc: 0.9511\n",
            "Epoch 44/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 4.5579 - acc: 0.9458 - val_loss: 3.8559 - val_acc: 0.9524\n",
            "Epoch 45/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 4.5195 - acc: 0.9462 - val_loss: 4.0210 - val_acc: 0.9445\n",
            "Epoch 46/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 4.4739 - acc: 0.9464 - val_loss: 3.8226 - val_acc: 0.9470\n",
            "Epoch 47/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 4.4380 - acc: 0.9471 - val_loss: 3.9338 - val_acc: 0.9530\n",
            "Epoch 48/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 4.4048 - acc: 0.9471 - val_loss: 3.8598 - val_acc: 0.9537\n",
            "Epoch 49/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 4.3691 - acc: 0.9474 - val_loss: 4.2153 - val_acc: 0.9496\n",
            "Epoch 50/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 4.3593 - acc: 0.9473 - val_loss: 3.6858 - val_acc: 0.9553\n",
            "Epoch 51/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 4.3082 - acc: 0.9485 - val_loss: 3.8142 - val_acc: 0.9518\n",
            "Epoch 52/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 4.2964 - acc: 0.9483 - val_loss: 3.7345 - val_acc: 0.9530\n",
            "Epoch 53/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 4.2779 - acc: 0.9483 - val_loss: 4.0596 - val_acc: 0.9517\n",
            "Epoch 54/200\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 4.2151 - acc: 0.9491 - val_loss: 3.5882 - val_acc: 0.9554\n",
            "Epoch 55/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 4.2101 - acc: 0.9489 - val_loss: 3.7157 - val_acc: 0.9500\n",
            "Epoch 56/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 4.1817 - acc: 0.9492 - val_loss: 3.9835 - val_acc: 0.9537\n",
            "Epoch 57/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 4.1522 - acc: 0.9494 - val_loss: 3.4716 - val_acc: 0.9559\n",
            "Epoch 58/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 4.1299 - acc: 0.9497 - val_loss: 3.5110 - val_acc: 0.9473\n",
            "Epoch 59/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 4.0953 - acc: 0.9502 - val_loss: 3.4174 - val_acc: 0.9576\n",
            "Epoch 60/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 4.0753 - acc: 0.9503 - val_loss: 3.7625 - val_acc: 0.9551\n",
            "Epoch 61/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 4.0479 - acc: 0.9504 - val_loss: 3.3028 - val_acc: 0.9610\n",
            "Epoch 62/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 4.0475 - acc: 0.9504 - val_loss: 3.7817 - val_acc: 0.9533\n",
            "Epoch 63/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.9893 - acc: 0.9512 - val_loss: 3.3814 - val_acc: 0.9554\n",
            "Epoch 64/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 3.9930 - acc: 0.9510 - val_loss: 3.2442 - val_acc: 0.9561\n",
            "Epoch 65/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.9685 - acc: 0.9512 - val_loss: 3.4719 - val_acc: 0.9547\n",
            "Epoch 66/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.9228 - acc: 0.9512 - val_loss: 3.4652 - val_acc: 0.9600\n",
            "Epoch 67/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 3.9155 - acc: 0.9514 - val_loss: 3.3390 - val_acc: 0.9527\n",
            "Epoch 68/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.9058 - acc: 0.9517 - val_loss: 3.4313 - val_acc: 0.9508\n",
            "Epoch 69/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.9027 - acc: 0.9518 - val_loss: 3.3032 - val_acc: 0.9564\n",
            "Epoch 70/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.8922 - acc: 0.9518 - val_loss: 3.1609 - val_acc: 0.9551\n",
            "Epoch 71/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.8322 - acc: 0.9522 - val_loss: 3.2978 - val_acc: 0.9536\n",
            "Epoch 72/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 3.8332 - acc: 0.9524 - val_loss: 3.2911 - val_acc: 0.9558\n",
            "Epoch 73/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 3.7863 - acc: 0.9526 - val_loss: 3.1428 - val_acc: 0.9574\n",
            "Epoch 74/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.7895 - acc: 0.9527 - val_loss: 3.1529 - val_acc: 0.9663\n",
            "Epoch 75/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.7704 - acc: 0.9527 - val_loss: 3.2634 - val_acc: 0.9566\n",
            "Epoch 76/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 3.7821 - acc: 0.9526 - val_loss: 3.1906 - val_acc: 0.9530\n",
            "Epoch 77/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.7265 - acc: 0.9527 - val_loss: 3.0697 - val_acc: 0.9536\n",
            "Epoch 78/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.7242 - acc: 0.9526 - val_loss: 3.0955 - val_acc: 0.9572\n",
            "Epoch 79/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 3.6941 - acc: 0.9535 - val_loss: 3.3007 - val_acc: 0.9625\n",
            "Epoch 80/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.6867 - acc: 0.9534 - val_loss: 3.1882 - val_acc: 0.9550\n",
            "Epoch 81/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.6742 - acc: 0.9536 - val_loss: 2.8938 - val_acc: 0.9632\n",
            "Epoch 82/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 3.6600 - acc: 0.9535 - val_loss: 3.0930 - val_acc: 0.9617\n",
            "Epoch 83/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.6457 - acc: 0.9541 - val_loss: 3.0110 - val_acc: 0.9585\n",
            "Epoch 84/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.6177 - acc: 0.9538 - val_loss: 3.3004 - val_acc: 0.9641\n",
            "Epoch 85/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 3.6016 - acc: 0.9540 - val_loss: 3.1809 - val_acc: 0.9566\n",
            "Epoch 86/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.5951 - acc: 0.9541 - val_loss: 2.8289 - val_acc: 0.9589\n",
            "Epoch 87/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 3.5622 - acc: 0.9545 - val_loss: 3.0681 - val_acc: 0.9599\n",
            "Epoch 88/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 3.5734 - acc: 0.9543 - val_loss: 3.0711 - val_acc: 0.9594\n",
            "Epoch 89/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 3.5293 - acc: 0.9547 - val_loss: 3.0145 - val_acc: 0.9553\n",
            "Epoch 90/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.5484 - acc: 0.9548 - val_loss: 3.1113 - val_acc: 0.9582\n",
            "Epoch 91/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 3.5077 - acc: 0.9544 - val_loss: 3.0370 - val_acc: 0.9577\n",
            "Epoch 92/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.5020 - acc: 0.9545 - val_loss: 2.8490 - val_acc: 0.9564\n",
            "Epoch 93/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 3.5014 - acc: 0.9550 - val_loss: 2.7738 - val_acc: 0.9635\n",
            "Epoch 94/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.5035 - acc: 0.9551 - val_loss: 3.0793 - val_acc: 0.9613\n",
            "Epoch 95/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.4862 - acc: 0.9545 - val_loss: 2.8429 - val_acc: 0.9592\n",
            "Epoch 96/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.4547 - acc: 0.9550 - val_loss: 3.0116 - val_acc: 0.9600\n",
            "Epoch 97/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.4273 - acc: 0.9550 - val_loss: 2.8073 - val_acc: 0.9578\n",
            "Epoch 98/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.4284 - acc: 0.9555 - val_loss: 2.7914 - val_acc: 0.9634\n",
            "Epoch 99/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 3.4271 - acc: 0.9552 - val_loss: 3.2082 - val_acc: 0.9563\n",
            "Epoch 100/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.4042 - acc: 0.9556 - val_loss: 2.9666 - val_acc: 0.9619\n",
            "Epoch 101/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.3910 - acc: 0.9557 - val_loss: 2.8413 - val_acc: 0.9565\n",
            "Epoch 102/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 3.3859 - acc: 0.9555 - val_loss: 2.7695 - val_acc: 0.9604\n",
            "Epoch 103/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 3.3680 - acc: 0.9558 - val_loss: 2.7921 - val_acc: 0.9636\n",
            "Epoch 104/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.3439 - acc: 0.9558 - val_loss: 2.8730 - val_acc: 0.9560\n",
            "Epoch 105/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.3528 - acc: 0.9559 - val_loss: 2.7184 - val_acc: 0.9559\n",
            "Epoch 106/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.3438 - acc: 0.9558 - val_loss: 2.8021 - val_acc: 0.9612\n",
            "Epoch 107/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.3319 - acc: 0.9561 - val_loss: 2.9849 - val_acc: 0.9578\n",
            "Epoch 108/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.3117 - acc: 0.9564 - val_loss: 2.7676 - val_acc: 0.9587\n",
            "Epoch 109/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.3098 - acc: 0.9558 - val_loss: 2.8493 - val_acc: 0.9583\n",
            "Epoch 110/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.2970 - acc: 0.9565 - val_loss: 2.6400 - val_acc: 0.9582\n",
            "Epoch 111/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 3.2697 - acc: 0.9562 - val_loss: 2.8000 - val_acc: 0.9595\n",
            "Epoch 112/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.2628 - acc: 0.9567 - val_loss: 2.7111 - val_acc: 0.9555\n",
            "Epoch 113/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 3.2490 - acc: 0.9568 - val_loss: 2.6498 - val_acc: 0.9552\n",
            "Epoch 114/200\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 3.2381 - acc: 0.9568 - val_loss: 2.7390 - val_acc: 0.9654\n",
            "Epoch 115/200\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 3.2251 - acc: 0.9569 - val_loss: 2.5655 - val_acc: 0.9610\n",
            "Epoch 116/200\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 3.2284 - acc: 0.9567 - val_loss: 2.8737 - val_acc: 0.9576\n",
            "Epoch 117/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 3.2278 - acc: 0.9571 - val_loss: 2.7805 - val_acc: 0.9629\n",
            "Epoch 118/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 3.2000 - acc: 0.9573 - val_loss: 2.7454 - val_acc: 0.9582\n",
            "Epoch 119/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.2093 - acc: 0.9567 - val_loss: 2.6438 - val_acc: 0.9606\n",
            "Epoch 120/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.1890 - acc: 0.9571 - val_loss: 2.6481 - val_acc: 0.9628\n",
            "Epoch 121/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.2035 - acc: 0.9571 - val_loss: 2.5073 - val_acc: 0.9624\n",
            "Epoch 122/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 3.1680 - acc: 0.9574 - val_loss: 2.5808 - val_acc: 0.9636\n",
            "Epoch 123/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.1621 - acc: 0.9573 - val_loss: 2.6189 - val_acc: 0.9600\n",
            "Epoch 124/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.1415 - acc: 0.9576 - val_loss: 2.7132 - val_acc: 0.9580\n",
            "Epoch 125/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.1273 - acc: 0.9575 - val_loss: 2.7142 - val_acc: 0.9572\n",
            "Epoch 126/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.1453 - acc: 0.9575 - val_loss: 2.6407 - val_acc: 0.9631\n",
            "Epoch 127/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.1380 - acc: 0.9574 - val_loss: 2.8285 - val_acc: 0.9654\n",
            "Epoch 128/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.1246 - acc: 0.9575 - val_loss: 2.6401 - val_acc: 0.9620\n",
            "Epoch 129/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.1238 - acc: 0.9574 - val_loss: 2.4879 - val_acc: 0.9676\n",
            "Epoch 130/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.1155 - acc: 0.9576 - val_loss: 2.6044 - val_acc: 0.9621\n",
            "Epoch 131/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.0960 - acc: 0.9578 - val_loss: 2.5877 - val_acc: 0.9603\n",
            "Epoch 132/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.0981 - acc: 0.9577 - val_loss: 2.5521 - val_acc: 0.9644\n",
            "Epoch 133/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.0812 - acc: 0.9576 - val_loss: 2.6946 - val_acc: 0.9584\n",
            "Epoch 134/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.0847 - acc: 0.9579 - val_loss: 2.5247 - val_acc: 0.9606\n",
            "Epoch 135/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 3.0606 - acc: 0.9581 - val_loss: 2.5286 - val_acc: 0.9646\n",
            "Epoch 136/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.0576 - acc: 0.9582 - val_loss: 2.5008 - val_acc: 0.9629\n",
            "Epoch 137/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 3.0645 - acc: 0.9579 - val_loss: 2.6323 - val_acc: 0.9600\n",
            "Epoch 138/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.0243 - acc: 0.9578 - val_loss: 2.5351 - val_acc: 0.9636\n",
            "Epoch 139/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 3.0177 - acc: 0.9583 - val_loss: 2.5433 - val_acc: 0.9631\n",
            "Epoch 140/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.0301 - acc: 0.9585 - val_loss: 2.5149 - val_acc: 0.9593\n",
            "Epoch 141/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.0271 - acc: 0.9580 - val_loss: 2.4888 - val_acc: 0.9610\n",
            "Epoch 142/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 3.0354 - acc: 0.9582 - val_loss: 2.5365 - val_acc: 0.9641\n",
            "Epoch 143/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.0038 - acc: 0.9587 - val_loss: 2.6039 - val_acc: 0.9574\n",
            "Epoch 144/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.9973 - acc: 0.9585 - val_loss: 2.5212 - val_acc: 0.9594\n",
            "Epoch 145/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 2.9920 - acc: 0.9586 - val_loss: 2.7091 - val_acc: 0.9594\n",
            "Epoch 146/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.0050 - acc: 0.9582 - val_loss: 2.7224 - val_acc: 0.9635\n",
            "Epoch 147/200\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 2.9784 - acc: 0.9588 - val_loss: 2.4375 - val_acc: 0.9635\n",
            "Epoch 148/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.9678 - acc: 0.9590 - val_loss: 2.5726 - val_acc: 0.9582\n",
            "Epoch 149/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.9708 - acc: 0.9587 - val_loss: 2.4716 - val_acc: 0.9632\n",
            "Epoch 150/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.9453 - acc: 0.9588 - val_loss: 2.3890 - val_acc: 0.9619\n",
            "Epoch 151/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.9564 - acc: 0.9587 - val_loss: 2.4037 - val_acc: 0.9626\n",
            "Epoch 152/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.9355 - acc: 0.9585 - val_loss: 2.3729 - val_acc: 0.9591\n",
            "Epoch 153/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 2.9465 - acc: 0.9591 - val_loss: 2.3974 - val_acc: 0.9624\n",
            "Epoch 154/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.9111 - acc: 0.9588 - val_loss: 2.3376 - val_acc: 0.9632\n",
            "Epoch 155/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.9289 - acc: 0.9589 - val_loss: 2.6119 - val_acc: 0.9522\n",
            "Epoch 156/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.9177 - acc: 0.9588 - val_loss: 2.2642 - val_acc: 0.9588\n",
            "Epoch 157/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.9056 - acc: 0.9591 - val_loss: 2.4067 - val_acc: 0.9571\n",
            "Epoch 158/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 2.9102 - acc: 0.9589 - val_loss: 2.4653 - val_acc: 0.9605\n",
            "Epoch 159/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.9018 - acc: 0.9592 - val_loss: 2.5085 - val_acc: 0.9606\n",
            "Epoch 160/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 2.8770 - acc: 0.9594 - val_loss: 2.3455 - val_acc: 0.9585\n",
            "Epoch 161/200\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 2.9058 - acc: 0.9588 - val_loss: 2.3402 - val_acc: 0.9650\n",
            "Epoch 162/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.8715 - acc: 0.9592 - val_loss: 2.3466 - val_acc: 0.9623\n",
            "Epoch 163/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 2.8627 - acc: 0.9591 - val_loss: 2.2712 - val_acc: 0.9590\n",
            "Epoch 164/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.8651 - acc: 0.9592 - val_loss: 2.2587 - val_acc: 0.9604\n",
            "Epoch 165/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.8709 - acc: 0.9592 - val_loss: 2.3203 - val_acc: 0.9600\n",
            "Epoch 166/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.8504 - acc: 0.9596 - val_loss: 2.3289 - val_acc: 0.9616\n",
            "Epoch 167/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.8431 - acc: 0.9594 - val_loss: 2.4361 - val_acc: 0.9584\n",
            "Epoch 168/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.8368 - acc: 0.9595 - val_loss: 2.3195 - val_acc: 0.9608\n",
            "Epoch 169/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.8416 - acc: 0.9593 - val_loss: 2.4449 - val_acc: 0.9616\n",
            "Epoch 170/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.8181 - acc: 0.9595 - val_loss: 2.2157 - val_acc: 0.9647\n",
            "Epoch 171/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.8136 - acc: 0.9597 - val_loss: 2.3023 - val_acc: 0.9566\n",
            "Epoch 172/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.8289 - acc: 0.9597 - val_loss: 2.1785 - val_acc: 0.9614\n",
            "Epoch 173/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.8285 - acc: 0.9595 - val_loss: 2.3794 - val_acc: 0.9627\n",
            "Epoch 174/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 2.8308 - acc: 0.9597 - val_loss: 2.3555 - val_acc: 0.9631\n",
            "Epoch 175/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7883 - acc: 0.9600 - val_loss: 2.2935 - val_acc: 0.9655\n",
            "Epoch 176/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 2.8061 - acc: 0.9597 - val_loss: 2.2400 - val_acc: 0.9611\n",
            "Epoch 177/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7945 - acc: 0.9597 - val_loss: 2.2511 - val_acc: 0.9650\n",
            "Epoch 178/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7809 - acc: 0.9596 - val_loss: 2.1940 - val_acc: 0.9643\n",
            "Epoch 179/200\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 2.7734 - acc: 0.9598 - val_loss: 2.2463 - val_acc: 0.9602\n",
            "Epoch 180/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7835 - acc: 0.9596 - val_loss: 2.2352 - val_acc: 0.9681\n",
            "Epoch 181/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7715 - acc: 0.9600 - val_loss: 2.2341 - val_acc: 0.9609\n",
            "Epoch 182/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7764 - acc: 0.9597 - val_loss: 2.2617 - val_acc: 0.9637\n",
            "Epoch 183/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7682 - acc: 0.9600 - val_loss: 2.2694 - val_acc: 0.9611\n",
            "Epoch 184/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7537 - acc: 0.9602 - val_loss: 2.2929 - val_acc: 0.9573\n",
            "Epoch 185/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.7575 - acc: 0.9602 - val_loss: 2.2857 - val_acc: 0.9641\n",
            "Epoch 186/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7597 - acc: 0.9599 - val_loss: 2.1994 - val_acc: 0.9617\n",
            "Epoch 187/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7505 - acc: 0.9600 - val_loss: 2.2123 - val_acc: 0.9552\n",
            "Epoch 188/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7365 - acc: 0.9600 - val_loss: 2.2707 - val_acc: 0.9652\n",
            "Epoch 189/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7398 - acc: 0.9601 - val_loss: 2.2802 - val_acc: 0.9670\n",
            "Epoch 190/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 2.7336 - acc: 0.9603 - val_loss: 2.2433 - val_acc: 0.9600\n",
            "Epoch 191/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7119 - acc: 0.9600 - val_loss: 2.3709 - val_acc: 0.9675\n",
            "Epoch 192/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7166 - acc: 0.9600 - val_loss: 2.2658 - val_acc: 0.9616\n",
            "Epoch 193/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7022 - acc: 0.9604 - val_loss: 2.1333 - val_acc: 0.9624\n",
            "Epoch 194/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.7087 - acc: 0.9602 - val_loss: 2.3246 - val_acc: 0.9592\n",
            "Epoch 195/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.7034 - acc: 0.9605 - val_loss: 2.2989 - val_acc: 0.9648\n",
            "Epoch 196/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7140 - acc: 0.9599 - val_loss: 2.1743 - val_acc: 0.9632\n",
            "Epoch 197/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.7031 - acc: 0.9605 - val_loss: 2.3796 - val_acc: 0.9619\n",
            "Epoch 198/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.6833 - acc: 0.9606 - val_loss: 2.0937 - val_acc: 0.9607\n",
            "Epoch 199/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.6873 - acc: 0.9604 - val_loss: 2.1381 - val_acc: 0.9602\n",
            "Epoch 200/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.6887 - acc: 0.9601 - val_loss: 2.1106 - val_acc: 0.9586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reiP4ekaoRAx",
        "colab_type": "code",
        "outputId": "7b5f0ed5-a611-438a-bfcb-09db5f4d2219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 모델 아키텍처\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"2397pt\" viewBox=\"0.00 0.00 439.00 1798.00\" width=\"585pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1.3333 1.3333) rotate(0) translate(4 1794)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-1794 435,-1794 435,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140488502397416 -->\n<g class=\"node\" id=\"node1\">\n<title>140488502397416</title>\n<polygon fill=\"none\" points=\"49,-1743.5 49,-1789.5 382,-1789.5 382,-1743.5 49,-1743.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-1762.8\">dense_364_input: InputLayer</text>\n<polyline fill=\"none\" points=\"237,-1743.5 237,-1789.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266\" y=\"-1774.3\">input:</text>\n<polyline fill=\"none\" points=\"237,-1766.5 295,-1766.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266\" y=\"-1751.3\">output:</text>\n<polyline fill=\"none\" points=\"295,-1743.5 295,-1789.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"338.5\" y=\"-1774.3\">(None, 226)</text>\n<polyline fill=\"none\" points=\"295,-1766.5 382,-1766.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"338.5\" y=\"-1751.3\">(None, 226)</text>\n</g>\n<!-- 140488501944616 -->\n<g class=\"node\" id=\"node2\">\n<title>140488501944616</title>\n<polygon fill=\"none\" points=\"82,-1660.5 82,-1706.5 349,-1706.5 349,-1660.5 82,-1660.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-1679.8\">dense_364: Dense</text>\n<polyline fill=\"none\" points=\"204,-1660.5 204,-1706.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-1691.3\">input:</text>\n<polyline fill=\"none\" points=\"204,-1683.5 262,-1683.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-1668.3\">output:</text>\n<polyline fill=\"none\" points=\"262,-1660.5 262,-1706.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-1691.3\">(None, 226)</text>\n<polyline fill=\"none\" points=\"262,-1683.5 349,-1683.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-1668.3\">(None, 239)</text>\n</g>\n<!-- 140488502397416&#45;&gt;140488501944616 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140488502397416-&gt;140488501944616</title>\n<path d=\"M215.5,-1743.3799C215.5,-1735.1745 215.5,-1725.7679 215.5,-1716.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-1716.784 215.5,-1706.784 212.0001,-1716.784 219.0001,-1716.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488502397360 -->\n<g class=\"node\" id=\"node3\">\n<title>140488502397360</title>\n<polygon fill=\"none\" points=\"82,-1577.5 82,-1623.5 349,-1623.5 349,-1577.5 82,-1577.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-1596.8\">dense_365: Dense</text>\n<polyline fill=\"none\" points=\"204,-1577.5 204,-1623.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-1608.3\">input:</text>\n<polyline fill=\"none\" points=\"204,-1600.5 262,-1600.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-1585.3\">output:</text>\n<polyline fill=\"none\" points=\"262,-1577.5 262,-1623.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-1608.3\">(None, 239)</text>\n<polyline fill=\"none\" points=\"262,-1600.5 349,-1600.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-1585.3\">(None, 239)</text>\n</g>\n<!-- 140488501944616&#45;&gt;140488502397360 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140488501944616-&gt;140488502397360</title>\n<path d=\"M215.5,-1660.3799C215.5,-1652.1745 215.5,-1642.7679 215.5,-1633.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-1633.784 215.5,-1623.784 212.0001,-1633.784 219.0001,-1633.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488502133872 -->\n<g class=\"node\" id=\"node4\">\n<title>140488502133872</title>\n<polygon fill=\"none\" points=\"0,-1494.5 0,-1540.5 431,-1540.5 431,-1494.5 0,-1494.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-1513.8\">batch_normalization_166: BatchNormalization</text>\n<polyline fill=\"none\" points=\"286,-1494.5 286,-1540.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315\" y=\"-1525.3\">input:</text>\n<polyline fill=\"none\" points=\"286,-1517.5 344,-1517.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315\" y=\"-1502.3\">output:</text>\n<polyline fill=\"none\" points=\"344,-1494.5 344,-1540.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387.5\" y=\"-1525.3\">(None, 239)</text>\n<polyline fill=\"none\" points=\"344,-1517.5 431,-1517.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387.5\" y=\"-1502.3\">(None, 239)</text>\n</g>\n<!-- 140488502397360&#45;&gt;140488502133872 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140488502397360-&gt;140488502133872</title>\n<path d=\"M215.5,-1577.3799C215.5,-1569.1745 215.5,-1559.7679 215.5,-1550.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-1550.784 215.5,-1540.784 212.0001,-1550.784 219.0001,-1550.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488502398760 -->\n<g class=\"node\" id=\"node5\">\n<title>140488502398760</title>\n<polygon fill=\"none\" points=\"58.5,-1411.5 58.5,-1457.5 372.5,-1457.5 372.5,-1411.5 58.5,-1411.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-1430.8\">activation_166: Activation</text>\n<polyline fill=\"none\" points=\"227.5,-1411.5 227.5,-1457.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256.5\" y=\"-1442.3\">input:</text>\n<polyline fill=\"none\" points=\"227.5,-1434.5 285.5,-1434.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256.5\" y=\"-1419.3\">output:</text>\n<polyline fill=\"none\" points=\"285.5,-1411.5 285.5,-1457.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-1442.3\">(None, 239)</text>\n<polyline fill=\"none\" points=\"285.5,-1434.5 372.5,-1434.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-1419.3\">(None, 239)</text>\n</g>\n<!-- 140488502133872&#45;&gt;140488502398760 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140488502133872-&gt;140488502398760</title>\n<path d=\"M215.5,-1494.3799C215.5,-1486.1745 215.5,-1476.7679 215.5,-1467.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-1467.784 215.5,-1457.784 212.0001,-1467.784 219.0001,-1467.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488501797776 -->\n<g class=\"node\" id=\"node6\">\n<title>140488501797776</title>\n<polygon fill=\"none\" points=\"82,-1328.5 82,-1374.5 349,-1374.5 349,-1328.5 82,-1328.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-1347.8\">dense_366: Dense</text>\n<polyline fill=\"none\" points=\"204,-1328.5 204,-1374.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-1359.3\">input:</text>\n<polyline fill=\"none\" points=\"204,-1351.5 262,-1351.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-1336.3\">output:</text>\n<polyline fill=\"none\" points=\"262,-1328.5 262,-1374.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-1359.3\">(None, 239)</text>\n<polyline fill=\"none\" points=\"262,-1351.5 349,-1351.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-1336.3\">(None, 252)</text>\n</g>\n<!-- 140488502398760&#45;&gt;140488501797776 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140488502398760-&gt;140488501797776</title>\n<path d=\"M215.5,-1411.3799C215.5,-1403.1745 215.5,-1393.7679 215.5,-1384.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-1384.784 215.5,-1374.784 212.0001,-1384.784 219.0001,-1384.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488501212272 -->\n<g class=\"node\" id=\"node7\">\n<title>140488501212272</title>\n<polygon fill=\"none\" points=\"82,-1245.5 82,-1291.5 349,-1291.5 349,-1245.5 82,-1245.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-1264.8\">dense_367: Dense</text>\n<polyline fill=\"none\" points=\"204,-1245.5 204,-1291.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-1276.3\">input:</text>\n<polyline fill=\"none\" points=\"204,-1268.5 262,-1268.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-1253.3\">output:</text>\n<polyline fill=\"none\" points=\"262,-1245.5 262,-1291.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-1276.3\">(None, 252)</text>\n<polyline fill=\"none\" points=\"262,-1268.5 349,-1268.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-1253.3\">(None, 252)</text>\n</g>\n<!-- 140488501797776&#45;&gt;140488501212272 -->\n<g class=\"edge\" id=\"edge6\">\n<title>140488501797776-&gt;140488501212272</title>\n<path d=\"M215.5,-1328.3799C215.5,-1320.1745 215.5,-1310.7679 215.5,-1301.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-1301.784 215.5,-1291.784 212.0001,-1301.784 219.0001,-1301.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488500843800 -->\n<g class=\"node\" id=\"node8\">\n<title>140488500843800</title>\n<polygon fill=\"none\" points=\"0,-1162.5 0,-1208.5 431,-1208.5 431,-1162.5 0,-1162.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-1181.8\">batch_normalization_167: BatchNormalization</text>\n<polyline fill=\"none\" points=\"286,-1162.5 286,-1208.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315\" y=\"-1193.3\">input:</text>\n<polyline fill=\"none\" points=\"286,-1185.5 344,-1185.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315\" y=\"-1170.3\">output:</text>\n<polyline fill=\"none\" points=\"344,-1162.5 344,-1208.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387.5\" y=\"-1193.3\">(None, 252)</text>\n<polyline fill=\"none\" points=\"344,-1185.5 431,-1185.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387.5\" y=\"-1170.3\">(None, 252)</text>\n</g>\n<!-- 140488501212272&#45;&gt;140488500843800 -->\n<g class=\"edge\" id=\"edge7\">\n<title>140488501212272-&gt;140488500843800</title>\n<path d=\"M215.5,-1245.3799C215.5,-1237.1745 215.5,-1227.7679 215.5,-1218.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-1218.784 215.5,-1208.784 212.0001,-1218.784 219.0001,-1218.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488500845088 -->\n<g class=\"node\" id=\"node9\">\n<title>140488500845088</title>\n<polygon fill=\"none\" points=\"58.5,-1079.5 58.5,-1125.5 372.5,-1125.5 372.5,-1079.5 58.5,-1079.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-1098.8\">activation_167: Activation</text>\n<polyline fill=\"none\" points=\"227.5,-1079.5 227.5,-1125.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256.5\" y=\"-1110.3\">input:</text>\n<polyline fill=\"none\" points=\"227.5,-1102.5 285.5,-1102.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256.5\" y=\"-1087.3\">output:</text>\n<polyline fill=\"none\" points=\"285.5,-1079.5 285.5,-1125.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-1110.3\">(None, 252)</text>\n<polyline fill=\"none\" points=\"285.5,-1102.5 372.5,-1102.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-1087.3\">(None, 252)</text>\n</g>\n<!-- 140488500843800&#45;&gt;140488500845088 -->\n<g class=\"edge\" id=\"edge8\">\n<title>140488500843800-&gt;140488500845088</title>\n<path d=\"M215.5,-1162.3799C215.5,-1154.1745 215.5,-1144.7679 215.5,-1135.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-1135.784 215.5,-1125.784 212.0001,-1135.784 219.0001,-1135.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488500870840 -->\n<g class=\"node\" id=\"node10\">\n<title>140488500870840</title>\n<polygon fill=\"none\" points=\"82,-996.5 82,-1042.5 349,-1042.5 349,-996.5 82,-996.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-1015.8\">dense_368: Dense</text>\n<polyline fill=\"none\" points=\"204,-996.5 204,-1042.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-1027.3\">input:</text>\n<polyline fill=\"none\" points=\"204,-1019.5 262,-1019.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-1004.3\">output:</text>\n<polyline fill=\"none\" points=\"262,-996.5 262,-1042.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-1027.3\">(None, 252)</text>\n<polyline fill=\"none\" points=\"262,-1019.5 349,-1019.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-1004.3\">(None, 265)</text>\n</g>\n<!-- 140488500845088&#45;&gt;140488500870840 -->\n<g class=\"edge\" id=\"edge9\">\n<title>140488500845088-&gt;140488500870840</title>\n<path d=\"M215.5,-1079.3799C215.5,-1071.1745 215.5,-1061.7679 215.5,-1052.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-1052.784 215.5,-1042.784 212.0001,-1052.784 219.0001,-1052.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488501007360 -->\n<g class=\"node\" id=\"node11\">\n<title>140488501007360</title>\n<polygon fill=\"none\" points=\"82,-913.5 82,-959.5 349,-959.5 349,-913.5 82,-913.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-932.8\">dense_369: Dense</text>\n<polyline fill=\"none\" points=\"204,-913.5 204,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-944.3\">input:</text>\n<polyline fill=\"none\" points=\"204,-936.5 262,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-921.3\">output:</text>\n<polyline fill=\"none\" points=\"262,-913.5 262,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-944.3\">(None, 265)</text>\n<polyline fill=\"none\" points=\"262,-936.5 349,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-921.3\">(None, 265)</text>\n</g>\n<!-- 140488500870840&#45;&gt;140488501007360 -->\n<g class=\"edge\" id=\"edge10\">\n<title>140488500870840-&gt;140488501007360</title>\n<path d=\"M215.5,-996.3799C215.5,-988.1745 215.5,-978.7679 215.5,-969.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-969.784 215.5,-959.784 212.0001,-969.784 219.0001,-969.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488500667392 -->\n<g class=\"node\" id=\"node12\">\n<title>140488500667392</title>\n<polygon fill=\"none\" points=\"0,-830.5 0,-876.5 431,-876.5 431,-830.5 0,-830.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-849.8\">batch_normalization_168: BatchNormalization</text>\n<polyline fill=\"none\" points=\"286,-830.5 286,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315\" y=\"-861.3\">input:</text>\n<polyline fill=\"none\" points=\"286,-853.5 344,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315\" y=\"-838.3\">output:</text>\n<polyline fill=\"none\" points=\"344,-830.5 344,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387.5\" y=\"-861.3\">(None, 265)</text>\n<polyline fill=\"none\" points=\"344,-853.5 431,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387.5\" y=\"-838.3\">(None, 265)</text>\n</g>\n<!-- 140488501007360&#45;&gt;140488500667392 -->\n<g class=\"edge\" id=\"edge11\">\n<title>140488501007360-&gt;140488500667392</title>\n<path d=\"M215.5,-913.3799C215.5,-905.1745 215.5,-895.7679 215.5,-886.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-886.784 215.5,-876.784 212.0001,-886.784 219.0001,-886.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488500668848 -->\n<g class=\"node\" id=\"node13\">\n<title>140488500668848</title>\n<polygon fill=\"none\" points=\"58.5,-747.5 58.5,-793.5 372.5,-793.5 372.5,-747.5 58.5,-747.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-766.8\">activation_168: Activation</text>\n<polyline fill=\"none\" points=\"227.5,-747.5 227.5,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256.5\" y=\"-778.3\">input:</text>\n<polyline fill=\"none\" points=\"227.5,-770.5 285.5,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256.5\" y=\"-755.3\">output:</text>\n<polyline fill=\"none\" points=\"285.5,-747.5 285.5,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-778.3\">(None, 265)</text>\n<polyline fill=\"none\" points=\"285.5,-770.5 372.5,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-755.3\">(None, 265)</text>\n</g>\n<!-- 140488500667392&#45;&gt;140488500668848 -->\n<g class=\"edge\" id=\"edge12\">\n<title>140488500667392-&gt;140488500668848</title>\n<path d=\"M215.5,-830.3799C215.5,-822.1745 215.5,-812.7679 215.5,-803.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-803.784 215.5,-793.784 212.0001,-803.784 219.0001,-803.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488500694600 -->\n<g class=\"node\" id=\"node14\">\n<title>140488500694600</title>\n<polygon fill=\"none\" points=\"82,-664.5 82,-710.5 349,-710.5 349,-664.5 82,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-683.8\">dense_370: Dense</text>\n<polyline fill=\"none\" points=\"204,-664.5 204,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"204,-687.5 262,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"262,-664.5 262,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-695.3\">(None, 265)</text>\n<polyline fill=\"none\" points=\"262,-687.5 349,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-672.3\">(None, 178)</text>\n</g>\n<!-- 140488500668848&#45;&gt;140488500694600 -->\n<g class=\"edge\" id=\"edge13\">\n<title>140488500668848-&gt;140488500694600</title>\n<path d=\"M215.5,-747.3799C215.5,-739.1745 215.5,-729.7679 215.5,-720.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-720.784 215.5,-710.784 212.0001,-720.784 219.0001,-720.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488500827024 -->\n<g class=\"node\" id=\"node15\">\n<title>140488500827024</title>\n<polygon fill=\"none\" points=\"82,-581.5 82,-627.5 349,-627.5 349,-581.5 82,-581.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-600.8\">dense_371: Dense</text>\n<polyline fill=\"none\" points=\"204,-581.5 204,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-612.3\">input:</text>\n<polyline fill=\"none\" points=\"204,-604.5 262,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-589.3\">output:</text>\n<polyline fill=\"none\" points=\"262,-581.5 262,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-612.3\">(None, 178)</text>\n<polyline fill=\"none\" points=\"262,-604.5 349,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-589.3\">(None, 178)</text>\n</g>\n<!-- 140488500694600&#45;&gt;140488500827024 -->\n<g class=\"edge\" id=\"edge14\">\n<title>140488500694600-&gt;140488500827024</title>\n<path d=\"M215.5,-664.3799C215.5,-656.1745 215.5,-646.7679 215.5,-637.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-637.784 215.5,-627.784 212.0001,-637.784 219.0001,-637.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488500483128 -->\n<g class=\"node\" id=\"node16\">\n<title>140488500483128</title>\n<polygon fill=\"none\" points=\"0,-498.5 0,-544.5 431,-544.5 431,-498.5 0,-498.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-517.8\">batch_normalization_169: BatchNormalization</text>\n<polyline fill=\"none\" points=\"286,-498.5 286,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315\" y=\"-529.3\">input:</text>\n<polyline fill=\"none\" points=\"286,-521.5 344,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315\" y=\"-506.3\">output:</text>\n<polyline fill=\"none\" points=\"344,-498.5 344,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387.5\" y=\"-529.3\">(None, 178)</text>\n<polyline fill=\"none\" points=\"344,-521.5 431,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387.5\" y=\"-506.3\">(None, 178)</text>\n</g>\n<!-- 140488500827024&#45;&gt;140488500483128 -->\n<g class=\"edge\" id=\"edge15\">\n<title>140488500827024-&gt;140488500483128</title>\n<path d=\"M215.5,-581.3799C215.5,-573.1745 215.5,-563.7679 215.5,-554.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-554.784 215.5,-544.784 212.0001,-554.784 219.0001,-554.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488500484416 -->\n<g class=\"node\" id=\"node17\">\n<title>140488500484416</title>\n<polygon fill=\"none\" points=\"58.5,-415.5 58.5,-461.5 372.5,-461.5 372.5,-415.5 58.5,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-434.8\">activation_169: Activation</text>\n<polyline fill=\"none\" points=\"227.5,-415.5 227.5,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256.5\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"227.5,-438.5 285.5,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256.5\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"285.5,-415.5 285.5,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-446.3\">(None, 178)</text>\n<polyline fill=\"none\" points=\"285.5,-438.5 372.5,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-423.3\">(None, 178)</text>\n</g>\n<!-- 140488500483128&#45;&gt;140488500484416 -->\n<g class=\"edge\" id=\"edge16\">\n<title>140488500483128-&gt;140488500484416</title>\n<path d=\"M215.5,-498.3799C215.5,-490.1745 215.5,-480.7679 215.5,-471.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-471.784 215.5,-461.784 212.0001,-471.784 219.0001,-471.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488500514488 -->\n<g class=\"node\" id=\"node18\">\n<title>140488500514488</title>\n<polygon fill=\"none\" points=\"82,-332.5 82,-378.5 349,-378.5 349,-332.5 82,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-351.8\">dense_372: Dense</text>\n<polyline fill=\"none\" points=\"204,-332.5 204,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"204,-355.5 262,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"262,-332.5 262,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-363.3\">(None, 178)</text>\n<polyline fill=\"none\" points=\"262,-355.5 349,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-340.3\">(None, 91)</text>\n</g>\n<!-- 140488500484416&#45;&gt;140488500514488 -->\n<g class=\"edge\" id=\"edge17\">\n<title>140488500484416-&gt;140488500514488</title>\n<path d=\"M215.5,-415.3799C215.5,-407.1745 215.5,-397.7679 215.5,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-388.784 215.5,-378.784 212.0001,-388.784 219.0001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488500122400 -->\n<g class=\"node\" id=\"node19\">\n<title>140488500122400</title>\n<polygon fill=\"none\" points=\"85.5,-249.5 85.5,-295.5 345.5,-295.5 345.5,-249.5 85.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146.5\" y=\"-268.8\">dense_373: Dense</text>\n<polyline fill=\"none\" points=\"207.5,-249.5 207.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"236.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"207.5,-272.5 265.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"236.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"265.5,-249.5 265.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-280.3\">(None, 91)</text>\n<polyline fill=\"none\" points=\"265.5,-272.5 345.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-257.3\">(None, 91)</text>\n</g>\n<!-- 140488500514488&#45;&gt;140488500122400 -->\n<g class=\"edge\" id=\"edge18\">\n<title>140488500514488-&gt;140488500122400</title>\n<path d=\"M215.5,-332.3799C215.5,-324.1745 215.5,-314.7679 215.5,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-305.784 215.5,-295.784 212.0001,-305.784 219.0001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488500294544 -->\n<g class=\"node\" id=\"node20\">\n<title>140488500294544</title>\n<polygon fill=\"none\" points=\"3.5,-166.5 3.5,-212.5 427.5,-212.5 427.5,-166.5 3.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146.5\" y=\"-185.8\">batch_normalization_170: BatchNormalization</text>\n<polyline fill=\"none\" points=\"289.5,-166.5 289.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"318.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"289.5,-189.5 347.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"318.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"347.5,-166.5 347.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387.5\" y=\"-197.3\">(None, 91)</text>\n<polyline fill=\"none\" points=\"347.5,-189.5 427.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387.5\" y=\"-174.3\">(None, 91)</text>\n</g>\n<!-- 140488500122400&#45;&gt;140488500294544 -->\n<g class=\"edge\" id=\"edge19\">\n<title>140488500122400-&gt;140488500294544</title>\n<path d=\"M215.5,-249.3799C215.5,-241.1745 215.5,-231.7679 215.5,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-222.784 215.5,-212.784 212.0001,-222.784 219.0001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488500295832 -->\n<g class=\"node\" id=\"node21\">\n<title>140488500295832</title>\n<polygon fill=\"none\" points=\"62,-83.5 62,-129.5 369,-129.5 369,-83.5 62,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146.5\" y=\"-102.8\">activation_170: Activation</text>\n<polyline fill=\"none\" points=\"231,-83.5 231,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"260\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"231,-106.5 289,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"260\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"289,-83.5 289,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-114.3\">(None, 91)</text>\n<polyline fill=\"none\" points=\"289,-106.5 369,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-91.3\">(None, 91)</text>\n</g>\n<!-- 140488500294544&#45;&gt;140488500295832 -->\n<g class=\"edge\" id=\"edge20\">\n<title>140488500294544-&gt;140488500295832</title>\n<path d=\"M215.5,-166.3799C215.5,-158.1745 215.5,-148.7679 215.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-139.784 215.5,-129.784 212.0001,-139.784 219.0001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140488499797856 -->\n<g class=\"node\" id=\"node22\">\n<title>140488499797856</title>\n<polygon fill=\"none\" points=\"85.5,-.5 85.5,-46.5 345.5,-46.5 345.5,-.5 85.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146.5\" y=\"-19.8\">dense_374: Dense</text>\n<polyline fill=\"none\" points=\"207.5,-.5 207.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"236.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"207.5,-23.5 265.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"236.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"265.5,-.5 265.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-31.3\">(None, 91)</text>\n<polyline fill=\"none\" points=\"265.5,-23.5 345.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-8.3\">(None, 4)</text>\n</g>\n<!-- 140488500295832&#45;&gt;140488499797856 -->\n<g class=\"edge\" id=\"edge21\">\n<title>140488500295832-&gt;140488499797856</title>\n<path d=\"M215.5,-83.3799C215.5,-75.1745 215.5,-65.7679 215.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.0001,-56.784 215.5,-46.784 212.0001,-56.784 219.0001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN6jDLIsoc2C",
        "colab_type": "code",
        "outputId": "d5a5f0b0-893b-449c-d602-8948b9c4639f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# 학습 과정\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('mean absolute error')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEJCAYAAAAuMNi1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nW/4f/YDHACkFUI8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-QWG0nuoc63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 예측값을 생성합니다.\n",
        "\n",
        "pred_test = model.predict(test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaTs0ftQoc_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# submission 파일을 생성합니다.\n",
        "sample_sub = pd.read_csv('/gdrive/My Drive/DACON-semiconductor-competition/dataset/sample_submission.csv', index_col=0)\n",
        "submission = sample_sub+pred_test\n",
        "submission.to_csv('/gdrive/My Drive/DACON-semiconductor-competition/submission_10.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56crgNgxGYhl",
        "colab_type": "text"
      },
      "source": [
        "### Bayesian Optimization\n",
        "http://research.sualab.com/introduction/practice/2019/02/19/bayesian-optimization-overview-1.html<br>\n",
        "http://research.sualab.com/introduction/practice/2019/04/01/bayesian-optimization-overview-2.html<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoBQ043NGZmD",
        "colab_type": "text"
      },
      "source": [
        "### Swish Activation\n",
        "https://www.machinecurve.com/index.php/2019/05/30/why-swish-could-perform-better-than-relu/#todays-activation-functions"
      ]
    }
  ]
}