{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "submission_12.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPFFHom18qMfHeWW76UOnze",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Inha-AI/DACON-semiconductor-competition/blob/feature%2FYoonSungLee/submission_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBPfb1Jhq4TA",
        "colab_type": "code",
        "outputId": "9ba467fd-d87e-45ef-c854-febf78bfc47f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "pip install bayesian-optimization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/72/0c/173ac467d0a53e33e41b521e4ceba74a8ac7c7873d7b857a8fbdca88302d/bayesian-optimization-1.0.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.17.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.22.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.14.1)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.0.1-cp36-none-any.whl size=10032 sha256=1e3f4940050b0d2a8a9cb167a91fba4ef0238f6fd2d99f4fc2b59fd8697f3669\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/0d/3b/6b9d4477a34b3905f246ff4e7acf6aafd4cc9b77d473629b77\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vVeUdyssBKw",
        "colab_type": "code",
        "outputId": "f3485286-4ba7-4288-fcca-bf3128c990d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bayes_opt import BayesianOptimization"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwHXDEigsNB0",
        "colab_type": "code",
        "outputId": "b2dbc946-3d02-422b-e26b-d3a68b6cccd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQF6BGGF1DqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 자료형을 적절히 변형시켜 데이터의 크기를 줄이는 방법\n",
        "\n",
        "# for col in df_train.columns:\n",
        "#     col_type = df_train[col].dtypes\n",
        "#     min1 = df_train[col].min()\n",
        "#     max1 = df_train[col].max()\n",
        "#     if str(col_type)[:3] == 'int':\n",
        "#         df_train[col] = df_train[col].astype(np.int16)\n",
        "#     else:\n",
        "#         if min1 > np.finfo(np.float16).min and max1 < np.finfo(np.float16).max:\n",
        "#             df_train[col] = trdf_trainain[col].astype(np.float16)\n",
        "#         elif min1 > np.finfo(np.float32).min and max1 < np.finfo(np.float32).max:\n",
        "#             df_train[col] = df_train[col].astype(np.float32)\n",
        "#         else:\n",
        "#             df_train[col] = df_train[col].astype(np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BNXAU5rsPJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv('/gdrive/My Drive/DACON-semiconductor-competition/dataset/train.csv')\n",
        "df_test = pd.read_csv('/gdrive/My Drive/DACON-semiconductor-competition/dataset/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdYXclfgsVr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 독립변수와 종속변수를 분리합니다.\n",
        "\n",
        "train_X = df_train.iloc[:,4:]\n",
        "train_Y = df_train.iloc[:,0:4]\n",
        "test_X = df_test.iloc[:,1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogdcz8YctuVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train set을 shuffle하여 다시 train set과 validation set으로 분리합니다.\n",
        "\n",
        "train_X, val_X, train_Y, val_Y = train_test_split(train_X, train_Y, test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tT4w6XasaSQ",
        "colab_type": "text"
      },
      "source": [
        "# Model 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PSSfoS5sfWK",
        "colab_type": "text"
      },
      "source": [
        "* 11 layers\n",
        "* 800 units, he_normal, swish\n",
        "* BatchNormalization\n",
        "* Adam(0.001)\n",
        "* epochs 200\n",
        "* batch_size 630\n",
        "<br><br>\n",
        "* BayesianOptimization을 통해 최적의 units를 도출해 내어 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGEkxH551LkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 케라스를 통해 모델 생성을 시작합니다.\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=800, input_dim=226, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=4, activation='linear'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Activation Function 정의\n",
        "\n",
        "def swish(x) :\n",
        "    return x * keras.activations.sigmoid(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35DQBYFkofR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "ec12c2e5-5028-402e-bdaf-709ff0d3ce4f"
      },
      "source": [
        "model = create_model()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpTWpi6eXFec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "e6b07e79-a3d3-40b2-8461-ede5ef3269df"
      },
      "source": [
        "adam = keras.optimizers.Adam(0.001)\n",
        "model.compile(loss='mae', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byKqdJKjXFjb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a0e376e-de3a-42d7-a7f6-414cea12929f"
      },
      "source": [
        "hist = model.fit(train_X, train_Y, epochs=200, batch_size=630,\n",
        "                    validation_data=(val_X, val_Y))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "607500/607500 [==============================] - 30s 49us/step - loss: 68.5536 - acc: 0.3582 - val_loss: 55.7735 - val_acc: 0.4257\n",
            "Epoch 2/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 31.8944 - acc: 0.6670 - val_loss: 29.1752 - val_acc: 0.7008\n",
            "Epoch 3/200\n",
            "607500/607500 [==============================] - 20s 33us/step - loss: 20.2049 - acc: 0.7926 - val_loss: 19.1672 - val_acc: 0.8053\n",
            "Epoch 4/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 15.0082 - acc: 0.8438 - val_loss: 14.1090 - val_acc: 0.8472\n",
            "Epoch 5/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 12.1089 - acc: 0.8724 - val_loss: 11.7625 - val_acc: 0.8709\n",
            "Epoch 6/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 10.3613 - acc: 0.8904 - val_loss: 11.4449 - val_acc: 0.8844\n",
            "Epoch 7/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 9.1373 - acc: 0.9028 - val_loss: 9.8528 - val_acc: 0.8883\n",
            "Epoch 8/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 8.2605 - acc: 0.9108 - val_loss: 8.7566 - val_acc: 0.9073\n",
            "Epoch 9/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 7.5637 - acc: 0.9181 - val_loss: 7.2044 - val_acc: 0.9239\n",
            "Epoch 10/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 7.0281 - acc: 0.9237 - val_loss: 6.6323 - val_acc: 0.9341\n",
            "Epoch 11/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 6.6355 - acc: 0.9274 - val_loss: 7.1273 - val_acc: 0.9244\n",
            "Epoch 12/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 6.2502 - acc: 0.9308 - val_loss: 6.2537 - val_acc: 0.9310\n",
            "Epoch 13/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 5.9304 - acc: 0.9339 - val_loss: 6.0971 - val_acc: 0.9405\n",
            "Epoch 14/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 5.6898 - acc: 0.9362 - val_loss: 5.6210 - val_acc: 0.9306\n",
            "Epoch 15/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 5.4088 - acc: 0.9385 - val_loss: 5.0372 - val_acc: 0.9483\n",
            "Epoch 16/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 5.2249 - acc: 0.9409 - val_loss: 5.7370 - val_acc: 0.9288\n",
            "Epoch 17/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 5.0295 - acc: 0.9420 - val_loss: 5.1636 - val_acc: 0.9378\n",
            "Epoch 18/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 4.8415 - acc: 0.9438 - val_loss: 5.3995 - val_acc: 0.9350\n",
            "Epoch 19/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 4.6926 - acc: 0.9453 - val_loss: 4.6477 - val_acc: 0.9503\n",
            "Epoch 20/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 4.5486 - acc: 0.9469 - val_loss: 4.2033 - val_acc: 0.9499\n",
            "Epoch 21/200\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 4.4037 - acc: 0.9475 - val_loss: 4.7230 - val_acc: 0.9451\n",
            "Epoch 22/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 4.2756 - acc: 0.9490 - val_loss: 4.6010 - val_acc: 0.9464\n",
            "Epoch 23/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 4.1893 - acc: 0.9492 - val_loss: 3.9972 - val_acc: 0.9503\n",
            "Epoch 24/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 4.0885 - acc: 0.9506 - val_loss: 3.8721 - val_acc: 0.9505\n",
            "Epoch 25/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.9701 - acc: 0.9518 - val_loss: 3.5886 - val_acc: 0.9505\n",
            "Epoch 26/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 3.8657 - acc: 0.9521 - val_loss: 3.7259 - val_acc: 0.9535\n",
            "Epoch 27/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.7838 - acc: 0.9527 - val_loss: 3.2162 - val_acc: 0.9540\n",
            "Epoch 28/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.6720 - acc: 0.9539 - val_loss: 3.5860 - val_acc: 0.9526\n",
            "Epoch 29/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.6036 - acc: 0.9546 - val_loss: 3.6657 - val_acc: 0.9554\n",
            "Epoch 30/200\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 3.5430 - acc: 0.9549 - val_loss: 3.3451 - val_acc: 0.9589\n",
            "Epoch 31/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 3.4565 - acc: 0.9554 - val_loss: 3.0028 - val_acc: 0.9634\n",
            "Epoch 32/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 3.3809 - acc: 0.9558 - val_loss: 3.4282 - val_acc: 0.9556\n",
            "Epoch 33/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 3.3493 - acc: 0.9561 - val_loss: 2.9808 - val_acc: 0.9557\n",
            "Epoch 34/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 3.2386 - acc: 0.9568 - val_loss: 3.0465 - val_acc: 0.9591\n",
            "Epoch 35/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 3.1874 - acc: 0.9576 - val_loss: 2.8566 - val_acc: 0.9590\n",
            "Epoch 36/200\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 3.1327 - acc: 0.9578 - val_loss: 2.9553 - val_acc: 0.9658\n",
            "Epoch 37/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 3.1013 - acc: 0.9580 - val_loss: 2.7115 - val_acc: 0.9587\n",
            "Epoch 38/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 3.0553 - acc: 0.9581 - val_loss: 3.0638 - val_acc: 0.9541\n",
            "Epoch 39/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.9795 - acc: 0.9586 - val_loss: 3.1337 - val_acc: 0.9532\n",
            "Epoch 40/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.9406 - acc: 0.9593 - val_loss: 2.9700 - val_acc: 0.9528\n",
            "Epoch 41/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.8815 - acc: 0.9594 - val_loss: 3.2365 - val_acc: 0.9540\n",
            "Epoch 42/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.8559 - acc: 0.9598 - val_loss: 2.8142 - val_acc: 0.9616\n",
            "Epoch 43/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.7883 - acc: 0.9598 - val_loss: 3.0646 - val_acc: 0.9622\n",
            "Epoch 44/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.7665 - acc: 0.9605 - val_loss: 2.5389 - val_acc: 0.9647\n",
            "Epoch 45/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.7251 - acc: 0.9609 - val_loss: 2.6274 - val_acc: 0.9579\n",
            "Epoch 46/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.7138 - acc: 0.9605 - val_loss: 2.7647 - val_acc: 0.9626\n",
            "Epoch 47/200\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 2.6630 - acc: 0.9609 - val_loss: 2.5353 - val_acc: 0.9644\n",
            "Epoch 48/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.6263 - acc: 0.9612 - val_loss: 2.6135 - val_acc: 0.9638\n",
            "Epoch 49/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.5947 - acc: 0.9613 - val_loss: 2.3401 - val_acc: 0.9626\n",
            "Epoch 50/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.5550 - acc: 0.9614 - val_loss: 2.5028 - val_acc: 0.9657\n",
            "Epoch 51/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.5210 - acc: 0.9616 - val_loss: 2.3797 - val_acc: 0.9573\n",
            "Epoch 52/200\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 2.4704 - acc: 0.9616 - val_loss: 2.1279 - val_acc: 0.9596\n",
            "Epoch 53/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.4589 - acc: 0.9620 - val_loss: 2.2678 - val_acc: 0.9636\n",
            "Epoch 54/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.4467 - acc: 0.9618 - val_loss: 2.4066 - val_acc: 0.9553\n",
            "Epoch 55/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.4375 - acc: 0.9619 - val_loss: 2.2287 - val_acc: 0.9652\n",
            "Epoch 56/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.3804 - acc: 0.9624 - val_loss: 2.2679 - val_acc: 0.9577\n",
            "Epoch 57/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.3531 - acc: 0.9624 - val_loss: 2.1555 - val_acc: 0.9622\n",
            "Epoch 58/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.3509 - acc: 0.9621 - val_loss: 2.1870 - val_acc: 0.9773\n",
            "Epoch 59/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.3118 - acc: 0.9624 - val_loss: 2.1706 - val_acc: 0.9686\n",
            "Epoch 60/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.2865 - acc: 0.9625 - val_loss: 2.2417 - val_acc: 0.9605\n",
            "Epoch 61/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.2547 - acc: 0.9630 - val_loss: 2.4879 - val_acc: 0.9591\n",
            "Epoch 62/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.2534 - acc: 0.9627 - val_loss: 2.0367 - val_acc: 0.9706\n",
            "Epoch 63/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.2176 - acc: 0.9631 - val_loss: 2.1265 - val_acc: 0.9637\n",
            "Epoch 64/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.2212 - acc: 0.9629 - val_loss: 2.1680 - val_acc: 0.9595\n",
            "Epoch 65/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.1704 - acc: 0.9635 - val_loss: 2.2562 - val_acc: 0.9591\n",
            "Epoch 66/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.1606 - acc: 0.9629 - val_loss: 2.1437 - val_acc: 0.9615\n",
            "Epoch 67/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.1497 - acc: 0.9632 - val_loss: 2.0609 - val_acc: 0.9688\n",
            "Epoch 68/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.1194 - acc: 0.9635 - val_loss: 2.0566 - val_acc: 0.9640\n",
            "Epoch 69/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.1256 - acc: 0.9631 - val_loss: 1.8746 - val_acc: 0.9563\n",
            "Epoch 70/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.0960 - acc: 0.9637 - val_loss: 2.0884 - val_acc: 0.9627\n",
            "Epoch 71/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.0888 - acc: 0.9634 - val_loss: 1.8543 - val_acc: 0.9674\n",
            "Epoch 72/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.0440 - acc: 0.9638 - val_loss: 1.9760 - val_acc: 0.9681\n",
            "Epoch 73/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 2.0487 - acc: 0.9636 - val_loss: 1.9555 - val_acc: 0.9736\n",
            "Epoch 74/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 2.0436 - acc: 0.9634 - val_loss: 2.0341 - val_acc: 0.9655\n",
            "Epoch 75/200\n",
            "607500/607500 [==============================] - 20s 32us/step - loss: 2.0357 - acc: 0.9637 - val_loss: 2.1355 - val_acc: 0.9673\n",
            "Epoch 76/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.9998 - acc: 0.9637 - val_loss: 2.0403 - val_acc: 0.9600\n",
            "Epoch 77/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.9920 - acc: 0.9641 - val_loss: 1.9934 - val_acc: 0.9660\n",
            "Epoch 78/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.9871 - acc: 0.9638 - val_loss: 2.1115 - val_acc: 0.9570\n",
            "Epoch 79/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.9560 - acc: 0.9640 - val_loss: 1.7324 - val_acc: 0.9629\n",
            "Epoch 80/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.9494 - acc: 0.9642 - val_loss: 1.7937 - val_acc: 0.9640\n",
            "Epoch 81/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.9499 - acc: 0.9639 - val_loss: 1.6731 - val_acc: 0.9679\n",
            "Epoch 82/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.9352 - acc: 0.9639 - val_loss: 1.7422 - val_acc: 0.9613\n",
            "Epoch 83/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.9254 - acc: 0.9640 - val_loss: 1.9620 - val_acc: 0.9637\n",
            "Epoch 84/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.9120 - acc: 0.9643 - val_loss: 1.8008 - val_acc: 0.9628\n",
            "Epoch 85/200\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 1.8805 - acc: 0.9643 - val_loss: 1.8675 - val_acc: 0.9614\n",
            "Epoch 86/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.8834 - acc: 0.9644 - val_loss: 1.6976 - val_acc: 0.9721\n",
            "Epoch 87/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.8672 - acc: 0.9646 - val_loss: 1.6256 - val_acc: 0.9531\n",
            "Epoch 88/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.8649 - acc: 0.9646 - val_loss: 1.6514 - val_acc: 0.9496\n",
            "Epoch 89/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.8447 - acc: 0.9644 - val_loss: 1.6333 - val_acc: 0.9670\n",
            "Epoch 90/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.8364 - acc: 0.9643 - val_loss: 1.8980 - val_acc: 0.9534\n",
            "Epoch 91/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.8213 - acc: 0.9643 - val_loss: 1.6768 - val_acc: 0.9693\n",
            "Epoch 92/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.8339 - acc: 0.9644 - val_loss: 1.8694 - val_acc: 0.9608\n",
            "Epoch 93/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.8087 - acc: 0.9644 - val_loss: 1.8289 - val_acc: 0.9656\n",
            "Epoch 94/200\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 1.8131 - acc: 0.9645 - val_loss: 1.8577 - val_acc: 0.9538\n",
            "Epoch 95/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.7826 - acc: 0.9644 - val_loss: 1.6939 - val_acc: 0.9594\n",
            "Epoch 96/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.7753 - acc: 0.9648 - val_loss: 1.7222 - val_acc: 0.9646\n",
            "Epoch 97/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.7849 - acc: 0.9643 - val_loss: 1.6312 - val_acc: 0.9698\n",
            "Epoch 98/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.7674 - acc: 0.9646 - val_loss: 1.7926 - val_acc: 0.9686\n",
            "Epoch 99/200\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 1.7614 - acc: 0.9645 - val_loss: 1.7238 - val_acc: 0.9633\n",
            "Epoch 100/200\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 1.7480 - acc: 0.9646 - val_loss: 1.7401 - val_acc: 0.9581\n",
            "Epoch 101/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.7468 - acc: 0.9647 - val_loss: 1.6117 - val_acc: 0.9676\n",
            "Epoch 102/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.7217 - acc: 0.9648 - val_loss: 1.5215 - val_acc: 0.9648\n",
            "Epoch 103/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.7362 - acc: 0.9647 - val_loss: 1.6323 - val_acc: 0.9640\n",
            "Epoch 104/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.7247 - acc: 0.9649 - val_loss: 1.6606 - val_acc: 0.9567\n",
            "Epoch 105/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.7083 - acc: 0.9647 - val_loss: 1.7224 - val_acc: 0.9608\n",
            "Epoch 106/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.7104 - acc: 0.9647 - val_loss: 1.6993 - val_acc: 0.9574\n",
            "Epoch 107/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.6930 - acc: 0.9649 - val_loss: 1.6121 - val_acc: 0.9694\n",
            "Epoch 108/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.6935 - acc: 0.9653 - val_loss: 1.5185 - val_acc: 0.9572\n",
            "Epoch 109/200\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 1.6826 - acc: 0.9651 - val_loss: 1.6548 - val_acc: 0.9672\n",
            "Epoch 110/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.6751 - acc: 0.9647 - val_loss: 1.7315 - val_acc: 0.9594\n",
            "Epoch 111/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.6599 - acc: 0.9651 - val_loss: 1.6511 - val_acc: 0.9584\n",
            "Epoch 112/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.6743 - acc: 0.9647 - val_loss: 1.3792 - val_acc: 0.9654\n",
            "Epoch 113/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.6555 - acc: 0.9651 - val_loss: 1.4570 - val_acc: 0.9638\n",
            "Epoch 114/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.6550 - acc: 0.9648 - val_loss: 1.6876 - val_acc: 0.9637\n",
            "Epoch 115/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.6359 - acc: 0.9646 - val_loss: 1.5029 - val_acc: 0.9684\n",
            "Epoch 116/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.6412 - acc: 0.9652 - val_loss: 1.4162 - val_acc: 0.9664\n",
            "Epoch 117/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.6430 - acc: 0.9651 - val_loss: 1.5657 - val_acc: 0.9715\n",
            "Epoch 118/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.6259 - acc: 0.9649 - val_loss: 1.6498 - val_acc: 0.9643\n",
            "Epoch 119/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.6289 - acc: 0.9651 - val_loss: 1.5753 - val_acc: 0.9623\n",
            "Epoch 120/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.6045 - acc: 0.9651 - val_loss: 1.4762 - val_acc: 0.9610\n",
            "Epoch 121/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.6083 - acc: 0.9656 - val_loss: 1.4549 - val_acc: 0.9589\n",
            "Epoch 122/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.6174 - acc: 0.9652 - val_loss: 1.3709 - val_acc: 0.9680\n",
            "Epoch 123/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.5989 - acc: 0.9650 - val_loss: 1.6402 - val_acc: 0.9655\n",
            "Epoch 124/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.5899 - acc: 0.9648 - val_loss: 1.3243 - val_acc: 0.9639\n",
            "Epoch 125/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.5766 - acc: 0.9651 - val_loss: 1.4766 - val_acc: 0.9605\n",
            "Epoch 126/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.5861 - acc: 0.9648 - val_loss: 1.5250 - val_acc: 0.9604\n",
            "Epoch 127/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.5741 - acc: 0.9650 - val_loss: 1.4502 - val_acc: 0.9760\n",
            "Epoch 128/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.5813 - acc: 0.9654 - val_loss: 1.4719 - val_acc: 0.9696\n",
            "Epoch 129/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.5576 - acc: 0.9652 - val_loss: 1.3889 - val_acc: 0.9637\n",
            "Epoch 130/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.5590 - acc: 0.9649 - val_loss: 1.7320 - val_acc: 0.9628\n",
            "Epoch 131/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.5600 - acc: 0.9649 - val_loss: 1.5669 - val_acc: 0.9714\n",
            "Epoch 132/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.5532 - acc: 0.9653 - val_loss: 1.6412 - val_acc: 0.9606\n",
            "Epoch 133/200\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 1.5414 - acc: 0.9655 - val_loss: 1.6378 - val_acc: 0.9570\n",
            "Epoch 134/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.5347 - acc: 0.9650 - val_loss: 1.3861 - val_acc: 0.9655\n",
            "Epoch 135/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.5419 - acc: 0.9648 - val_loss: 1.4497 - val_acc: 0.9658\n",
            "Epoch 136/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.5337 - acc: 0.9648 - val_loss: 1.3732 - val_acc: 0.9530\n",
            "Epoch 137/200\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 1.5150 - acc: 0.9653 - val_loss: 1.3383 - val_acc: 0.9596\n",
            "Epoch 138/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.5211 - acc: 0.9652 - val_loss: 1.4484 - val_acc: 0.9653\n",
            "Epoch 139/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.5165 - acc: 0.9651 - val_loss: 1.4594 - val_acc: 0.9670\n",
            "Epoch 140/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.5005 - acc: 0.9654 - val_loss: 1.3939 - val_acc: 0.9577\n",
            "Epoch 141/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.5017 - acc: 0.9652 - val_loss: 1.3850 - val_acc: 0.9647\n",
            "Epoch 142/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4975 - acc: 0.9656 - val_loss: 1.3583 - val_acc: 0.9718\n",
            "Epoch 143/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4913 - acc: 0.9654 - val_loss: 1.4861 - val_acc: 0.9776\n",
            "Epoch 144/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4975 - acc: 0.9651 - val_loss: 1.3687 - val_acc: 0.9683\n",
            "Epoch 145/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4805 - acc: 0.9654 - val_loss: 1.5657 - val_acc: 0.9792\n",
            "Epoch 146/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4875 - acc: 0.9653 - val_loss: 1.3987 - val_acc: 0.9563\n",
            "Epoch 147/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4777 - acc: 0.9655 - val_loss: 1.3677 - val_acc: 0.9686\n",
            "Epoch 148/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4771 - acc: 0.9652 - val_loss: 1.2805 - val_acc: 0.9623\n",
            "Epoch 149/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4780 - acc: 0.9654 - val_loss: 1.1520 - val_acc: 0.9678\n",
            "Epoch 150/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.4628 - acc: 0.9655 - val_loss: 1.4436 - val_acc: 0.9662\n",
            "Epoch 151/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4720 - acc: 0.9653 - val_loss: 1.3301 - val_acc: 0.9648\n",
            "Epoch 152/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.4633 - acc: 0.9653 - val_loss: 1.3019 - val_acc: 0.9631\n",
            "Epoch 153/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4467 - acc: 0.9652 - val_loss: 1.3214 - val_acc: 0.9680\n",
            "Epoch 154/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4661 - acc: 0.9656 - val_loss: 1.3445 - val_acc: 0.9741\n",
            "Epoch 155/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4387 - acc: 0.9656 - val_loss: 1.3807 - val_acc: 0.9628\n",
            "Epoch 156/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4491 - acc: 0.9655 - val_loss: 1.3502 - val_acc: 0.9632\n",
            "Epoch 157/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4447 - acc: 0.9656 - val_loss: 1.2903 - val_acc: 0.9605\n",
            "Epoch 158/200\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 1.4367 - acc: 0.9654 - val_loss: 1.3245 - val_acc: 0.9721\n",
            "Epoch 159/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4269 - acc: 0.9656 - val_loss: 1.3399 - val_acc: 0.9649\n",
            "Epoch 160/200\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 1.4389 - acc: 0.9655 - val_loss: 1.2176 - val_acc: 0.9655\n",
            "Epoch 161/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.4239 - acc: 0.9656 - val_loss: 1.2364 - val_acc: 0.9661\n",
            "Epoch 162/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4208 - acc: 0.9655 - val_loss: 1.2363 - val_acc: 0.9632\n",
            "Epoch 163/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.4166 - acc: 0.9657 - val_loss: 1.3183 - val_acc: 0.9638\n",
            "Epoch 164/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4284 - acc: 0.9657 - val_loss: 1.3078 - val_acc: 0.9680\n",
            "Epoch 165/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4090 - acc: 0.9655 - val_loss: 1.2607 - val_acc: 0.9679\n",
            "Epoch 166/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.4156 - acc: 0.9653 - val_loss: 1.2919 - val_acc: 0.9696\n",
            "Epoch 167/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3958 - acc: 0.9656 - val_loss: 1.3026 - val_acc: 0.9650\n",
            "Epoch 168/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.4017 - acc: 0.9658 - val_loss: 1.1552 - val_acc: 0.9646\n",
            "Epoch 169/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3944 - acc: 0.9653 - val_loss: 1.3180 - val_acc: 0.9673\n",
            "Epoch 170/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3922 - acc: 0.9654 - val_loss: 1.3194 - val_acc: 0.9689\n",
            "Epoch 171/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3957 - acc: 0.9656 - val_loss: 1.3013 - val_acc: 0.9620\n",
            "Epoch 172/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3815 - acc: 0.9655 - val_loss: 1.3950 - val_acc: 0.9603\n",
            "Epoch 173/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3941 - acc: 0.9661 - val_loss: 1.2892 - val_acc: 0.9601\n",
            "Epoch 174/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3818 - acc: 0.9656 - val_loss: 1.2478 - val_acc: 0.9679\n",
            "Epoch 175/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3866 - acc: 0.9655 - val_loss: 1.2293 - val_acc: 0.9633\n",
            "Epoch 176/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3811 - acc: 0.9655 - val_loss: 1.2320 - val_acc: 0.9715\n",
            "Epoch 177/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3791 - acc: 0.9655 - val_loss: 1.3885 - val_acc: 0.9605\n",
            "Epoch 178/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3856 - acc: 0.9654 - val_loss: 1.2141 - val_acc: 0.9669\n",
            "Epoch 179/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3696 - acc: 0.9657 - val_loss: 1.2834 - val_acc: 0.9638\n",
            "Epoch 180/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3583 - acc: 0.9658 - val_loss: 1.2471 - val_acc: 0.9590\n",
            "Epoch 181/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3598 - acc: 0.9654 - val_loss: 1.3920 - val_acc: 0.9678\n",
            "Epoch 182/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3688 - acc: 0.9653 - val_loss: 1.1902 - val_acc: 0.9616\n",
            "Epoch 183/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3565 - acc: 0.9656 - val_loss: 1.1938 - val_acc: 0.9629\n",
            "Epoch 184/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3594 - acc: 0.9658 - val_loss: 1.3281 - val_acc: 0.9551\n",
            "Epoch 185/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3539 - acc: 0.9657 - val_loss: 1.3241 - val_acc: 0.9727\n",
            "Epoch 186/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3410 - acc: 0.9657 - val_loss: 1.2973 - val_acc: 0.9713\n",
            "Epoch 187/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3498 - acc: 0.9656 - val_loss: 1.2391 - val_acc: 0.9614\n",
            "Epoch 188/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3389 - acc: 0.9655 - val_loss: 1.2574 - val_acc: 0.9630\n",
            "Epoch 189/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3549 - acc: 0.9657 - val_loss: 1.3096 - val_acc: 0.9653\n",
            "Epoch 190/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3536 - acc: 0.9656 - val_loss: 1.2371 - val_acc: 0.9737\n",
            "Epoch 191/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3348 - acc: 0.9656 - val_loss: 1.2425 - val_acc: 0.9597\n",
            "Epoch 192/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3337 - acc: 0.9657 - val_loss: 1.3879 - val_acc: 0.9575\n",
            "Epoch 193/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3362 - acc: 0.9656 - val_loss: 1.3467 - val_acc: 0.9680\n",
            "Epoch 194/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3236 - acc: 0.9658 - val_loss: 1.5820 - val_acc: 0.9679\n",
            "Epoch 195/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3294 - acc: 0.9657 - val_loss: 1.1269 - val_acc: 0.9654\n",
            "Epoch 196/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3310 - acc: 0.9661 - val_loss: 1.2266 - val_acc: 0.9690\n",
            "Epoch 197/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3275 - acc: 0.9656 - val_loss: 1.2357 - val_acc: 0.9564\n",
            "Epoch 198/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3290 - acc: 0.9657 - val_loss: 1.1214 - val_acc: 0.9747\n",
            "Epoch 199/200\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3148 - acc: 0.9658 - val_loss: 1.1876 - val_acc: 0.9697\n",
            "Epoch 200/200\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.3078 - acc: 0.9656 - val_loss: 1.2859 - val_acc: 0.9672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsfen6zmofgX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d32c7fbf-2c38-4a25-fcb1-397581857ce2"
      },
      "source": [
        "# 모델 아키텍처\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"2397pt\" viewBox=\"0.00 0.00 424.00 1798.00\" width=\"565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1.3333 1.3333) rotate(0) translate(4 1794)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-1794 420,-1794 420,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140246506833904 -->\n<g class=\"node\" id=\"node1\">\n<title>140246506833904</title>\n<polygon fill=\"none\" points=\"49,-1743.5 49,-1789.5 367,-1789.5 367,-1743.5 49,-1743.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1762.8\">dense_1_input: InputLayer</text>\n<polyline fill=\"none\" points=\"222,-1743.5 222,-1789.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251\" y=\"-1774.3\">input:</text>\n<polyline fill=\"none\" points=\"222,-1766.5 280,-1766.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251\" y=\"-1751.3\">output:</text>\n<polyline fill=\"none\" points=\"280,-1743.5 280,-1789.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.5\" y=\"-1774.3\">(None, 226)</text>\n<polyline fill=\"none\" points=\"280,-1766.5 367,-1766.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.5\" y=\"-1751.3\">(None, 226)</text>\n</g>\n<!-- 140246510591840 -->\n<g class=\"node\" id=\"node2\">\n<title>140246510591840</title>\n<polygon fill=\"none\" points=\"82,-1660.5 82,-1706.5 334,-1706.5 334,-1660.5 82,-1660.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1679.8\">dense_1: Dense</text>\n<polyline fill=\"none\" points=\"189,-1660.5 189,-1706.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1691.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-1683.5 247,-1683.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1668.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-1660.5 247,-1706.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1691.3\">(None, 226)</text>\n<polyline fill=\"none\" points=\"247,-1683.5 334,-1683.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1668.3\">(None, 800)</text>\n</g>\n<!-- 140246506833904&#45;&gt;140246510591840 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140246506833904-&gt;140246510591840</title>\n<path d=\"M208,-1743.3799C208,-1735.1745 208,-1725.7679 208,-1716.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1716.784 208,-1706.784 204.5001,-1716.784 211.5001,-1716.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246506832336 -->\n<g class=\"node\" id=\"node3\">\n<title>140246506832336</title>\n<polygon fill=\"none\" points=\"82,-1577.5 82,-1623.5 334,-1623.5 334,-1577.5 82,-1577.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1596.8\">dense_2: Dense</text>\n<polyline fill=\"none\" points=\"189,-1577.5 189,-1623.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1608.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-1600.5 247,-1600.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1585.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-1577.5 247,-1623.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1608.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"247,-1600.5 334,-1600.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1585.3\">(None, 800)</text>\n</g>\n<!-- 140246510591840&#45;&gt;140246506832336 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140246510591840-&gt;140246506832336</title>\n<path d=\"M208,-1660.3799C208,-1652.1745 208,-1642.7679 208,-1633.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1633.784 208,-1623.784 204.5001,-1633.784 211.5001,-1633.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246509085360 -->\n<g class=\"node\" id=\"node4\">\n<title>140246509085360</title>\n<polygon fill=\"none\" points=\"0,-1494.5 0,-1540.5 416,-1540.5 416,-1494.5 0,-1494.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1513.8\">batch_normalization_1: BatchNormalization</text>\n<polyline fill=\"none\" points=\"271,-1494.5 271,-1540.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-1525.3\">input:</text>\n<polyline fill=\"none\" points=\"271,-1517.5 329,-1517.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-1502.3\">output:</text>\n<polyline fill=\"none\" points=\"329,-1494.5 329,-1540.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-1525.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"329,-1517.5 416,-1517.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-1502.3\">(None, 800)</text>\n</g>\n<!-- 140246506832336&#45;&gt;140246509085360 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140246506832336-&gt;140246509085360</title>\n<path d=\"M208,-1577.3799C208,-1569.1745 208,-1559.7679 208,-1550.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1550.784 208,-1540.784 204.5001,-1550.784 211.5001,-1550.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246506832056 -->\n<g class=\"node\" id=\"node5\">\n<title>140246506832056</title>\n<polygon fill=\"none\" points=\"58.5,-1411.5 58.5,-1457.5 357.5,-1457.5 357.5,-1411.5 58.5,-1411.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1430.8\">activation_1: Activation</text>\n<polyline fill=\"none\" points=\"212.5,-1411.5 212.5,-1457.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-1442.3\">input:</text>\n<polyline fill=\"none\" points=\"212.5,-1434.5 270.5,-1434.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-1419.3\">output:</text>\n<polyline fill=\"none\" points=\"270.5,-1411.5 270.5,-1457.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-1442.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"270.5,-1434.5 357.5,-1434.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-1419.3\">(None, 800)</text>\n</g>\n<!-- 140246509085360&#45;&gt;140246506832056 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140246509085360-&gt;140246506832056</title>\n<path d=\"M208,-1494.3799C208,-1486.1745 208,-1476.7679 208,-1467.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1467.784 208,-1457.784 204.5001,-1467.784 211.5001,-1467.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246508032352 -->\n<g class=\"node\" id=\"node6\">\n<title>140246508032352</title>\n<polygon fill=\"none\" points=\"82,-1328.5 82,-1374.5 334,-1374.5 334,-1328.5 82,-1328.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1347.8\">dense_3: Dense</text>\n<polyline fill=\"none\" points=\"189,-1328.5 189,-1374.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1359.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-1351.5 247,-1351.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1336.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-1328.5 247,-1374.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1359.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"247,-1351.5 334,-1351.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1336.3\">(None, 800)</text>\n</g>\n<!-- 140246506832056&#45;&gt;140246508032352 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140246506832056-&gt;140246508032352</title>\n<path d=\"M208,-1411.3799C208,-1403.1745 208,-1393.7679 208,-1384.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1384.784 208,-1374.784 204.5001,-1384.784 211.5001,-1384.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246508085824 -->\n<g class=\"node\" id=\"node7\">\n<title>140246508085824</title>\n<polygon fill=\"none\" points=\"82,-1245.5 82,-1291.5 334,-1291.5 334,-1245.5 82,-1245.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1264.8\">dense_4: Dense</text>\n<polyline fill=\"none\" points=\"189,-1245.5 189,-1291.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1276.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-1268.5 247,-1268.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1253.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-1245.5 247,-1291.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1276.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"247,-1268.5 334,-1268.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1253.3\">(None, 800)</text>\n</g>\n<!-- 140246508032352&#45;&gt;140246508085824 -->\n<g class=\"edge\" id=\"edge6\">\n<title>140246508032352-&gt;140246508085824</title>\n<path d=\"M208,-1328.3799C208,-1320.1745 208,-1310.7679 208,-1301.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1301.784 208,-1291.784 204.5001,-1301.784 211.5001,-1301.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246507906720 -->\n<g class=\"node\" id=\"node8\">\n<title>140246507906720</title>\n<polygon fill=\"none\" points=\"0,-1162.5 0,-1208.5 416,-1208.5 416,-1162.5 0,-1162.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1181.8\">batch_normalization_2: BatchNormalization</text>\n<polyline fill=\"none\" points=\"271,-1162.5 271,-1208.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-1193.3\">input:</text>\n<polyline fill=\"none\" points=\"271,-1185.5 329,-1185.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-1170.3\">output:</text>\n<polyline fill=\"none\" points=\"329,-1162.5 329,-1208.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-1193.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"329,-1185.5 416,-1185.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-1170.3\">(None, 800)</text>\n</g>\n<!-- 140246508085824&#45;&gt;140246507906720 -->\n<g class=\"edge\" id=\"edge7\">\n<title>140246508085824-&gt;140246507906720</title>\n<path d=\"M208,-1245.3799C208,-1237.1745 208,-1227.7679 208,-1218.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1218.784 208,-1208.784 204.5001,-1218.784 211.5001,-1218.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246507986392 -->\n<g class=\"node\" id=\"node9\">\n<title>140246507986392</title>\n<polygon fill=\"none\" points=\"58.5,-1079.5 58.5,-1125.5 357.5,-1125.5 357.5,-1079.5 58.5,-1079.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1098.8\">activation_2: Activation</text>\n<polyline fill=\"none\" points=\"212.5,-1079.5 212.5,-1125.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-1110.3\">input:</text>\n<polyline fill=\"none\" points=\"212.5,-1102.5 270.5,-1102.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-1087.3\">output:</text>\n<polyline fill=\"none\" points=\"270.5,-1079.5 270.5,-1125.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-1110.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"270.5,-1102.5 357.5,-1102.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-1087.3\">(None, 800)</text>\n</g>\n<!-- 140246507906720&#45;&gt;140246507986392 -->\n<g class=\"edge\" id=\"edge8\">\n<title>140246507906720-&gt;140246507986392</title>\n<path d=\"M208,-1162.3799C208,-1154.1745 208,-1144.7679 208,-1135.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1135.784 208,-1125.784 204.5001,-1135.784 211.5001,-1135.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246508963096 -->\n<g class=\"node\" id=\"node10\">\n<title>140246508963096</title>\n<polygon fill=\"none\" points=\"82,-996.5 82,-1042.5 334,-1042.5 334,-996.5 82,-996.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1015.8\">dense_5: Dense</text>\n<polyline fill=\"none\" points=\"189,-996.5 189,-1042.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1027.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-1019.5 247,-1019.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1004.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-996.5 247,-1042.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1027.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"247,-1019.5 334,-1019.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1004.3\">(None, 800)</text>\n</g>\n<!-- 140246507986392&#45;&gt;140246508963096 -->\n<g class=\"edge\" id=\"edge9\">\n<title>140246507986392-&gt;140246508963096</title>\n<path d=\"M208,-1079.3799C208,-1071.1745 208,-1061.7679 208,-1052.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1052.784 208,-1042.784 204.5001,-1052.784 211.5001,-1052.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246507470184 -->\n<g class=\"node\" id=\"node11\">\n<title>140246507470184</title>\n<polygon fill=\"none\" points=\"82,-913.5 82,-959.5 334,-959.5 334,-913.5 82,-913.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-932.8\">dense_6: Dense</text>\n<polyline fill=\"none\" points=\"189,-913.5 189,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-944.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-936.5 247,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-921.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-913.5 247,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-944.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"247,-936.5 334,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-921.3\">(None, 800)</text>\n</g>\n<!-- 140246508963096&#45;&gt;140246507470184 -->\n<g class=\"edge\" id=\"edge10\">\n<title>140246508963096-&gt;140246507470184</title>\n<path d=\"M208,-996.3799C208,-988.1745 208,-978.7679 208,-969.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-969.784 208,-959.784 204.5001,-969.784 211.5001,-969.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246510221072 -->\n<g class=\"node\" id=\"node12\">\n<title>140246510221072</title>\n<polygon fill=\"none\" points=\"0,-830.5 0,-876.5 416,-876.5 416,-830.5 0,-830.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-849.8\">batch_normalization_3: BatchNormalization</text>\n<polyline fill=\"none\" points=\"271,-830.5 271,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-861.3\">input:</text>\n<polyline fill=\"none\" points=\"271,-853.5 329,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-838.3\">output:</text>\n<polyline fill=\"none\" points=\"329,-830.5 329,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-861.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"329,-853.5 416,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-838.3\">(None, 800)</text>\n</g>\n<!-- 140246507470184&#45;&gt;140246510221072 -->\n<g class=\"edge\" id=\"edge11\">\n<title>140246507470184-&gt;140246510221072</title>\n<path d=\"M208,-913.3799C208,-905.1745 208,-895.7679 208,-886.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-886.784 208,-876.784 204.5001,-886.784 211.5001,-886.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246510259784 -->\n<g class=\"node\" id=\"node13\">\n<title>140246510259784</title>\n<polygon fill=\"none\" points=\"58.5,-747.5 58.5,-793.5 357.5,-793.5 357.5,-747.5 58.5,-747.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-766.8\">activation_3: Activation</text>\n<polyline fill=\"none\" points=\"212.5,-747.5 212.5,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-778.3\">input:</text>\n<polyline fill=\"none\" points=\"212.5,-770.5 270.5,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-755.3\">output:</text>\n<polyline fill=\"none\" points=\"270.5,-747.5 270.5,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-778.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"270.5,-770.5 357.5,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-755.3\">(None, 800)</text>\n</g>\n<!-- 140246510221072&#45;&gt;140246510259784 -->\n<g class=\"edge\" id=\"edge12\">\n<title>140246510221072-&gt;140246510259784</title>\n<path d=\"M208,-830.3799C208,-822.1745 208,-812.7679 208,-803.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-803.784 208,-793.784 204.5001,-803.784 211.5001,-803.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246510282120 -->\n<g class=\"node\" id=\"node14\">\n<title>140246510282120</title>\n<polygon fill=\"none\" points=\"82,-664.5 82,-710.5 334,-710.5 334,-664.5 82,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-683.8\">dense_7: Dense</text>\n<polyline fill=\"none\" points=\"189,-664.5 189,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-687.5 247,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-664.5 247,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-695.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"247,-687.5 334,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-672.3\">(None, 800)</text>\n</g>\n<!-- 140246510259784&#45;&gt;140246510282120 -->\n<g class=\"edge\" id=\"edge13\">\n<title>140246510259784-&gt;140246510282120</title>\n<path d=\"M208,-747.3799C208,-739.1745 208,-729.7679 208,-720.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-720.784 208,-710.784 204.5001,-720.784 211.5001,-720.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246509980808 -->\n<g class=\"node\" id=\"node15\">\n<title>140246509980808</title>\n<polygon fill=\"none\" points=\"82,-581.5 82,-627.5 334,-627.5 334,-581.5 82,-581.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-600.8\">dense_8: Dense</text>\n<polyline fill=\"none\" points=\"189,-581.5 189,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-612.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-604.5 247,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-589.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-581.5 247,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-612.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"247,-604.5 334,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-589.3\">(None, 800)</text>\n</g>\n<!-- 140246510282120&#45;&gt;140246509980808 -->\n<g class=\"edge\" id=\"edge14\">\n<title>140246510282120-&gt;140246509980808</title>\n<path d=\"M208,-664.3799C208,-656.1745 208,-646.7679 208,-637.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-637.784 208,-627.784 204.5001,-637.784 211.5001,-637.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246510041296 -->\n<g class=\"node\" id=\"node16\">\n<title>140246510041296</title>\n<polygon fill=\"none\" points=\"0,-498.5 0,-544.5 416,-544.5 416,-498.5 0,-498.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-517.8\">batch_normalization_4: BatchNormalization</text>\n<polyline fill=\"none\" points=\"271,-498.5 271,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-529.3\">input:</text>\n<polyline fill=\"none\" points=\"271,-521.5 329,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-506.3\">output:</text>\n<polyline fill=\"none\" points=\"329,-498.5 329,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-529.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"329,-521.5 416,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-506.3\">(None, 800)</text>\n</g>\n<!-- 140246509980808&#45;&gt;140246510041296 -->\n<g class=\"edge\" id=\"edge15\">\n<title>140246509980808-&gt;140246510041296</title>\n<path d=\"M208,-581.3799C208,-573.1745 208,-563.7679 208,-554.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-554.784 208,-544.784 204.5001,-554.784 211.5001,-554.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246510067328 -->\n<g class=\"node\" id=\"node17\">\n<title>140246510067328</title>\n<polygon fill=\"none\" points=\"58.5,-415.5 58.5,-461.5 357.5,-461.5 357.5,-415.5 58.5,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-434.8\">activation_4: Activation</text>\n<polyline fill=\"none\" points=\"212.5,-415.5 212.5,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"212.5,-438.5 270.5,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"270.5,-415.5 270.5,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-446.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"270.5,-438.5 357.5,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-423.3\">(None, 800)</text>\n</g>\n<!-- 140246510041296&#45;&gt;140246510067328 -->\n<g class=\"edge\" id=\"edge16\">\n<title>140246510041296-&gt;140246510067328</title>\n<path d=\"M208,-498.3799C208,-490.1745 208,-480.7679 208,-471.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-471.784 208,-461.784 204.5001,-471.784 211.5001,-471.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246510093648 -->\n<g class=\"node\" id=\"node18\">\n<title>140246510093648</title>\n<polygon fill=\"none\" points=\"82,-332.5 82,-378.5 334,-378.5 334,-332.5 82,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-351.8\">dense_9: Dense</text>\n<polyline fill=\"none\" points=\"189,-332.5 189,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-355.5 247,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-332.5 247,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-363.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"247,-355.5 334,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-340.3\">(None, 800)</text>\n</g>\n<!-- 140246510067328&#45;&gt;140246510093648 -->\n<g class=\"edge\" id=\"edge17\">\n<title>140246510067328-&gt;140246510093648</title>\n<path d=\"M208,-415.3799C208,-407.1745 208,-397.7679 208,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-388.784 208,-378.784 204.5001,-388.784 211.5001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246509801312 -->\n<g class=\"node\" id=\"node19\">\n<title>140246509801312</title>\n<polygon fill=\"none\" points=\"78.5,-249.5 78.5,-295.5 337.5,-295.5 337.5,-249.5 78.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-268.8\">dense_10: Dense</text>\n<polyline fill=\"none\" points=\"192.5,-249.5 192.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"192.5,-272.5 250.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"250.5,-249.5 250.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"294\" y=\"-280.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"250.5,-272.5 337.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"294\" y=\"-257.3\">(None, 800)</text>\n</g>\n<!-- 140246510093648&#45;&gt;140246509801312 -->\n<g class=\"edge\" id=\"edge18\">\n<title>140246510093648-&gt;140246509801312</title>\n<path d=\"M208,-332.3799C208,-324.1745 208,-314.7679 208,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-305.784 208,-295.784 204.5001,-305.784 211.5001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246509862808 -->\n<g class=\"node\" id=\"node20\">\n<title>140246509862808</title>\n<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 416,-212.5 416,-166.5 0,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-185.8\">batch_normalization_5: BatchNormalization</text>\n<polyline fill=\"none\" points=\"271,-166.5 271,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"271,-189.5 329,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"329,-166.5 329,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-197.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"329,-189.5 416,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-174.3\">(None, 800)</text>\n</g>\n<!-- 140246509801312&#45;&gt;140246509862808 -->\n<g class=\"edge\" id=\"edge19\">\n<title>140246509801312-&gt;140246509862808</title>\n<path d=\"M208,-249.3799C208,-241.1745 208,-231.7679 208,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-222.784 208,-212.784 204.5001,-222.784 211.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246509862696 -->\n<g class=\"node\" id=\"node21\">\n<title>140246509862696</title>\n<polygon fill=\"none\" points=\"58.5,-83.5 58.5,-129.5 357.5,-129.5 357.5,-83.5 58.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-102.8\">activation_5: Activation</text>\n<polyline fill=\"none\" points=\"212.5,-83.5 212.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"212.5,-106.5 270.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"270.5,-83.5 270.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-114.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"270.5,-106.5 357.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-91.3\">(None, 800)</text>\n</g>\n<!-- 140246509862808&#45;&gt;140246509862696 -->\n<g class=\"edge\" id=\"edge20\">\n<title>140246509862808-&gt;140246509862696</title>\n<path d=\"M208,-166.3799C208,-158.1745 208,-148.7679 208,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-139.784 208,-129.784 204.5001,-139.784 211.5001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140246509920944 -->\n<g class=\"node\" id=\"node22\">\n<title>140246509920944</title>\n<polygon fill=\"none\" points=\"79,-.5 79,-46.5 337,-46.5 337,-.5 79,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-19.8\">dense_11: Dense</text>\n<polyline fill=\"none\" points=\"192,-.5 192,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"192,-23.5 250,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"250,-.5 250,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293.5\" y=\"-31.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"250,-23.5 337,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293.5\" y=\"-8.3\">(None, 4)</text>\n</g>\n<!-- 140246509862696&#45;&gt;140246509920944 -->\n<g class=\"edge\" id=\"edge21\">\n<title>140246509862696-&gt;140246509920944</title>\n<path d=\"M208,-83.3799C208,-75.1745 208,-65.7679 208,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-56.784 208,-46.784 204.5001,-56.784 211.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtD82zROogDP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "00946549-9038-4e04-dc0c-5fa8a8fab3cb"
      },
      "source": [
        "# 학습 과정\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('mean absolute error')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAAEHCAYAAAADGCkMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRMuL2mFogIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 예측값을 생성합니다.\n",
        "\n",
        "pred_test = model.predict(test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv6yp_Hzoq4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# submission 파일을 생성합니다.\n",
        "sample_sub = pd.read_csv('/gdrive/My Drive/DACON-semiconductor-competition/dataset/sample_submission.csv', index_col=0)\n",
        "submission = sample_sub+pred_test\n",
        "submission.to_csv('/gdrive/My Drive/DACON-semiconductor-competition/submission_12.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPOj7kOro8tD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습된 모델을 저장합니다.\n",
        "\n",
        "model.save('/gdrive/My Drive/DACON-semiconductor-competition/model_12.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VUa7WaWravb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f17a775e-4b3f-4d4e-a0f6-30fa652362ac"
      },
      "source": [
        "# 추가 학습\n",
        "\n",
        "hist = model.fit(train_X, train_Y, epochs=100, batch_size=630,\n",
        "                    validation_data=(val_X, val_Y))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/100\n",
            "607500/607500 [==============================] - 19s 32us/step - loss: 1.3029 - acc: 0.9660 - val_loss: 1.3019 - val_acc: 0.9628\n",
            "Epoch 2/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3134 - acc: 0.9657 - val_loss: 1.3008 - val_acc: 0.9572\n",
            "Epoch 3/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3144 - acc: 0.9655 - val_loss: 1.3140 - val_acc: 0.9666\n",
            "Epoch 4/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.3075 - acc: 0.9656 - val_loss: 1.2389 - val_acc: 0.9679\n",
            "Epoch 5/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2976 - acc: 0.9657 - val_loss: 1.2241 - val_acc: 0.9606\n",
            "Epoch 6/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2949 - acc: 0.9659 - val_loss: 1.1605 - val_acc: 0.9690\n",
            "Epoch 7/100\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 1.2943 - acc: 0.9659 - val_loss: 1.2962 - val_acc: 0.9591\n",
            "Epoch 8/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2988 - acc: 0.9657 - val_loss: 1.3161 - val_acc: 0.9698\n",
            "Epoch 9/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2901 - acc: 0.9660 - val_loss: 1.1440 - val_acc: 0.9579\n",
            "Epoch 10/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2904 - acc: 0.9657 - val_loss: 1.2123 - val_acc: 0.9735\n",
            "Epoch 11/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2864 - acc: 0.9660 - val_loss: 1.1883 - val_acc: 0.9628\n",
            "Epoch 12/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2877 - acc: 0.9659 - val_loss: 1.1809 - val_acc: 0.9610\n",
            "Epoch 13/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2799 - acc: 0.9656 - val_loss: 1.3658 - val_acc: 0.9715\n",
            "Epoch 14/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2903 - acc: 0.9658 - val_loss: 1.1273 - val_acc: 0.9729\n",
            "Epoch 15/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2719 - acc: 0.9660 - val_loss: 1.2132 - val_acc: 0.9590\n",
            "Epoch 16/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2759 - acc: 0.9658 - val_loss: 1.1321 - val_acc: 0.9757\n",
            "Epoch 17/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2806 - acc: 0.9656 - val_loss: 1.2587 - val_acc: 0.9680\n",
            "Epoch 18/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2708 - acc: 0.9658 - val_loss: 1.2045 - val_acc: 0.9672\n",
            "Epoch 19/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2611 - acc: 0.9656 - val_loss: 1.1941 - val_acc: 0.9569\n",
            "Epoch 20/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2617 - acc: 0.9660 - val_loss: 1.1830 - val_acc: 0.9603\n",
            "Epoch 21/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2638 - acc: 0.9659 - val_loss: 1.3532 - val_acc: 0.9646\n",
            "Epoch 22/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2584 - acc: 0.9657 - val_loss: 1.2042 - val_acc: 0.9638\n",
            "Epoch 23/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2688 - acc: 0.9658 - val_loss: 1.2008 - val_acc: 0.9766\n",
            "Epoch 24/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2632 - acc: 0.9657 - val_loss: 1.2388 - val_acc: 0.9695\n",
            "Epoch 25/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2562 - acc: 0.9659 - val_loss: 1.1190 - val_acc: 0.9617\n",
            "Epoch 26/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2521 - acc: 0.9657 - val_loss: 1.1652 - val_acc: 0.9677\n",
            "Epoch 27/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2371 - acc: 0.9659 - val_loss: 1.0666 - val_acc: 0.9686\n",
            "Epoch 28/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2562 - acc: 0.9654 - val_loss: 1.4713 - val_acc: 0.9688\n",
            "Epoch 29/100\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 1.2462 - acc: 0.9661 - val_loss: 1.2550 - val_acc: 0.9642\n",
            "Epoch 30/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2522 - acc: 0.9656 - val_loss: 1.1425 - val_acc: 0.9668\n",
            "Epoch 31/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2448 - acc: 0.9657 - val_loss: 1.2195 - val_acc: 0.9664\n",
            "Epoch 32/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2431 - acc: 0.9659 - val_loss: 1.1949 - val_acc: 0.9713\n",
            "Epoch 33/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2386 - acc: 0.9659 - val_loss: 1.1554 - val_acc: 0.9698\n",
            "Epoch 34/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2294 - acc: 0.9660 - val_loss: 1.0476 - val_acc: 0.9679\n",
            "Epoch 35/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2413 - acc: 0.9661 - val_loss: 1.1581 - val_acc: 0.9722\n",
            "Epoch 36/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2371 - acc: 0.9658 - val_loss: 1.0218 - val_acc: 0.9684\n",
            "Epoch 37/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2323 - acc: 0.9658 - val_loss: 1.1094 - val_acc: 0.9631\n",
            "Epoch 38/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2211 - acc: 0.9659 - val_loss: 1.3338 - val_acc: 0.9550\n",
            "Epoch 39/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2299 - acc: 0.9661 - val_loss: 1.1646 - val_acc: 0.9686\n",
            "Epoch 40/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2285 - acc: 0.9657 - val_loss: 1.1788 - val_acc: 0.9633\n",
            "Epoch 41/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2198 - acc: 0.9656 - val_loss: 1.0681 - val_acc: 0.9647\n",
            "Epoch 42/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2241 - acc: 0.9658 - val_loss: 1.1884 - val_acc: 0.9626\n",
            "Epoch 43/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2238 - acc: 0.9658 - val_loss: 1.1265 - val_acc: 0.9599\n",
            "Epoch 44/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2184 - acc: 0.9656 - val_loss: 1.0970 - val_acc: 0.9673\n",
            "Epoch 45/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2122 - acc: 0.9658 - val_loss: 1.1229 - val_acc: 0.9636\n",
            "Epoch 46/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2090 - acc: 0.9658 - val_loss: 1.1658 - val_acc: 0.9643\n",
            "Epoch 47/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2171 - acc: 0.9658 - val_loss: 1.1481 - val_acc: 0.9666\n",
            "Epoch 48/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2178 - acc: 0.9658 - val_loss: 1.0568 - val_acc: 0.9656\n",
            "Epoch 49/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2137 - acc: 0.9660 - val_loss: 1.2635 - val_acc: 0.9722\n",
            "Epoch 50/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2014 - acc: 0.9660 - val_loss: 1.1177 - val_acc: 0.9667\n",
            "Epoch 51/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2101 - acc: 0.9658 - val_loss: 1.1053 - val_acc: 0.9710\n",
            "Epoch 52/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.2053 - acc: 0.9660 - val_loss: 1.1077 - val_acc: 0.9697\n",
            "Epoch 53/100\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 1.2079 - acc: 0.9659 - val_loss: 1.1803 - val_acc: 0.9732\n",
            "Epoch 54/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2098 - acc: 0.9658 - val_loss: 1.1297 - val_acc: 0.9598\n",
            "Epoch 55/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.2026 - acc: 0.9661 - val_loss: 1.0743 - val_acc: 0.9659\n",
            "Epoch 56/100\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 1.1935 - acc: 0.9658 - val_loss: 1.0912 - val_acc: 0.9670\n",
            "Epoch 57/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1916 - acc: 0.9659 - val_loss: 1.0823 - val_acc: 0.9671\n",
            "Epoch 58/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1899 - acc: 0.9660 - val_loss: 1.0059 - val_acc: 0.9651\n",
            "Epoch 59/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1902 - acc: 0.9657 - val_loss: 1.0409 - val_acc: 0.9592\n",
            "Epoch 60/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1881 - acc: 0.9660 - val_loss: 1.0815 - val_acc: 0.9728\n",
            "Epoch 61/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1963 - acc: 0.9660 - val_loss: 1.0396 - val_acc: 0.9633\n",
            "Epoch 62/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1877 - acc: 0.9657 - val_loss: 1.0654 - val_acc: 0.9663\n",
            "Epoch 63/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1880 - acc: 0.9659 - val_loss: 1.1514 - val_acc: 0.9693\n",
            "Epoch 64/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1713 - acc: 0.9661 - val_loss: 1.0626 - val_acc: 0.9728\n",
            "Epoch 65/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1846 - acc: 0.9662 - val_loss: 1.0743 - val_acc: 0.9599\n",
            "Epoch 66/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1854 - acc: 0.9657 - val_loss: 1.0551 - val_acc: 0.9620\n",
            "Epoch 67/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1852 - acc: 0.9658 - val_loss: 1.1669 - val_acc: 0.9615\n",
            "Epoch 68/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1700 - acc: 0.9661 - val_loss: 1.0998 - val_acc: 0.9704\n",
            "Epoch 69/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1769 - acc: 0.9658 - val_loss: 1.0588 - val_acc: 0.9783\n",
            "Epoch 70/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1748 - acc: 0.9658 - val_loss: 1.1223 - val_acc: 0.9716\n",
            "Epoch 71/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1709 - acc: 0.9658 - val_loss: 1.0630 - val_acc: 0.9650\n",
            "Epoch 72/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1745 - acc: 0.9660 - val_loss: 1.1245 - val_acc: 0.9654\n",
            "Epoch 73/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1728 - acc: 0.9659 - val_loss: 1.1184 - val_acc: 0.9593\n",
            "Epoch 74/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1768 - acc: 0.9659 - val_loss: 1.0333 - val_acc: 0.9644\n",
            "Epoch 75/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1665 - acc: 0.9657 - val_loss: 1.0273 - val_acc: 0.9658\n",
            "Epoch 76/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1642 - acc: 0.9659 - val_loss: 1.0654 - val_acc: 0.9680\n",
            "Epoch 77/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1742 - acc: 0.9660 - val_loss: 1.2141 - val_acc: 0.9720\n",
            "Epoch 78/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1579 - acc: 0.9659 - val_loss: 1.0091 - val_acc: 0.9673\n",
            "Epoch 79/100\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 1.1709 - acc: 0.9660 - val_loss: 1.0441 - val_acc: 0.9646\n",
            "Epoch 80/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1586 - acc: 0.9662 - val_loss: 1.1231 - val_acc: 0.9664\n",
            "Epoch 81/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1595 - acc: 0.9658 - val_loss: 1.0459 - val_acc: 0.9636\n",
            "Epoch 82/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1555 - acc: 0.9661 - val_loss: 1.0175 - val_acc: 0.9728\n",
            "Epoch 83/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1507 - acc: 0.9659 - val_loss: 1.0342 - val_acc: 0.9606\n",
            "Epoch 84/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1531 - acc: 0.9660 - val_loss: 0.9662 - val_acc: 0.9742\n",
            "Epoch 85/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1602 - acc: 0.9662 - val_loss: 1.2285 - val_acc: 0.9718\n",
            "Epoch 86/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1502 - acc: 0.9663 - val_loss: 1.1923 - val_acc: 0.9694\n",
            "Epoch 87/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1491 - acc: 0.9660 - val_loss: 0.9645 - val_acc: 0.9625\n",
            "Epoch 88/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1559 - acc: 0.9660 - val_loss: 1.0092 - val_acc: 0.9655\n",
            "Epoch 89/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1432 - acc: 0.9660 - val_loss: 1.0465 - val_acc: 0.9665\n",
            "Epoch 90/100\n",
            "607500/607500 [==============================] - 19s 30us/step - loss: 1.1376 - acc: 0.9661 - val_loss: 1.0449 - val_acc: 0.9721\n",
            "Epoch 91/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1504 - acc: 0.9661 - val_loss: 0.9893 - val_acc: 0.9758\n",
            "Epoch 92/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1513 - acc: 0.9661 - val_loss: 1.1171 - val_acc: 0.9746\n",
            "Epoch 93/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1386 - acc: 0.9660 - val_loss: 1.0900 - val_acc: 0.9746\n",
            "Epoch 94/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1409 - acc: 0.9658 - val_loss: 1.0578 - val_acc: 0.9553\n",
            "Epoch 95/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1406 - acc: 0.9661 - val_loss: 1.0339 - val_acc: 0.9686\n",
            "Epoch 96/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1366 - acc: 0.9658 - val_loss: 1.0709 - val_acc: 0.9738\n",
            "Epoch 97/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1380 - acc: 0.9660 - val_loss: 1.2020 - val_acc: 0.9708\n",
            "Epoch 98/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1295 - acc: 0.9660 - val_loss: 1.0610 - val_acc: 0.9704\n",
            "Epoch 99/100\n",
            "607500/607500 [==============================] - 18s 30us/step - loss: 1.1414 - acc: 0.9659 - val_loss: 1.0564 - val_acc: 0.9721\n",
            "Epoch 100/100\n",
            "607500/607500 [==============================] - 19s 31us/step - loss: 1.1239 - acc: 0.9660 - val_loss: 1.0687 - val_acc: 0.9703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbHwRShAyuCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 예측값을 생성합니다.\n",
        "\n",
        "pred_test = model.predict(test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuNsrIxpywow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# submission 파일을 생성합니다.\n",
        "sample_sub = pd.read_csv('/gdrive/My Drive/DACON-semiconductor-competition/dataset/sample_submission.csv', index_col=0)\n",
        "submission = sample_sub+pred_test\n",
        "submission.to_csv('/gdrive/My Drive/DACON-semiconductor-competition/submission_12_300.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D82x896pywtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습된 모델을 저장합니다.\n",
        "\n",
        "model.save('/gdrive/My Drive/DACON-semiconductor-competition/model_12_300.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3IaqhJqmV_5",
        "colab_type": "text"
      },
      "source": [
        "### Bayesian Optimization\n",
        "http://research.sualab.com/introduction/practice/2019/02/19/bayesian-optimization-overview-1.html<br>\n",
        "http://research.sualab.com/introduction/practice/2019/04/01/bayesian-optimization-overview-2.html<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNwDuaRWmWg4",
        "colab_type": "text"
      },
      "source": [
        "### Swish Activation\n",
        "https://www.machinecurve.com/index.php/2019/05/30/why-swish-could-perform-better-than-relu/#todays-activation-functions"
      ]
    }
  ]
}