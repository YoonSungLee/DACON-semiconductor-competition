{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "submission_13.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOyaij7Cvm/3znG0Kl0bOEi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Inha-AI/DACON-semiconductor-competition/blob/feature%2FYoonSungLee/submission_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBPfb1Jhq4TA",
        "colab_type": "code",
        "outputId": "6fc6a804-db21-432e-c410-80303783dd25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "pip install bayesian-optimization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/72/0c/173ac467d0a53e33e41b521e4ceba74a8ac7c7873d7b857a8fbdca88302d/bayesian-optimization-1.0.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.17.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.22.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.14.1)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.0.1-cp36-none-any.whl size=10032 sha256=7c33169f4d678fbe336de52205323b3b3daa391e9b8b7d4bf2c68da8b8cf8ac9\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/0d/3b/6b9d4477a34b3905f246ff4e7acf6aafd4cc9b77d473629b77\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vVeUdyssBKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bayes_opt import BayesianOptimization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwHXDEigsNB0",
        "colab_type": "code",
        "outputId": "a8a75111-81b8-4ee9-a317-04024f57a97f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQF6BGGF1DqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 자료형을 적절히 변형시켜 데이터의 크기를 줄이는 방법\n",
        "\n",
        "# for col in df_train.columns:\n",
        "#     col_type = df_train[col].dtypes\n",
        "#     min1 = df_train[col].min()\n",
        "#     max1 = df_train[col].max()\n",
        "#     if str(col_type)[:3] == 'int':\n",
        "#         df_train[col] = df_train[col].astype(np.int16)\n",
        "#     else:\n",
        "#         if min1 > np.finfo(np.float16).min and max1 < np.finfo(np.float16).max:\n",
        "#             df_train[col] = trdf_trainain[col].astype(np.float16)\n",
        "#         elif min1 > np.finfo(np.float32).min and max1 < np.finfo(np.float32).max:\n",
        "#             df_train[col] = df_train[col].astype(np.float32)\n",
        "#         else:\n",
        "#             df_train[col] = df_train[col].astype(np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BNXAU5rsPJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv('/gdrive/My Drive/DACON-semiconductor-competition/dataset/train.csv')\n",
        "df_test = pd.read_csv('/gdrive/My Drive/DACON-semiconductor-competition/dataset/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdYXclfgsVr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 독립변수와 종속변수를 분리합니다.\n",
        "\n",
        "train_X = df_train.iloc[:,4:]\n",
        "train_Y = df_train.iloc[:,0:4]\n",
        "test_X = df_test.iloc[:,1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogdcz8YctuVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train set을 shuffle하여 다시 train set과 validation set으로 분리합니다.\n",
        "\n",
        "train_X, val_X, train_Y, val_Y = train_test_split(train_X, train_Y, test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tT4w6XasaSQ",
        "colab_type": "text"
      },
      "source": [
        "# Model 13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PSSfoS5sfWK",
        "colab_type": "text"
      },
      "source": [
        "* 13 layers\n",
        "* 800 units, he_normal, swish\n",
        "* BatchNormalization\n",
        "* Adam(0.001)\n",
        "* epochs 200\n",
        "* batch_size 630\n",
        "<br><br>\n",
        "* BayesianOptimization을 통해 최적의 units를 도출해 내어 적용한 결과가 좋기 때문에, 층을 더욱 깊게하여 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGEkxH551LkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 케라스를 통해 모델 생성을 시작합니다.\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=800, input_dim=226, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(Dense(units=800, kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(swish))\n",
        "    model.add(Dense(units=4, activation='linear'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Activation Function 정의\n",
        "\n",
        "def swish(x) :\n",
        "    return x * keras.activations.sigmoid(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35DQBYFkofR6",
        "colab_type": "code",
        "outputId": "ec183775-1910-4d17-9ac8-82596bbbdbfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "model = create_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpTWpi6eXFec",
        "colab_type": "code",
        "outputId": "72dabc45-47ed-4b86-c4bf-c486b1477494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "adam = keras.optimizers.Adam(0.001)\n",
        "model.compile(loss='mae', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byKqdJKjXFjb",
        "colab_type": "code",
        "outputId": "fb7481b0-2992-46fa-cbcc-eae4e4b32226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist = model.fit(train_X, train_Y, epochs=200, batch_size=630,\n",
        "                    validation_data=(val_X, val_Y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "607500/607500 [==============================] - 33s 54us/step - loss: 68.1616 - acc: 0.3622 - val_loss: 59.0159 - val_acc: 0.4159\n",
            "Epoch 2/200\n",
            "607500/607500 [==============================] - 22s 37us/step - loss: 30.0622 - acc: 0.6902 - val_loss: 27.0253 - val_acc: 0.7236\n",
            "Epoch 3/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 18.6063 - acc: 0.8081 - val_loss: 20.5102 - val_acc: 0.8000\n",
            "Epoch 4/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 13.5395 - acc: 0.8586 - val_loss: 13.2191 - val_acc: 0.8562\n",
            "Epoch 5/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 10.7730 - acc: 0.8852 - val_loss: 11.4903 - val_acc: 0.8713\n",
            "Epoch 6/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 9.1148 - acc: 0.9023 - val_loss: 9.7902 - val_acc: 0.8857\n",
            "Epoch 7/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 8.0314 - acc: 0.9123 - val_loss: 8.9309 - val_acc: 0.9064\n",
            "Epoch 8/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 7.2572 - acc: 0.9202 - val_loss: 7.5480 - val_acc: 0.9228\n",
            "Epoch 9/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 6.7065 - acc: 0.9259 - val_loss: 6.0532 - val_acc: 0.9234\n",
            "Epoch 10/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 6.3037 - acc: 0.9298 - val_loss: 6.5774 - val_acc: 0.9302\n",
            "Epoch 11/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 5.9121 - acc: 0.9337 - val_loss: 6.5432 - val_acc: 0.9338\n",
            "Epoch 12/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 5.6016 - acc: 0.9369 - val_loss: 5.8368 - val_acc: 0.9373\n",
            "Epoch 13/200\n",
            "607500/607500 [==============================] - 22s 37us/step - loss: 5.3536 - acc: 0.9385 - val_loss: 4.9375 - val_acc: 0.9391\n",
            "Epoch 14/200\n",
            "607500/607500 [==============================] - 23s 37us/step - loss: 5.1025 - acc: 0.9408 - val_loss: 5.1160 - val_acc: 0.9448\n",
            "Epoch 15/200\n",
            "607500/607500 [==============================] - 23s 37us/step - loss: 4.9472 - acc: 0.9423 - val_loss: 4.9920 - val_acc: 0.9400\n",
            "Epoch 16/200\n",
            "607500/607500 [==============================] - 22s 37us/step - loss: 4.7469 - acc: 0.9438 - val_loss: 4.3374 - val_acc: 0.9483\n",
            "Epoch 17/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 4.5712 - acc: 0.9459 - val_loss: 4.7282 - val_acc: 0.9351\n",
            "Epoch 18/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 4.4694 - acc: 0.9466 - val_loss: 4.3568 - val_acc: 0.9505\n",
            "Epoch 19/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 4.2944 - acc: 0.9484 - val_loss: 4.4742 - val_acc: 0.9466\n",
            "Epoch 20/200\n",
            "607500/607500 [==============================] - 22s 35us/step - loss: 4.1559 - acc: 0.9492 - val_loss: 4.4797 - val_acc: 0.9362\n",
            "Epoch 21/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 4.0636 - acc: 0.9504 - val_loss: 3.7794 - val_acc: 0.9544\n",
            "Epoch 22/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 3.9284 - acc: 0.9511 - val_loss: 3.6402 - val_acc: 0.9558\n",
            "Epoch 23/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 3.8395 - acc: 0.9524 - val_loss: 3.5176 - val_acc: 0.9560\n",
            "Epoch 24/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 3.7721 - acc: 0.9527 - val_loss: 3.5441 - val_acc: 0.9517\n",
            "Epoch 25/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 3.6047 - acc: 0.9538 - val_loss: 3.7348 - val_acc: 0.9518\n",
            "Epoch 26/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 3.5400 - acc: 0.9546 - val_loss: 3.8245 - val_acc: 0.9523\n",
            "Epoch 27/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 3.4323 - acc: 0.9556 - val_loss: 3.2891 - val_acc: 0.9496\n",
            "Epoch 28/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 3.3741 - acc: 0.9557 - val_loss: 3.1915 - val_acc: 0.9605\n",
            "Epoch 29/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 3.3252 - acc: 0.9562 - val_loss: 3.1106 - val_acc: 0.9573\n",
            "Epoch 30/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 3.2229 - acc: 0.9568 - val_loss: 3.5067 - val_acc: 0.9639\n",
            "Epoch 31/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 3.1511 - acc: 0.9575 - val_loss: 3.0543 - val_acc: 0.9613\n",
            "Epoch 32/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 3.1094 - acc: 0.9578 - val_loss: 2.9409 - val_acc: 0.9699\n",
            "Epoch 33/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 3.0391 - acc: 0.9582 - val_loss: 3.3269 - val_acc: 0.9593\n",
            "Epoch 34/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 2.9852 - acc: 0.9585 - val_loss: 2.7008 - val_acc: 0.9659\n",
            "Epoch 35/200\n",
            "607500/607500 [==============================] - 22s 37us/step - loss: 2.8943 - acc: 0.9593 - val_loss: 2.9802 - val_acc: 0.9558\n",
            "Epoch 36/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 2.8749 - acc: 0.9590 - val_loss: 3.2082 - val_acc: 0.9595\n",
            "Epoch 37/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 2.7892 - acc: 0.9600 - val_loss: 2.6793 - val_acc: 0.9528\n",
            "Epoch 38/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 2.7766 - acc: 0.9600 - val_loss: 3.0064 - val_acc: 0.9638\n",
            "Epoch 39/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 2.6974 - acc: 0.9604 - val_loss: 2.4024 - val_acc: 0.9554\n",
            "Epoch 40/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 2.6248 - acc: 0.9613 - val_loss: 2.6796 - val_acc: 0.9619\n",
            "Epoch 41/200\n",
            "607500/607500 [==============================] - 22s 35us/step - loss: 2.5995 - acc: 0.9612 - val_loss: 2.6104 - val_acc: 0.9704\n",
            "Epoch 42/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 2.5727 - acc: 0.9609 - val_loss: 2.5329 - val_acc: 0.9703\n",
            "Epoch 43/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 2.5145 - acc: 0.9615 - val_loss: 2.4873 - val_acc: 0.9641\n",
            "Epoch 44/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 2.4811 - acc: 0.9617 - val_loss: 2.4952 - val_acc: 0.9685\n",
            "Epoch 45/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 2.4418 - acc: 0.9619 - val_loss: 2.5920 - val_acc: 0.9629\n",
            "Epoch 46/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 2.4259 - acc: 0.9620 - val_loss: 2.1832 - val_acc: 0.9580\n",
            "Epoch 47/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 2.3606 - acc: 0.9623 - val_loss: 2.4183 - val_acc: 0.9600\n",
            "Epoch 48/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 2.3525 - acc: 0.9624 - val_loss: 2.5114 - val_acc: 0.9620\n",
            "Epoch 49/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 2.2923 - acc: 0.9624 - val_loss: 2.1008 - val_acc: 0.9675\n",
            "Epoch 50/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 2.2853 - acc: 0.9627 - val_loss: 2.2758 - val_acc: 0.9648\n",
            "Epoch 51/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 2.2649 - acc: 0.9628 - val_loss: 2.2963 - val_acc: 0.9588\n",
            "Epoch 52/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 2.2344 - acc: 0.9627 - val_loss: 2.2734 - val_acc: 0.9648\n",
            "Epoch 53/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 2.2047 - acc: 0.9629 - val_loss: 2.2382 - val_acc: 0.9625\n",
            "Epoch 54/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 2.1671 - acc: 0.9630 - val_loss: 1.8794 - val_acc: 0.9638\n",
            "Epoch 55/200\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 2.1661 - acc: 0.9631 - val_loss: 2.1088 - val_acc: 0.9685\n",
            "Epoch 56/200\n",
            "607500/607500 [==============================] - 22s 35us/step - loss: 2.1207 - acc: 0.9634 - val_loss: 2.1174 - val_acc: 0.9560\n",
            "Epoch 57/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 2.1001 - acc: 0.9637 - val_loss: 1.9762 - val_acc: 0.9707\n",
            "Epoch 58/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 2.0874 - acc: 0.9634 - val_loss: 2.1541 - val_acc: 0.9537\n",
            "Epoch 59/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 2.0879 - acc: 0.9634 - val_loss: 2.0755 - val_acc: 0.9703\n",
            "Epoch 60/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 2.0535 - acc: 0.9635 - val_loss: 1.9525 - val_acc: 0.9628\n",
            "Epoch 61/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 2.0437 - acc: 0.9636 - val_loss: 1.8834 - val_acc: 0.9647\n",
            "Epoch 62/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 2.0107 - acc: 0.9637 - val_loss: 1.8951 - val_acc: 0.9603\n",
            "Epoch 63/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.9889 - acc: 0.9640 - val_loss: 2.4123 - val_acc: 0.9611\n",
            "Epoch 64/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 2.0025 - acc: 0.9638 - val_loss: 2.0224 - val_acc: 0.9655\n",
            "Epoch 65/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.9648 - acc: 0.9641 - val_loss: 2.0534 - val_acc: 0.9584\n",
            "Epoch 66/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.9523 - acc: 0.9639 - val_loss: 1.8208 - val_acc: 0.9632\n",
            "Epoch 67/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.9560 - acc: 0.9637 - val_loss: 2.1454 - val_acc: 0.9720\n",
            "Epoch 68/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.9450 - acc: 0.9642 - val_loss: 1.6816 - val_acc: 0.9680\n",
            "Epoch 69/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.9134 - acc: 0.9644 - val_loss: 1.8957 - val_acc: 0.9673\n",
            "Epoch 70/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.8902 - acc: 0.9642 - val_loss: 1.8325 - val_acc: 0.9697\n",
            "Epoch 71/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.8863 - acc: 0.9641 - val_loss: 2.0934 - val_acc: 0.9661\n",
            "Epoch 72/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.9054 - acc: 0.9641 - val_loss: 1.8851 - val_acc: 0.9697\n",
            "Epoch 73/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.8596 - acc: 0.9643 - val_loss: 1.7641 - val_acc: 0.9629\n",
            "Epoch 74/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.8473 - acc: 0.9644 - val_loss: 1.8359 - val_acc: 0.9773\n",
            "Epoch 75/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.8193 - acc: 0.9646 - val_loss: 1.5960 - val_acc: 0.9689\n",
            "Epoch 76/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.8415 - acc: 0.9643 - val_loss: 1.7967 - val_acc: 0.9723\n",
            "Epoch 77/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.8097 - acc: 0.9642 - val_loss: 1.6967 - val_acc: 0.9584\n",
            "Epoch 78/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.8017 - acc: 0.9646 - val_loss: 2.1285 - val_acc: 0.9603\n",
            "Epoch 79/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.7809 - acc: 0.9645 - val_loss: 1.7340 - val_acc: 0.9591\n",
            "Epoch 80/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.7939 - acc: 0.9644 - val_loss: 1.8179 - val_acc: 0.9768\n",
            "Epoch 81/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.7825 - acc: 0.9647 - val_loss: 1.5931 - val_acc: 0.9678\n",
            "Epoch 82/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.7598 - acc: 0.9645 - val_loss: 1.9476 - val_acc: 0.9641\n",
            "Epoch 83/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.7562 - acc: 0.9646 - val_loss: 1.7583 - val_acc: 0.9595\n",
            "Epoch 84/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.7490 - acc: 0.9644 - val_loss: 1.7279 - val_acc: 0.9676\n",
            "Epoch 85/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.7183 - acc: 0.9647 - val_loss: 1.7310 - val_acc: 0.9597\n",
            "Epoch 86/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.7271 - acc: 0.9649 - val_loss: 1.8980 - val_acc: 0.9625\n",
            "Epoch 87/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.7061 - acc: 0.9647 - val_loss: 1.5772 - val_acc: 0.9619\n",
            "Epoch 88/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.7073 - acc: 0.9645 - val_loss: 1.5455 - val_acc: 0.9685\n",
            "Epoch 89/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.6914 - acc: 0.9650 - val_loss: 1.9657 - val_acc: 0.9669\n",
            "Epoch 90/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.6762 - acc: 0.9651 - val_loss: 1.7742 - val_acc: 0.9692\n",
            "Epoch 91/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 1.6935 - acc: 0.9653 - val_loss: 1.7570 - val_acc: 0.9758\n",
            "Epoch 92/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.6733 - acc: 0.9651 - val_loss: 1.6618 - val_acc: 0.9639\n",
            "Epoch 93/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.6845 - acc: 0.9649 - val_loss: 1.7095 - val_acc: 0.9667\n",
            "Epoch 94/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.6417 - acc: 0.9649 - val_loss: 1.8508 - val_acc: 0.9733\n",
            "Epoch 95/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.6378 - acc: 0.9651 - val_loss: 1.6269 - val_acc: 0.9633\n",
            "Epoch 96/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.6570 - acc: 0.9646 - val_loss: 1.6635 - val_acc: 0.9597\n",
            "Epoch 97/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.6312 - acc: 0.9650 - val_loss: 1.6461 - val_acc: 0.9665\n",
            "Epoch 98/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.6108 - acc: 0.9650 - val_loss: 1.5516 - val_acc: 0.9692\n",
            "Epoch 99/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.6207 - acc: 0.9650 - val_loss: 1.4850 - val_acc: 0.9684\n",
            "Epoch 100/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.6051 - acc: 0.9647 - val_loss: 1.5130 - val_acc: 0.9712\n",
            "Epoch 101/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.6220 - acc: 0.9651 - val_loss: 1.5963 - val_acc: 0.9662\n",
            "Epoch 102/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.5786 - acc: 0.9652 - val_loss: 1.5365 - val_acc: 0.9614\n",
            "Epoch 103/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 1.6013 - acc: 0.9652 - val_loss: 1.6716 - val_acc: 0.9615\n",
            "Epoch 104/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.5849 - acc: 0.9652 - val_loss: 1.5277 - val_acc: 0.9663\n",
            "Epoch 105/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.5831 - acc: 0.9652 - val_loss: 1.5729 - val_acc: 0.9556\n",
            "Epoch 106/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.5753 - acc: 0.9651 - val_loss: 1.4681 - val_acc: 0.9697\n",
            "Epoch 107/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.5604 - acc: 0.9653 - val_loss: 1.6768 - val_acc: 0.9638\n",
            "Epoch 108/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.5537 - acc: 0.9653 - val_loss: 1.5197 - val_acc: 0.9705\n",
            "Epoch 109/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.5613 - acc: 0.9650 - val_loss: 1.7753 - val_acc: 0.9723\n",
            "Epoch 110/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.5561 - acc: 0.9653 - val_loss: 1.3722 - val_acc: 0.9638\n",
            "Epoch 111/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.5438 - acc: 0.9653 - val_loss: 1.4995 - val_acc: 0.9641\n",
            "Epoch 112/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.5367 - acc: 0.9651 - val_loss: 1.5177 - val_acc: 0.9676\n",
            "Epoch 113/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.5259 - acc: 0.9650 - val_loss: 1.6578 - val_acc: 0.9711\n",
            "Epoch 114/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.5301 - acc: 0.9653 - val_loss: 1.4806 - val_acc: 0.9642\n",
            "Epoch 115/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 1.5209 - acc: 0.9650 - val_loss: 1.3588 - val_acc: 0.9673\n",
            "Epoch 116/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.5233 - acc: 0.9654 - val_loss: 1.4497 - val_acc: 0.9751\n",
            "Epoch 117/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.5127 - acc: 0.9652 - val_loss: 1.4008 - val_acc: 0.9651\n",
            "Epoch 118/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 1.4983 - acc: 0.9652 - val_loss: 1.3958 - val_acc: 0.9595\n",
            "Epoch 119/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.4993 - acc: 0.9652 - val_loss: 1.5551 - val_acc: 0.9578\n",
            "Epoch 120/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.4864 - acc: 0.9654 - val_loss: 1.5244 - val_acc: 0.9676\n",
            "Epoch 121/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 1.4803 - acc: 0.9651 - val_loss: 1.6141 - val_acc: 0.9697\n",
            "Epoch 122/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 1.4916 - acc: 0.9651 - val_loss: 1.3629 - val_acc: 0.9746\n",
            "Epoch 123/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.4649 - acc: 0.9656 - val_loss: 1.3660 - val_acc: 0.9521\n",
            "Epoch 124/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.4661 - acc: 0.9655 - val_loss: 1.5057 - val_acc: 0.9653\n",
            "Epoch 125/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.4665 - acc: 0.9653 - val_loss: 1.4773 - val_acc: 0.9698\n",
            "Epoch 126/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.4788 - acc: 0.9653 - val_loss: 1.5203 - val_acc: 0.9669\n",
            "Epoch 127/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.4548 - acc: 0.9655 - val_loss: 1.4979 - val_acc: 0.9672\n",
            "Epoch 128/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 1.4490 - acc: 0.9653 - val_loss: 1.4363 - val_acc: 0.9700\n",
            "Epoch 129/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.4534 - acc: 0.9653 - val_loss: 1.2794 - val_acc: 0.9607\n",
            "Epoch 130/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 1.4477 - acc: 0.9654 - val_loss: 1.5270 - val_acc: 0.9678\n",
            "Epoch 131/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.4341 - acc: 0.9654 - val_loss: 1.4206 - val_acc: 0.9575\n",
            "Epoch 132/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.4328 - acc: 0.9655 - val_loss: 1.3993 - val_acc: 0.9728\n",
            "Epoch 133/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 1.4375 - acc: 0.9656 - val_loss: 1.3017 - val_acc: 0.9568\n",
            "Epoch 134/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.4324 - acc: 0.9656 - val_loss: 1.2224 - val_acc: 0.9714\n",
            "Epoch 135/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.4373 - acc: 0.9656 - val_loss: 1.3525 - val_acc: 0.9668\n",
            "Epoch 136/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 1.4113 - acc: 0.9655 - val_loss: 1.3580 - val_acc: 0.9662\n",
            "Epoch 137/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.4112 - acc: 0.9656 - val_loss: 1.2675 - val_acc: 0.9646\n",
            "Epoch 138/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.4237 - acc: 0.9655 - val_loss: 1.2578 - val_acc: 0.9676\n",
            "Epoch 139/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.4129 - acc: 0.9654 - val_loss: 1.2742 - val_acc: 0.9684\n",
            "Epoch 140/200\n",
            "607500/607500 [==============================] - 20s 34us/step - loss: 1.4130 - acc: 0.9657 - val_loss: 1.3176 - val_acc: 0.9557\n",
            "Epoch 141/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.4045 - acc: 0.9653 - val_loss: 1.3085 - val_acc: 0.9658\n",
            "Epoch 142/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.3838 - acc: 0.9659 - val_loss: 1.3731 - val_acc: 0.9695\n",
            "Epoch 143/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.3925 - acc: 0.9658 - val_loss: 1.4647 - val_acc: 0.9596\n",
            "Epoch 144/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.3942 - acc: 0.9656 - val_loss: 1.2164 - val_acc: 0.9697\n",
            "Epoch 145/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.3906 - acc: 0.9658 - val_loss: 1.4616 - val_acc: 0.9628\n",
            "Epoch 146/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.3961 - acc: 0.9656 - val_loss: 1.5125 - val_acc: 0.9768\n",
            "Epoch 147/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.3781 - acc: 0.9654 - val_loss: 1.2923 - val_acc: 0.9618\n",
            "Epoch 148/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.3588 - acc: 0.9657 - val_loss: 1.2062 - val_acc: 0.9700\n",
            "Epoch 149/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.3654 - acc: 0.9659 - val_loss: 1.2345 - val_acc: 0.9695\n",
            "Epoch 150/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.3720 - acc: 0.9656 - val_loss: 1.2837 - val_acc: 0.9535\n",
            "Epoch 151/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.3692 - acc: 0.9657 - val_loss: 1.6407 - val_acc: 0.9617\n",
            "Epoch 152/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.3468 - acc: 0.9657 - val_loss: 1.2161 - val_acc: 0.9627\n",
            "Epoch 153/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.3677 - acc: 0.9656 - val_loss: 1.3290 - val_acc: 0.9551\n",
            "Epoch 154/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.3548 - acc: 0.9655 - val_loss: 1.3855 - val_acc: 0.9779\n",
            "Epoch 155/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.3518 - acc: 0.9656 - val_loss: 1.2886 - val_acc: 0.9709\n",
            "Epoch 156/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.3462 - acc: 0.9657 - val_loss: 1.2920 - val_acc: 0.9645\n",
            "Epoch 157/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.3410 - acc: 0.9658 - val_loss: 1.1703 - val_acc: 0.9614\n",
            "Epoch 158/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.3473 - acc: 0.9656 - val_loss: 1.2981 - val_acc: 0.9652\n",
            "Epoch 159/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.3318 - acc: 0.9657 - val_loss: 1.2486 - val_acc: 0.9634\n",
            "Epoch 160/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.3313 - acc: 0.9656 - val_loss: 1.1926 - val_acc: 0.9610\n",
            "Epoch 161/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.3322 - acc: 0.9657 - val_loss: 1.4332 - val_acc: 0.9720\n",
            "Epoch 162/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.3326 - acc: 0.9655 - val_loss: 1.3428 - val_acc: 0.9686\n",
            "Epoch 163/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.3072 - acc: 0.9660 - val_loss: 1.2636 - val_acc: 0.9737\n",
            "Epoch 164/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.3251 - acc: 0.9656 - val_loss: 1.2764 - val_acc: 0.9591\n",
            "Epoch 165/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.3092 - acc: 0.9658 - val_loss: 1.1631 - val_acc: 0.9587\n",
            "Epoch 166/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.3151 - acc: 0.9659 - val_loss: 1.3440 - val_acc: 0.9644\n",
            "Epoch 167/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.3035 - acc: 0.9660 - val_loss: 1.3974 - val_acc: 0.9715\n",
            "Epoch 168/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.2990 - acc: 0.9658 - val_loss: 1.1793 - val_acc: 0.9619\n",
            "Epoch 169/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.3070 - acc: 0.9658 - val_loss: 1.3029 - val_acc: 0.9639\n",
            "Epoch 170/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.3042 - acc: 0.9656 - val_loss: 1.3591 - val_acc: 0.9702\n",
            "Epoch 171/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.3049 - acc: 0.9660 - val_loss: 1.2334 - val_acc: 0.9597\n",
            "Epoch 172/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.3049 - acc: 0.9654 - val_loss: 1.1639 - val_acc: 0.9714\n",
            "Epoch 173/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.2927 - acc: 0.9659 - val_loss: 1.3386 - val_acc: 0.9623\n",
            "Epoch 174/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.2973 - acc: 0.9659 - val_loss: 1.3122 - val_acc: 0.9727\n",
            "Epoch 175/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2906 - acc: 0.9659 - val_loss: 1.1472 - val_acc: 0.9602\n",
            "Epoch 176/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2865 - acc: 0.9658 - val_loss: 1.4028 - val_acc: 0.9636\n",
            "Epoch 177/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2758 - acc: 0.9660 - val_loss: 1.3305 - val_acc: 0.9664\n",
            "Epoch 178/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2689 - acc: 0.9655 - val_loss: 1.2209 - val_acc: 0.9737\n",
            "Epoch 179/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2736 - acc: 0.9661 - val_loss: 1.1328 - val_acc: 0.9667\n",
            "Epoch 180/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2723 - acc: 0.9656 - val_loss: 1.2744 - val_acc: 0.9665\n",
            "Epoch 181/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2874 - acc: 0.9656 - val_loss: 1.1060 - val_acc: 0.9693\n",
            "Epoch 182/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2648 - acc: 0.9658 - val_loss: 1.2618 - val_acc: 0.9653\n",
            "Epoch 183/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2620 - acc: 0.9660 - val_loss: 1.3235 - val_acc: 0.9629\n",
            "Epoch 184/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.2573 - acc: 0.9660 - val_loss: 1.3534 - val_acc: 0.9713\n",
            "Epoch 185/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2684 - acc: 0.9659 - val_loss: 1.2580 - val_acc: 0.9712\n",
            "Epoch 186/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2518 - acc: 0.9661 - val_loss: 1.3062 - val_acc: 0.9574\n",
            "Epoch 187/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2606 - acc: 0.9658 - val_loss: 1.2696 - val_acc: 0.9696\n",
            "Epoch 188/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2505 - acc: 0.9659 - val_loss: 1.2015 - val_acc: 0.9616\n",
            "Epoch 189/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2540 - acc: 0.9658 - val_loss: 1.2158 - val_acc: 0.9728\n",
            "Epoch 190/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2423 - acc: 0.9657 - val_loss: 1.2223 - val_acc: 0.9676\n",
            "Epoch 191/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2598 - acc: 0.9657 - val_loss: 1.3981 - val_acc: 0.9683\n",
            "Epoch 192/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2360 - acc: 0.9658 - val_loss: 1.3312 - val_acc: 0.9571\n",
            "Epoch 193/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2448 - acc: 0.9663 - val_loss: 1.1654 - val_acc: 0.9682\n",
            "Epoch 194/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2344 - acc: 0.9658 - val_loss: 1.2794 - val_acc: 0.9688\n",
            "Epoch 195/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2363 - acc: 0.9663 - val_loss: 1.2394 - val_acc: 0.9679\n",
            "Epoch 196/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2278 - acc: 0.9659 - val_loss: 1.1888 - val_acc: 0.9693\n",
            "Epoch 197/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2311 - acc: 0.9660 - val_loss: 1.1485 - val_acc: 0.9584\n",
            "Epoch 198/200\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.2316 - acc: 0.9658 - val_loss: 1.1358 - val_acc: 0.9759\n",
            "Epoch 199/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2349 - acc: 0.9659 - val_loss: 1.3228 - val_acc: 0.9611\n",
            "Epoch 200/200\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2224 - acc: 0.9658 - val_loss: 1.2483 - val_acc: 0.9677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsfen6zmofgX",
        "colab_type": "code",
        "outputId": "7f0ee54c-0770-4147-fbe4-5df41cbc539c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 모델 아키텍처\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"2840pt\" viewBox=\"0.00 0.00 424.00 2130.00\" width=\"565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1.3333 1.3333) rotate(0) translate(4 2126)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-2126 420,-2126 420,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140633886873752 -->\n<g class=\"node\" id=\"node1\">\n<title>140633886873752</title>\n<polygon fill=\"none\" points=\"49,-2075.5 49,-2121.5 367,-2121.5 367,-2075.5 49,-2075.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-2094.8\">dense_1_input: InputLayer</text>\n<polyline fill=\"none\" points=\"222,-2075.5 222,-2121.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251\" y=\"-2106.3\">input:</text>\n<polyline fill=\"none\" points=\"222,-2098.5 280,-2098.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251\" y=\"-2083.3\">output:</text>\n<polyline fill=\"none\" points=\"280,-2075.5 280,-2121.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.5\" y=\"-2106.3\">(None, 226)</text>\n<polyline fill=\"none\" points=\"280,-2098.5 367,-2098.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.5\" y=\"-2083.3\">(None, 226)</text>\n</g>\n<!-- 140633886873360 -->\n<g class=\"node\" id=\"node2\">\n<title>140633886873360</title>\n<polygon fill=\"none\" points=\"82,-1992.5 82,-2038.5 334,-2038.5 334,-1992.5 82,-1992.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-2011.8\">dense_1: Dense</text>\n<polyline fill=\"none\" points=\"189,-1992.5 189,-2038.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-2023.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-2015.5 247,-2015.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-2000.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-1992.5 247,-2038.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-2023.3\">(None, 226)</text>\n<polyline fill=\"none\" points=\"247,-2015.5 334,-2015.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-2000.3\">(None, 800)</text>\n</g>\n<!-- 140633886873752&#45;&gt;140633886873360 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140633886873752-&gt;140633886873360</title>\n<path d=\"M208,-2075.3799C208,-2067.1745 208,-2057.7679 208,-2048.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-2048.784 208,-2038.784 204.5001,-2048.784 211.5001,-2048.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633886874592 -->\n<g class=\"node\" id=\"node3\">\n<title>140633886874592</title>\n<polygon fill=\"none\" points=\"82,-1909.5 82,-1955.5 334,-1955.5 334,-1909.5 82,-1909.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1928.8\">dense_2: Dense</text>\n<polyline fill=\"none\" points=\"189,-1909.5 189,-1955.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1940.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-1932.5 247,-1932.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1917.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-1909.5 247,-1955.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1940.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"247,-1932.5 334,-1932.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1917.3\">(None, 800)</text>\n</g>\n<!-- 140633886873360&#45;&gt;140633886874592 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140633886873360-&gt;140633886874592</title>\n<path d=\"M208,-1992.3799C208,-1984.1745 208,-1974.7679 208,-1965.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1965.784 208,-1955.784 204.5001,-1965.784 211.5001,-1965.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633869748208 -->\n<g class=\"node\" id=\"node4\">\n<title>140633869748208</title>\n<polygon fill=\"none\" points=\"0,-1826.5 0,-1872.5 416,-1872.5 416,-1826.5 0,-1826.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1845.8\">batch_normalization_1: BatchNormalization</text>\n<polyline fill=\"none\" points=\"271,-1826.5 271,-1872.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-1857.3\">input:</text>\n<polyline fill=\"none\" points=\"271,-1849.5 329,-1849.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-1834.3\">output:</text>\n<polyline fill=\"none\" points=\"329,-1826.5 329,-1872.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-1857.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"329,-1849.5 416,-1849.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-1834.3\">(None, 800)</text>\n</g>\n<!-- 140633886874592&#45;&gt;140633869748208 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140633886874592-&gt;140633869748208</title>\n<path d=\"M208,-1909.3799C208,-1901.1745 208,-1891.7679 208,-1882.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1882.784 208,-1872.784 204.5001,-1882.784 211.5001,-1882.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633886874088 -->\n<g class=\"node\" id=\"node5\">\n<title>140633886874088</title>\n<polygon fill=\"none\" points=\"58.5,-1743.5 58.5,-1789.5 357.5,-1789.5 357.5,-1743.5 58.5,-1743.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1762.8\">activation_1: Activation</text>\n<polyline fill=\"none\" points=\"212.5,-1743.5 212.5,-1789.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-1774.3\">input:</text>\n<polyline fill=\"none\" points=\"212.5,-1766.5 270.5,-1766.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-1751.3\">output:</text>\n<polyline fill=\"none\" points=\"270.5,-1743.5 270.5,-1789.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-1774.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"270.5,-1766.5 357.5,-1766.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-1751.3\">(None, 800)</text>\n</g>\n<!-- 140633869748208&#45;&gt;140633886874088 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140633869748208-&gt;140633886874088</title>\n<path d=\"M208,-1826.3799C208,-1818.1745 208,-1808.7679 208,-1799.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1799.784 208,-1789.784 204.5001,-1799.784 211.5001,-1799.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633871643816 -->\n<g class=\"node\" id=\"node6\">\n<title>140633871643816</title>\n<polygon fill=\"none\" points=\"82,-1660.5 82,-1706.5 334,-1706.5 334,-1660.5 82,-1660.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1679.8\">dense_3: Dense</text>\n<polyline fill=\"none\" points=\"189,-1660.5 189,-1706.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1691.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-1683.5 247,-1683.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1668.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-1660.5 247,-1706.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1691.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"247,-1683.5 334,-1683.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1668.3\">(None, 800)</text>\n</g>\n<!-- 140633886874088&#45;&gt;140633871643816 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140633886874088-&gt;140633871643816</title>\n<path d=\"M208,-1743.3799C208,-1735.1745 208,-1725.7679 208,-1716.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1716.784 208,-1706.784 204.5001,-1716.784 211.5001,-1716.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633870830056 -->\n<g class=\"node\" id=\"node7\">\n<title>140633870830056</title>\n<polygon fill=\"none\" points=\"82,-1577.5 82,-1623.5 334,-1623.5 334,-1577.5 82,-1577.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1596.8\">dense_4: Dense</text>\n<polyline fill=\"none\" points=\"189,-1577.5 189,-1623.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1608.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-1600.5 247,-1600.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1585.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-1577.5 247,-1623.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1608.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"247,-1600.5 334,-1600.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1585.3\">(None, 800)</text>\n</g>\n<!-- 140633871643816&#45;&gt;140633870830056 -->\n<g class=\"edge\" id=\"edge6\">\n<title>140633871643816-&gt;140633870830056</title>\n<path d=\"M208,-1660.3799C208,-1652.1745 208,-1642.7679 208,-1633.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1633.784 208,-1623.784 204.5001,-1633.784 211.5001,-1633.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633870857160 -->\n<g class=\"node\" id=\"node8\">\n<title>140633870857160</title>\n<polygon fill=\"none\" points=\"0,-1494.5 0,-1540.5 416,-1540.5 416,-1494.5 0,-1494.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1513.8\">batch_normalization_2: BatchNormalization</text>\n<polyline fill=\"none\" points=\"271,-1494.5 271,-1540.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-1525.3\">input:</text>\n<polyline fill=\"none\" points=\"271,-1517.5 329,-1517.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-1502.3\">output:</text>\n<polyline fill=\"none\" points=\"329,-1494.5 329,-1540.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-1525.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"329,-1517.5 416,-1517.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-1502.3\">(None, 800)</text>\n</g>\n<!-- 140633870830056&#45;&gt;140633870857160 -->\n<g class=\"edge\" id=\"edge7\">\n<title>140633870830056-&gt;140633870857160</title>\n<path d=\"M208,-1577.3799C208,-1569.1745 208,-1559.7679 208,-1550.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1550.784 208,-1540.784 204.5001,-1550.784 211.5001,-1550.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633870856824 -->\n<g class=\"node\" id=\"node9\">\n<title>140633870856824</title>\n<polygon fill=\"none\" points=\"58.5,-1411.5 58.5,-1457.5 357.5,-1457.5 357.5,-1411.5 58.5,-1411.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1430.8\">activation_2: Activation</text>\n<polyline fill=\"none\" points=\"212.5,-1411.5 212.5,-1457.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-1442.3\">input:</text>\n<polyline fill=\"none\" points=\"212.5,-1434.5 270.5,-1434.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-1419.3\">output:</text>\n<polyline fill=\"none\" points=\"270.5,-1411.5 270.5,-1457.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-1442.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"270.5,-1434.5 357.5,-1434.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-1419.3\">(None, 800)</text>\n</g>\n<!-- 140633870857160&#45;&gt;140633870856824 -->\n<g class=\"edge\" id=\"edge8\">\n<title>140633870857160-&gt;140633870856824</title>\n<path d=\"M208,-1494.3799C208,-1486.1745 208,-1476.7679 208,-1467.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1467.784 208,-1457.784 204.5001,-1467.784 211.5001,-1467.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633871088104 -->\n<g class=\"node\" id=\"node10\">\n<title>140633871088104</title>\n<polygon fill=\"none\" points=\"82,-1328.5 82,-1374.5 334,-1374.5 334,-1328.5 82,-1328.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1347.8\">dense_5: Dense</text>\n<polyline fill=\"none\" points=\"189,-1328.5 189,-1374.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1359.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-1351.5 247,-1351.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1336.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-1328.5 247,-1374.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1359.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"247,-1351.5 334,-1351.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1336.3\">(None, 800)</text>\n</g>\n<!-- 140633870856824&#45;&gt;140633871088104 -->\n<g class=\"edge\" id=\"edge9\">\n<title>140633870856824-&gt;140633871088104</title>\n<path d=\"M208,-1411.3799C208,-1403.1745 208,-1393.7679 208,-1384.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1384.784 208,-1374.784 204.5001,-1384.784 211.5001,-1384.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633870584408 -->\n<g class=\"node\" id=\"node11\">\n<title>140633870584408</title>\n<polygon fill=\"none\" points=\"82,-1245.5 82,-1291.5 334,-1291.5 334,-1245.5 82,-1245.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1264.8\">dense_6: Dense</text>\n<polyline fill=\"none\" points=\"189,-1245.5 189,-1291.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1276.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-1268.5 247,-1268.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1253.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-1245.5 247,-1291.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1276.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"247,-1268.5 334,-1268.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1253.3\">(None, 800)</text>\n</g>\n<!-- 140633871088104&#45;&gt;140633870584408 -->\n<g class=\"edge\" id=\"edge10\">\n<title>140633871088104-&gt;140633870584408</title>\n<path d=\"M208,-1328.3799C208,-1320.1745 208,-1310.7679 208,-1301.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1301.784 208,-1291.784 204.5001,-1301.784 211.5001,-1301.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633869956152 -->\n<g class=\"node\" id=\"node12\">\n<title>140633869956152</title>\n<polygon fill=\"none\" points=\"0,-1162.5 0,-1208.5 416,-1208.5 416,-1162.5 0,-1162.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1181.8\">batch_normalization_3: BatchNormalization</text>\n<polyline fill=\"none\" points=\"271,-1162.5 271,-1208.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-1193.3\">input:</text>\n<polyline fill=\"none\" points=\"271,-1185.5 329,-1185.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-1170.3\">output:</text>\n<polyline fill=\"none\" points=\"329,-1162.5 329,-1208.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-1193.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"329,-1185.5 416,-1185.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-1170.3\">(None, 800)</text>\n</g>\n<!-- 140633870584408&#45;&gt;140633869956152 -->\n<g class=\"edge\" id=\"edge11\">\n<title>140633870584408-&gt;140633869956152</title>\n<path d=\"M208,-1245.3799C208,-1237.1745 208,-1227.7679 208,-1218.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1218.784 208,-1208.784 204.5001,-1218.784 211.5001,-1218.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633869955816 -->\n<g class=\"node\" id=\"node13\">\n<title>140633869955816</title>\n<polygon fill=\"none\" points=\"58.5,-1079.5 58.5,-1125.5 357.5,-1125.5 357.5,-1079.5 58.5,-1079.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1098.8\">activation_3: Activation</text>\n<polyline fill=\"none\" points=\"212.5,-1079.5 212.5,-1125.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-1110.3\">input:</text>\n<polyline fill=\"none\" points=\"212.5,-1102.5 270.5,-1102.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-1087.3\">output:</text>\n<polyline fill=\"none\" points=\"270.5,-1079.5 270.5,-1125.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-1110.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"270.5,-1102.5 357.5,-1102.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-1087.3\">(None, 800)</text>\n</g>\n<!-- 140633869956152&#45;&gt;140633869955816 -->\n<g class=\"edge\" id=\"edge12\">\n<title>140633869956152-&gt;140633869955816</title>\n<path d=\"M208,-1162.3799C208,-1154.1745 208,-1144.7679 208,-1135.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1135.784 208,-1125.784 204.5001,-1135.784 211.5001,-1135.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633870031448 -->\n<g class=\"node\" id=\"node14\">\n<title>140633870031448</title>\n<polygon fill=\"none\" points=\"82,-996.5 82,-1042.5 334,-1042.5 334,-996.5 82,-996.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-1015.8\">dense_7: Dense</text>\n<polyline fill=\"none\" points=\"189,-996.5 189,-1042.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1027.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-1019.5 247,-1019.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-1004.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-996.5 247,-1042.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1027.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"247,-1019.5 334,-1019.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-1004.3\">(None, 800)</text>\n</g>\n<!-- 140633869955816&#45;&gt;140633870031448 -->\n<g class=\"edge\" id=\"edge13\">\n<title>140633869955816-&gt;140633870031448</title>\n<path d=\"M208,-1079.3799C208,-1071.1745 208,-1061.7679 208,-1052.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-1052.784 208,-1042.784 204.5001,-1052.784 211.5001,-1052.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633886415784 -->\n<g class=\"node\" id=\"node15\">\n<title>140633886415784</title>\n<polygon fill=\"none\" points=\"82,-913.5 82,-959.5 334,-959.5 334,-913.5 82,-913.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-932.8\">dense_8: Dense</text>\n<polyline fill=\"none\" points=\"189,-913.5 189,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-944.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-936.5 247,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-921.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-913.5 247,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-944.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"247,-936.5 334,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-921.3\">(None, 800)</text>\n</g>\n<!-- 140633870031448&#45;&gt;140633886415784 -->\n<g class=\"edge\" id=\"edge14\">\n<title>140633870031448-&gt;140633886415784</title>\n<path d=\"M208,-996.3799C208,-988.1745 208,-978.7679 208,-969.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-969.784 208,-959.784 204.5001,-969.784 211.5001,-969.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633886499952 -->\n<g class=\"node\" id=\"node16\">\n<title>140633886499952</title>\n<polygon fill=\"none\" points=\"0,-830.5 0,-876.5 416,-876.5 416,-830.5 0,-830.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-849.8\">batch_normalization_4: BatchNormalization</text>\n<polyline fill=\"none\" points=\"271,-830.5 271,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-861.3\">input:</text>\n<polyline fill=\"none\" points=\"271,-853.5 329,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-838.3\">output:</text>\n<polyline fill=\"none\" points=\"329,-830.5 329,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-861.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"329,-853.5 416,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-838.3\">(None, 800)</text>\n</g>\n<!-- 140633886415784&#45;&gt;140633886499952 -->\n<g class=\"edge\" id=\"edge15\">\n<title>140633886415784-&gt;140633886499952</title>\n<path d=\"M208,-913.3799C208,-905.1745 208,-895.7679 208,-886.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-886.784 208,-876.784 204.5001,-886.784 211.5001,-886.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633886499168 -->\n<g class=\"node\" id=\"node17\">\n<title>140633886499168</title>\n<polygon fill=\"none\" points=\"58.5,-747.5 58.5,-793.5 357.5,-793.5 357.5,-747.5 58.5,-747.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-766.8\">activation_4: Activation</text>\n<polyline fill=\"none\" points=\"212.5,-747.5 212.5,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-778.3\">input:</text>\n<polyline fill=\"none\" points=\"212.5,-770.5 270.5,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-755.3\">output:</text>\n<polyline fill=\"none\" points=\"270.5,-747.5 270.5,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-778.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"270.5,-770.5 357.5,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-755.3\">(None, 800)</text>\n</g>\n<!-- 140633886499952&#45;&gt;140633886499168 -->\n<g class=\"edge\" id=\"edge16\">\n<title>140633886499952-&gt;140633886499168</title>\n<path d=\"M208,-830.3799C208,-822.1745 208,-812.7679 208,-803.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-803.784 208,-793.784 204.5001,-803.784 211.5001,-803.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633886518184 -->\n<g class=\"node\" id=\"node18\">\n<title>140633886518184</title>\n<polygon fill=\"none\" points=\"82,-664.5 82,-710.5 334,-710.5 334,-664.5 82,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-683.8\">dense_9: Dense</text>\n<polyline fill=\"none\" points=\"189,-664.5 189,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"189,-687.5 247,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"247,-664.5 247,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-695.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"247,-687.5 334,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290.5\" y=\"-672.3\">(None, 800)</text>\n</g>\n<!-- 140633886499168&#45;&gt;140633886518184 -->\n<g class=\"edge\" id=\"edge17\">\n<title>140633886499168-&gt;140633886518184</title>\n<path d=\"M208,-747.3799C208,-739.1745 208,-729.7679 208,-720.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-720.784 208,-710.784 204.5001,-720.784 211.5001,-720.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633885961576 -->\n<g class=\"node\" id=\"node19\">\n<title>140633885961576</title>\n<polygon fill=\"none\" points=\"78.5,-581.5 78.5,-627.5 337.5,-627.5 337.5,-581.5 78.5,-581.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-600.8\">dense_10: Dense</text>\n<polyline fill=\"none\" points=\"192.5,-581.5 192.5,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221.5\" y=\"-612.3\">input:</text>\n<polyline fill=\"none\" points=\"192.5,-604.5 250.5,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221.5\" y=\"-589.3\">output:</text>\n<polyline fill=\"none\" points=\"250.5,-581.5 250.5,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"294\" y=\"-612.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"250.5,-604.5 337.5,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"294\" y=\"-589.3\">(None, 800)</text>\n</g>\n<!-- 140633886518184&#45;&gt;140633885961576 -->\n<g class=\"edge\" id=\"edge18\">\n<title>140633886518184-&gt;140633885961576</title>\n<path d=\"M208,-664.3799C208,-656.1745 208,-646.7679 208,-637.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-637.784 208,-627.784 204.5001,-637.784 211.5001,-637.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633886049840 -->\n<g class=\"node\" id=\"node20\">\n<title>140633886049840</title>\n<polygon fill=\"none\" points=\"0,-498.5 0,-544.5 416,-544.5 416,-498.5 0,-498.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-517.8\">batch_normalization_5: BatchNormalization</text>\n<polyline fill=\"none\" points=\"271,-498.5 271,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-529.3\">input:</text>\n<polyline fill=\"none\" points=\"271,-521.5 329,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-506.3\">output:</text>\n<polyline fill=\"none\" points=\"329,-498.5 329,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-529.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"329,-521.5 416,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-506.3\">(None, 800)</text>\n</g>\n<!-- 140633885961576&#45;&gt;140633886049840 -->\n<g class=\"edge\" id=\"edge19\">\n<title>140633885961576-&gt;140633886049840</title>\n<path d=\"M208,-581.3799C208,-573.1745 208,-563.7679 208,-554.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-554.784 208,-544.784 204.5001,-554.784 211.5001,-554.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633886049784 -->\n<g class=\"node\" id=\"node21\">\n<title>140633886049784</title>\n<polygon fill=\"none\" points=\"58.5,-415.5 58.5,-461.5 357.5,-461.5 357.5,-415.5 58.5,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-434.8\">activation_5: Activation</text>\n<polyline fill=\"none\" points=\"212.5,-415.5 212.5,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"212.5,-438.5 270.5,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"270.5,-415.5 270.5,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-446.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"270.5,-438.5 357.5,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-423.3\">(None, 800)</text>\n</g>\n<!-- 140633886049840&#45;&gt;140633886049784 -->\n<g class=\"edge\" id=\"edge20\">\n<title>140633886049840-&gt;140633886049784</title>\n<path d=\"M208,-498.3799C208,-490.1745 208,-480.7679 208,-471.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-471.784 208,-461.784 204.5001,-471.784 211.5001,-471.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633886091416 -->\n<g class=\"node\" id=\"node22\">\n<title>140633886091416</title>\n<polygon fill=\"none\" points=\"79,-332.5 79,-378.5 337,-378.5 337,-332.5 79,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-351.8\">dense_11: Dense</text>\n<polyline fill=\"none\" points=\"192,-332.5 192,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"192,-355.5 250,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"250,-332.5 250,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293.5\" y=\"-363.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"250,-355.5 337,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293.5\" y=\"-340.3\">(None, 800)</text>\n</g>\n<!-- 140633886049784&#45;&gt;140633886091416 -->\n<g class=\"edge\" id=\"edge21\">\n<title>140633886049784-&gt;140633886091416</title>\n<path d=\"M208,-415.3799C208,-407.1745 208,-397.7679 208,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-388.784 208,-378.784 204.5001,-388.784 211.5001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633872499320 -->\n<g class=\"node\" id=\"node23\">\n<title>140633872499320</title>\n<polygon fill=\"none\" points=\"78.5,-249.5 78.5,-295.5 337.5,-295.5 337.5,-249.5 78.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-268.8\">dense_12: Dense</text>\n<polyline fill=\"none\" points=\"192.5,-249.5 192.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"192.5,-272.5 250.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"250.5,-249.5 250.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"294\" y=\"-280.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"250.5,-272.5 337.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"294\" y=\"-257.3\">(None, 800)</text>\n</g>\n<!-- 140633886091416&#45;&gt;140633872499320 -->\n<g class=\"edge\" id=\"edge22\">\n<title>140633886091416-&gt;140633872499320</title>\n<path d=\"M208,-332.3799C208,-324.1745 208,-314.7679 208,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-305.784 208,-295.784 204.5001,-305.784 211.5001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633872163504 -->\n<g class=\"node\" id=\"node24\">\n<title>140633872163504</title>\n<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 416,-212.5 416,-166.5 0,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-185.8\">batch_normalization_6: BatchNormalization</text>\n<polyline fill=\"none\" points=\"271,-166.5 271,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"271,-189.5 329,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"329,-166.5 329,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-197.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"329,-189.5 416,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-174.3\">(None, 800)</text>\n</g>\n<!-- 140633872499320&#45;&gt;140633872163504 -->\n<g class=\"edge\" id=\"edge23\">\n<title>140633872499320-&gt;140633872163504</title>\n<path d=\"M208,-249.3799C208,-241.1745 208,-231.7679 208,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-222.784 208,-212.784 204.5001,-222.784 211.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633872164960 -->\n<g class=\"node\" id=\"node25\">\n<title>140633872164960</title>\n<polygon fill=\"none\" points=\"58.5,-83.5 58.5,-129.5 357.5,-129.5 357.5,-83.5 58.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-102.8\">activation_6: Activation</text>\n<polyline fill=\"none\" points=\"212.5,-83.5 212.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"212.5,-106.5 270.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"270.5,-83.5 270.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-114.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"270.5,-106.5 357.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-91.3\">(None, 800)</text>\n</g>\n<!-- 140633872163504&#45;&gt;140633872164960 -->\n<g class=\"edge\" id=\"edge24\">\n<title>140633872163504-&gt;140633872164960</title>\n<path d=\"M208,-166.3799C208,-158.1745 208,-148.7679 208,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-139.784 208,-129.784 204.5001,-139.784 211.5001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140633872199464 -->\n<g class=\"node\" id=\"node26\">\n<title>140633872199464</title>\n<polygon fill=\"none\" points=\"78.5,-.5 78.5,-46.5 337.5,-46.5 337.5,-.5 78.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-19.8\">dense_13: Dense</text>\n<polyline fill=\"none\" points=\"192.5,-.5 192.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"192.5,-23.5 250.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"250.5,-.5 250.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"294\" y=\"-31.3\">(None, 800)</text>\n<polyline fill=\"none\" points=\"250.5,-23.5 337.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"294\" y=\"-8.3\">(None, 4)</text>\n</g>\n<!-- 140633872164960&#45;&gt;140633872199464 -->\n<g class=\"edge\" id=\"edge25\">\n<title>140633872164960-&gt;140633872199464</title>\n<path d=\"M208,-83.3799C208,-75.1745 208,-65.7679 208,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.5001,-56.784 208,-46.784 204.5001,-56.784 211.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtD82zROogDP",
        "colab_type": "code",
        "outputId": "110e5589-99a6-48e3-b19d-7e9c2d6563d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# 학습 과정\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('mean absolute error')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAAEICAYAAADyTpvZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMAwj70wyMgzDMPLOJCPDMAwj7/4fzxtWu4lhevsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRMuL2mFogIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 예측값을 생성합니다.\n",
        "\n",
        "pred_test = model.predict(test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv6yp_Hzoq4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# submission 파일을 생성합니다.\n",
        "sample_sub = pd.read_csv('/gdrive/My Drive/DACON-semiconductor-competition/dataset/sample_submission.csv', index_col=0)\n",
        "submission = sample_sub+pred_test\n",
        "submission.to_csv('/gdrive/My Drive/DACON-semiconductor-competition/submission_13.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPOj7kOro8tD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습된 모델을 저장합니다.\n",
        "\n",
        "model.save('/gdrive/My Drive/DACON-semiconductor-competition/model_13.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VUa7WaWravb",
        "colab_type": "code",
        "outputId": "6973c0c8-8698-411a-d297-0df24a795503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 추가 학습\n",
        "\n",
        "hist = model.fit(train_X, train_Y, epochs=50, batch_size=630,\n",
        "                    validation_data=(val_X, val_Y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2211 - acc: 0.9659 - val_loss: 1.3729 - val_acc: 0.9599\n",
            "Epoch 2/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.2122 - acc: 0.9658 - val_loss: 1.3440 - val_acc: 0.9740\n",
            "Epoch 3/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2218 - acc: 0.9659 - val_loss: 1.0979 - val_acc: 0.9635\n",
            "Epoch 4/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2106 - acc: 0.9662 - val_loss: 1.0552 - val_acc: 0.9650\n",
            "Epoch 5/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2190 - acc: 0.9657 - val_loss: 1.1822 - val_acc: 0.9635\n",
            "Epoch 6/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2136 - acc: 0.9658 - val_loss: 1.2069 - val_acc: 0.9628\n",
            "Epoch 7/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1959 - acc: 0.9661 - val_loss: 1.1296 - val_acc: 0.9668\n",
            "Epoch 8/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2008 - acc: 0.9658 - val_loss: 1.1135 - val_acc: 0.9650\n",
            "Epoch 9/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.2145 - acc: 0.9661 - val_loss: 1.1580 - val_acc: 0.9744\n",
            "Epoch 10/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1964 - acc: 0.9659 - val_loss: 1.2558 - val_acc: 0.9740\n",
            "Epoch 11/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.2055 - acc: 0.9659 - val_loss: 1.2965 - val_acc: 0.9729\n",
            "Epoch 12/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1967 - acc: 0.9658 - val_loss: 1.4355 - val_acc: 0.9611\n",
            "Epoch 13/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1962 - acc: 0.9657 - val_loss: 1.2362 - val_acc: 0.9583\n",
            "Epoch 14/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1956 - acc: 0.9660 - val_loss: 1.1034 - val_acc: 0.9584\n",
            "Epoch 15/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1997 - acc: 0.9658 - val_loss: 1.2185 - val_acc: 0.9692\n",
            "Epoch 16/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1771 - acc: 0.9661 - val_loss: 1.1188 - val_acc: 0.9669\n",
            "Epoch 17/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.2008 - acc: 0.9657 - val_loss: 1.2350 - val_acc: 0.9699\n",
            "Epoch 18/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1890 - acc: 0.9660 - val_loss: 0.9767 - val_acc: 0.9630\n",
            "Epoch 19/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1821 - acc: 0.9661 - val_loss: 1.2195 - val_acc: 0.9526\n",
            "Epoch 20/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1773 - acc: 0.9660 - val_loss: 1.2657 - val_acc: 0.9582\n",
            "Epoch 21/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1787 - acc: 0.9656 - val_loss: 1.0872 - val_acc: 0.9568\n",
            "Epoch 22/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1751 - acc: 0.9658 - val_loss: 1.3708 - val_acc: 0.9651\n",
            "Epoch 23/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1889 - acc: 0.9660 - val_loss: 1.2616 - val_acc: 0.9658\n",
            "Epoch 24/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1812 - acc: 0.9661 - val_loss: 1.0681 - val_acc: 0.9697\n",
            "Epoch 25/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1690 - acc: 0.9660 - val_loss: 1.0652 - val_acc: 0.9590\n",
            "Epoch 26/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1620 - acc: 0.9659 - val_loss: 1.0304 - val_acc: 0.9724\n",
            "Epoch 27/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1813 - acc: 0.9660 - val_loss: 1.0476 - val_acc: 0.9696\n",
            "Epoch 28/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1647 - acc: 0.9659 - val_loss: 1.1564 - val_acc: 0.9651\n",
            "Epoch 29/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1708 - acc: 0.9658 - val_loss: 1.0894 - val_acc: 0.9706\n",
            "Epoch 30/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1605 - acc: 0.9664 - val_loss: 1.0497 - val_acc: 0.9633\n",
            "Epoch 31/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1526 - acc: 0.9660 - val_loss: 1.3443 - val_acc: 0.9647\n",
            "Epoch 32/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1648 - acc: 0.9659 - val_loss: 1.2109 - val_acc: 0.9722\n",
            "Epoch 33/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1549 - acc: 0.9660 - val_loss: 1.1763 - val_acc: 0.9698\n",
            "Epoch 34/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1637 - acc: 0.9662 - val_loss: 1.0486 - val_acc: 0.9622\n",
            "Epoch 35/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1584 - acc: 0.9657 - val_loss: 1.0914 - val_acc: 0.9674\n",
            "Epoch 36/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1570 - acc: 0.9661 - val_loss: 1.2411 - val_acc: 0.9759\n",
            "Epoch 37/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1500 - acc: 0.9659 - val_loss: 1.1425 - val_acc: 0.9724\n",
            "Epoch 38/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1453 - acc: 0.9664 - val_loss: 1.0485 - val_acc: 0.9744\n",
            "Epoch 39/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1574 - acc: 0.9662 - val_loss: 0.9667 - val_acc: 0.9671\n",
            "Epoch 40/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1385 - acc: 0.9660 - val_loss: 1.0938 - val_acc: 0.9646\n",
            "Epoch 41/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1446 - acc: 0.9658 - val_loss: 1.1093 - val_acc: 0.9688\n",
            "Epoch 42/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1424 - acc: 0.9661 - val_loss: 1.2491 - val_acc: 0.9749\n",
            "Epoch 43/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1467 - acc: 0.9661 - val_loss: 1.0922 - val_acc: 0.9711\n",
            "Epoch 44/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1391 - acc: 0.9660 - val_loss: 1.0177 - val_acc: 0.9582\n",
            "Epoch 45/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1361 - acc: 0.9661 - val_loss: 1.0691 - val_acc: 0.9626\n",
            "Epoch 46/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1434 - acc: 0.9660 - val_loss: 1.2716 - val_acc: 0.9597\n",
            "Epoch 47/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1304 - acc: 0.9660 - val_loss: 1.1861 - val_acc: 0.9780\n",
            "Epoch 48/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1302 - acc: 0.9661 - val_loss: 1.1871 - val_acc: 0.9637\n",
            "Epoch 49/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1256 - acc: 0.9662 - val_loss: 0.9944 - val_acc: 0.9747\n",
            "Epoch 50/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1314 - acc: 0.9662 - val_loss: 1.0332 - val_acc: 0.9647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v08W3hDeNH__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# submission 파일을 생성합니다.\n",
        "sample_sub = pd.read_csv('/gdrive/My Drive/DACON-semiconductor-competition/dataset/sample_submission.csv', index_col=0)\n",
        "submission = sample_sub+pred_test\n",
        "submission.to_csv('/gdrive/My Drive/DACON-semiconductor-competition/submission_13_250.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVjog7JoNIMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습된 모델을 저장합니다.\n",
        "\n",
        "model.save('/gdrive/My Drive/DACON-semiconductor-competition/model_13_250.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKHfHdgONN7V",
        "colab_type": "code",
        "outputId": "c46438d8-135a-4e4c-b535-698e9b8fc45d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 추가 학습\n",
        "\n",
        "hist = model.fit(train_X, train_Y, epochs=50, batch_size=630,\n",
        "                    validation_data=(val_X, val_Y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 607500 samples, validate on 202500 samples\n",
            "Epoch 1/50\n",
            "607500/607500 [==============================] - 22s 37us/step - loss: 1.1327 - acc: 0.9661 - val_loss: 1.0723 - val_acc: 0.9721\n",
            "Epoch 2/50\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 1.1227 - acc: 0.9660 - val_loss: 1.0855 - val_acc: 0.9711\n",
            "Epoch 3/50\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 1.1269 - acc: 0.9659 - val_loss: 0.9968 - val_acc: 0.9579\n",
            "Epoch 4/50\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 1.1155 - acc: 0.9659 - val_loss: 1.1191 - val_acc: 0.9677\n",
            "Epoch 5/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1322 - acc: 0.9661 - val_loss: 1.0796 - val_acc: 0.9616\n",
            "Epoch 6/50\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 1.1185 - acc: 0.9661 - val_loss: 1.4093 - val_acc: 0.9618\n",
            "Epoch 7/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1234 - acc: 0.9660 - val_loss: 1.1125 - val_acc: 0.9634\n",
            "Epoch 8/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1089 - acc: 0.9664 - val_loss: 1.2668 - val_acc: 0.9630\n",
            "Epoch 9/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1165 - acc: 0.9658 - val_loss: 1.1176 - val_acc: 0.9680\n",
            "Epoch 10/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1118 - acc: 0.9661 - val_loss: 1.1505 - val_acc: 0.9727\n",
            "Epoch 11/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1104 - acc: 0.9661 - val_loss: 1.1464 - val_acc: 0.9756\n",
            "Epoch 12/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1127 - acc: 0.9661 - val_loss: 0.9575 - val_acc: 0.9753\n",
            "Epoch 13/50\n",
            "607500/607500 [==============================] - 22s 35us/step - loss: 1.1036 - acc: 0.9659 - val_loss: 1.0831 - val_acc: 0.9691\n",
            "Epoch 14/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1107 - acc: 0.9659 - val_loss: 1.0570 - val_acc: 0.9591\n",
            "Epoch 15/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.1102 - acc: 0.9659 - val_loss: 1.0366 - val_acc: 0.9727\n",
            "Epoch 16/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0895 - acc: 0.9664 - val_loss: 1.0517 - val_acc: 0.9695\n",
            "Epoch 17/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1080 - acc: 0.9663 - val_loss: 1.1103 - val_acc: 0.9743\n",
            "Epoch 18/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1054 - acc: 0.9661 - val_loss: 1.0100 - val_acc: 0.9637\n",
            "Epoch 19/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0981 - acc: 0.9663 - val_loss: 1.1397 - val_acc: 0.9752\n",
            "Epoch 20/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.1013 - acc: 0.9664 - val_loss: 1.1315 - val_acc: 0.9736\n",
            "Epoch 21/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.0953 - acc: 0.9659 - val_loss: 1.0018 - val_acc: 0.9639\n",
            "Epoch 22/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0959 - acc: 0.9661 - val_loss: 1.1174 - val_acc: 0.9688\n",
            "Epoch 23/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.0959 - acc: 0.9663 - val_loss: 0.9590 - val_acc: 0.9713\n",
            "Epoch 24/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0951 - acc: 0.9659 - val_loss: 1.1690 - val_acc: 0.9563\n",
            "Epoch 25/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0895 - acc: 0.9660 - val_loss: 1.1251 - val_acc: 0.9702\n",
            "Epoch 26/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0861 - acc: 0.9662 - val_loss: 1.1874 - val_acc: 0.9620\n",
            "Epoch 27/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.0936 - acc: 0.9661 - val_loss: 0.9478 - val_acc: 0.9694\n",
            "Epoch 28/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.0888 - acc: 0.9660 - val_loss: 0.9807 - val_acc: 0.9734\n",
            "Epoch 29/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0775 - acc: 0.9660 - val_loss: 0.9518 - val_acc: 0.9707\n",
            "Epoch 30/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.0874 - acc: 0.9663 - val_loss: 1.0975 - val_acc: 0.9701\n",
            "Epoch 31/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.0835 - acc: 0.9663 - val_loss: 1.0706 - val_acc: 0.9739\n",
            "Epoch 32/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.0831 - acc: 0.9662 - val_loss: 0.9809 - val_acc: 0.9682\n",
            "Epoch 33/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.0793 - acc: 0.9659 - val_loss: 1.0776 - val_acc: 0.9625\n",
            "Epoch 34/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.0711 - acc: 0.9662 - val_loss: 0.9319 - val_acc: 0.9668\n",
            "Epoch 35/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0846 - acc: 0.9662 - val_loss: 1.0016 - val_acc: 0.9616\n",
            "Epoch 36/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0706 - acc: 0.9664 - val_loss: 1.0226 - val_acc: 0.9743\n",
            "Epoch 37/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0769 - acc: 0.9663 - val_loss: 1.0699 - val_acc: 0.9656\n",
            "Epoch 38/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0725 - acc: 0.9663 - val_loss: 0.9616 - val_acc: 0.9682\n",
            "Epoch 39/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0758 - acc: 0.9660 - val_loss: 0.9619 - val_acc: 0.9660\n",
            "Epoch 40/50\n",
            "607500/607500 [==============================] - 21s 35us/step - loss: 1.0635 - acc: 0.9662 - val_loss: 1.0921 - val_acc: 0.9545\n",
            "Epoch 41/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0806 - acc: 0.9663 - val_loss: 0.9503 - val_acc: 0.9555\n",
            "Epoch 42/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0732 - acc: 0.9662 - val_loss: 1.1356 - val_acc: 0.9750\n",
            "Epoch 43/50\n",
            "607500/607500 [==============================] - 22s 36us/step - loss: 1.0609 - acc: 0.9663 - val_loss: 0.9016 - val_acc: 0.9569\n",
            "Epoch 44/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0770 - acc: 0.9661 - val_loss: 0.9569 - val_acc: 0.9666\n",
            "Epoch 45/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0590 - acc: 0.9661 - val_loss: 1.1238 - val_acc: 0.9691\n",
            "Epoch 46/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0633 - acc: 0.9661 - val_loss: 1.0171 - val_acc: 0.9740\n",
            "Epoch 47/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0741 - acc: 0.9660 - val_loss: 0.9743 - val_acc: 0.9761\n",
            "Epoch 48/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0659 - acc: 0.9663 - val_loss: 1.0489 - val_acc: 0.9698\n",
            "Epoch 49/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0652 - acc: 0.9662 - val_loss: 0.9802 - val_acc: 0.9618\n",
            "Epoch 50/50\n",
            "607500/607500 [==============================] - 21s 34us/step - loss: 1.0616 - acc: 0.9660 - val_loss: 1.2976 - val_acc: 0.9682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJxjEgdHNOCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# submission 파일을 생성합니다.\n",
        "sample_sub = pd.read_csv('/gdrive/My Drive/DACON-semiconductor-competition/dataset/sample_submission.csv', index_col=0)\n",
        "submission = sample_sub+pred_test\n",
        "submission.to_csv('/gdrive/My Drive/DACON-semiconductor-competition/submission_13_300.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh_oqG80NOLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습된 모델을 저장합니다.\n",
        "\n",
        "model.save('/gdrive/My Drive/DACON-semiconductor-competition/model_13_300.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3IaqhJqmV_5",
        "colab_type": "text"
      },
      "source": [
        "### Bayesian Optimization\n",
        "http://research.sualab.com/introduction/practice/2019/02/19/bayesian-optimization-overview-1.html<br>\n",
        "http://research.sualab.com/introduction/practice/2019/04/01/bayesian-optimization-overview-2.html<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNwDuaRWmWg4",
        "colab_type": "text"
      },
      "source": [
        "### Swish Activation\n",
        "https://www.machinecurve.com/index.php/2019/05/30/why-swish-could-perform-better-than-relu/#todays-activation-functions"
      ]
    }
  ]
}